{"cells":[{"attachments":{},"cell_type":"markdown","id":"e76168a0-0f0d-4a0e-a787-7960f88bb805","metadata":{"deletable":false,"editable":false},"source":["# Tinkering Notebook for Lecture 7 - Policy Gradient\n","\n","This notebook focuses on policy gradient method. In the notebook we will implement REINFORCE, REINFORCE with a base line and One-step Actor-Critic. \n","\n","Note that the code in the notebook does not average over several runs, so the results can deviate from the corresponding examples in the textbook each run (especially for methods with high variance). Thus you should re-run all examples several times, and possibly add code to do averaging. "]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["# Table of content\n","* ### [1. Imports ](#sec1)\n","* ### [2. The short corridor environment](#sec2)\n","* ### [3. REINFORCE (with baseline)](#sec3)\n"," * #### [3.1 Without a baseline](#sec3_1)\n"," * #### [3.2 With baseline](#sec3_2)\n","* ### [4. One-step Actorc-Critic](#sec4)\n"]},{"attachments":{},"cell_type":"markdown","id":"a3522f9c-cdaa-49ef-91bf-e6243c71f227","metadata":{"deletable":false,"editable":false},"source":["# 1. Imports  <a id=\"sec1\">"]},{"cell_type":"code","execution_count":null,"id":"e2bd1061","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"id":"1cf42c38-c9d0-4c52-86b7-d5ecb4065afb","metadata":{},"outputs":[],"source":["import gymnasium as gym \n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.special import softmax\n","import gym_RLcourse\n","from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."]},{"attachments":{},"cell_type":"markdown","id":"8693c2f2-6fac-41e5-834f-5dfa31f81b7d","metadata":{"deletable":false,"editable":false},"source":["# 2. The short corridor environment <a id=\"sec2\">"]},{"attachments":{},"cell_type":"markdown","id":"1ee799c1-3f91-43db-916a-a49ae6dda79f","metadata":{"deletable":false,"editable":false},"source":["In this notebook we will use the short corridor environment discussed in Example 13.1 of the textbook."]},{"cell_type":"code","execution_count":24,"id":"6f9dbb0f-0384-4d56-aa69-231d53af1f52","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["State space: Discrete(4)\n","Action space: Discrete(2)\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["env = gym.make('ShortCorridor-v0', render_mode=\"human\")\n","state, info = env.reset()\n","print('State space:', env.observation_space)\n","print('Action space:', env.action_space)"]},{"attachments":{},"cell_type":"markdown","id":"54c95574-5328-4cee-aee0-2b73e98e6b47","metadata":{"deletable":false,"editable":false},"source":["The environment has three non-terminal states and 2 actions (left or right)."]},{"cell_type":"code","execution_count":null,"id":"2596cc29-7149-41c1-b687-7bda89525309","metadata":{},"outputs":[],"source":["LEFT = 0\n","RIGHT = 1"]},{"attachments":{},"cell_type":"markdown","id":"86b03050-f219-4909-80f2-e0c43a9a20bf","metadata":{"deletable":false,"editable":false},"source":["The meaning of these are exactly as they sound"]},{"cell_type":"code","execution_count":null,"id":"72636da3-2441-4d8b-b933-afe7b27e30c9","metadata":{},"outputs":[],"source":["state, reward, terminated, truncated, info = env.step(RIGHT)\n","print('Reward:', reward)"]},{"attachments":{},"cell_type":"markdown","id":"8b6bd6ca-00ce-4fe0-9da3-77cc53ff18f4","metadata":{"deletable":false,"editable":false},"source":["However, in the middle non-terminal state, the actions are reversed. Hence, if we go `RIGHT` again we will end up at the start."]},{"cell_type":"code","execution_count":null,"id":"0f2114cd-a6ac-40cf-989e-b94d158484c3","metadata":{},"outputs":[],"source":["state, reward, terminated, truncated, info = env.step(RIGHT)\n","print('Reward:', reward)"]},{"attachments":{},"cell_type":"markdown","id":"5bb16f89-09c0-4530-bf5a-c096b3deb4b9","metadata":{"deletable":false,"editable":false},"source":["Finally we close the environment to close the graphical window. "]},{"cell_type":"code","execution_count":null,"id":"ba66c510-ddfd-4a78-a2d7-ecced297f6ba","metadata":{},"outputs":[],"source":["env.close()"]},{"attachments":{},"cell_type":"markdown","id":"04b07af0-79f2-4e35-a964-de4a685868dd","metadata":{"deletable":false,"editable":false},"source":["This is a quite simple environment to solve and any tabular RL-method can handle it.\n","\n","However, to be able to determine the optimal action to take we must know what state we are in. Now assume that we use function approximation with the features \n","\n","$$\n","\\mathbf{x}(s, RIGHT) = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{x}(s, LEFT) = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\quad \\text{for all } s\n","$$"]},{"attachments":{},"cell_type":"markdown","id":"efc40465-9466-40a9-a47f-5e75bc680d38","metadata":{"deletable":false,"editable":false},"source":["Notice that with these features the agent is not able to differentiate between the states based on only looking at the features. Hence, a policy based on these features must choose the action without knowing which state it is in (with these features the agent is blind).\n","\n","For example, if we find a policy by first estimating $\\hat{q}$ with linear function approximation then we get\n","\n","$$\n","\\hat{q}(s,a,\\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{x}(s,a) = \\begin{cases} w_1 & \\text{if } a = RIGHT \\\\ w_2 & \\text{if } a = LEFT\\end{cases}\n","$$\n","Hence, the greedy action w.r.t to $\\hat{q}$ would be always `RIGHT` if $w_1 > w_2$ and always `LEFT` if $w_2 > w_1$. However, such a policy would never reach the goal in this environment. Hence, when the agent uses these features a stochastic policy is needed, and the goal is to learn the optimal probability for `RIGHT` vs `LEFT`. \n","\n","___Remark:___ Of course, the features we use here are not very good for this environment, since they lose all information about what state we are in. However, it is an interesting example for two reasons:\n","1. It gives us a simple situation to experiment with where the optimal policy is stochastic. \n","2. In a real scenario it may be the case that we have no way of knowing what the current state is, and we thus have to pick an action without knowing the state. With the features described above we end up in this situation. In the code we use below, you will see that the information about current state is never actually used by the agent. "]},{"attachments":{},"cell_type":"markdown","id":"800f3798-bf7a-4e63-ba2b-7426470669d8","metadata":{"deletable":false,"editable":false},"source":["# 3. REINFORCE (with baseline) <a id=\"sec3\">"]},{"attachments":{},"cell_type":"markdown","id":"fd1e3a83-37d9-4e10-b0ad-1f524e96bae8","metadata":{"deletable":false,"editable":false},"source":["In this section we will implement the REINFORCE algorithm (potentially with $\\hat{v}(s, \\mathbf{w})$ as a baseline). For this we will implement three classes (SC stands for short corridor):\n","\n","1. `PolicySC` which implements the policy $\\pi(a|s, \\boldsymbol{\\theta})$. Here we will implement a soft-max policy with linear preferences using the features discussed in the previous section. The class has one method that returns the probability for each action with the current $\\boldsymbol{\\theta}$, one method that picks an action according to these probabilities, and one method for computing $\\nabla \\ln \\pi(a|s, \\boldsymbol{\\theta})$.\n","\n","2. `ValueFunctionSC` which implements the estimate $\\hat{v}(s, \\mathbf{w})$ (used as a baseline). Here we implement a very simple function estimator with just one weight (scalar) $\\hat{v}(s, w) = w$. The function has one method for computing $\\hat{v}(s, w)$ and one for computing $\\nabla \\hat{v}(s, w)$. Note that with this approximation we give the same approximated value to all states (since the agent cannot differentiate between the states).\n","\n","3. `REINFORCE` implements acting and learning. We will assume that $\\gamma = 1$ in the implementation (see e.g. algorithms in lecture slides), but if you want you can extend it to handle the case when $\\gamma < 1$ by looking at the algorithms in the textbook."]},{"cell_type":"code","execution_count":null,"id":"bdffe839","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"5ae9dca0-df59-44f8-abbd-c0fbc32e62d2","metadata":{},"outputs":[],"source":["class PolicySC:\n","    \n","    def __init__(self):\n","        \n","        self.actions = np.array([0, 1])\n","        self.theta = np.array([ -1.47, 1.47]) # Initially higher preference for LEFT (theta[1]) than RIGHT (theta[0])\n","        self.features = np.array([[0, 1], [1, 0]])  # self.features[:, 0] for left and self.features[:,1] for right.\n","        \n","    def a_probs(self, state):\n","        # Compute softmax(theta^T * features) for all a \n","        h = np.dot(self.theta, self.features)\n","        return softmax(h)\n","    \n","    def act(self, state):\n","        return np.random.choice(self.actions, p=self.a_probs(state))\n","    \n","    def ln_grad(self, state, action):\n","        # See \"Exercises on policy-gradient methods\" or equation (13.9) in textbook\n","        # Also note that in this example the features do not depend on the state\n","        return self.features[:, action] - np.dot(self.features, self.a_probs(state))"]},{"attachments":{},"cell_type":"markdown","id":"c47f5ea2-daf0-4b26-908e-0bc904c41fa6","metadata":{"deletable":false,"editable":false},"source":["__TASK__: Make sure that you understand `PolicySC`. Note that `a_probs` returns an array with probability for taking left or right. With the initial $\\boldsymbol{\\theta}$ the probabilities are given by"]},{"cell_type":"code","execution_count":null,"id":"80738f6d-6450-456b-a117-510cdfd63be6","metadata":{},"outputs":[],"source":["policy = PolicySC()\n","print(\"Probabilities:\", policy.a_probs(0)) \n","print(\"Probability for LEFT: %.2f\" % (policy.a_probs(0)[0]))\n","print(\"Probability for RIGHT: %.2f\" % (policy.a_probs(0)[1]))"]},{"attachments":{},"cell_type":"markdown","id":"4262905f-81b6-48aa-972b-d2b08b9f0f40","metadata":{"deletable":false,"editable":false},"source":["To get one action with these probabilities we can use (note that you will get `LEFT=0` about 95% of the time, and `RIGHT=1` only around 5% of the time."]},{"cell_type":"code","execution_count":null,"id":"4dbba130-fc35-46a3-a8fe-59fd62dd8680","metadata":{},"outputs":[],"source":["print(\"Random action:\", policy.act(0))"]},{"cell_type":"code","execution_count":null,"id":"a6473989-41cd-41b2-bc95-73a2e61b5772","metadata":{},"outputs":[],"source":["class ValueFunctionSC:\n","    \n","    def __init__(self):\n","        self.w = 0\n","        \n","    def value(self, state):\n","        return self.w\n","    \n","    def grad(self, state):\n","        return 1 "]},{"attachments":{},"cell_type":"markdown","id":"960daa25-bf03-4443-ac3d-a2587f134a7a","metadata":{"deletable":false,"editable":false},"source":["__TASK__: Make sure you understand `ValueFunctionSC`. Note that here we use the very simple function approximation $\\hat{v}(s, w) = w$ for all $s$."]},{"cell_type":"code","execution_count":null,"id":"9937c347-b3e9-4ebb-96f2-ad8a5726f8a8","metadata":{},"outputs":[],"source":["class REINFORCE:\n","    \n","    def __init__(self, policy, valuefunc, alpha_theta=(2**-13), alpha_w = (2**-6), baseline=False):\n","        \n","        self.alpha_theta = alpha_theta\n","        self.alpha_w = alpha_w\n","        self.baseline = baseline\n","        \n","        self.policy = policy\n","        self.valuefunc = valuefunc\n","        \n","    def act(self, state):\n","        return self.policy.act(state)\n","        \n","    def learn(self, states, actions, rewards):\n","        T = len(states)\n","        \n","        for t in range(0,T-1):\n","            Gt = np.sum(rewards[t+1:T])\n","            state = states[t]\n","            action = actions[t]\n","            \n","            if self.baseline:\n","                delta = Gt - self.valuefunc.value(state)\n","                self.valuefunc.w += self.alpha_w*delta*self.valuefunc.grad(state)\n","                self.policy.theta += self.alpha_theta*delta*self.policy.ln_grad(state, action)\n","            else:\n","                # TODO Implement update for REINFORCE without baseline"]},{"attachments":{},"cell_type":"markdown","id":"49c6a84c-dc80-4933-8c99-cc9063addf2d","metadata":{"deletable":false,"editable":false},"source":["__TASK__: \n","1. Make sure that you understand the code in `REINFORCE`. \n","2. Write the code for updating $\\boldsymbol{\\theta}$ when there is no baseline."]},{"attachments":{},"cell_type":"markdown","id":"89735755-831f-4324-975b-1095cab010bd","metadata":{"deletable":false,"editable":false},"source":["Finally we write a function for training our `REINFORCE`-agent. It will run a full episode using the policy `agent.act` and then send all states, rewards and actions seen in the episode to `agent.learn`. "]},{"cell_type":"code","execution_count":null,"id":"ece5e62c-2575-4cca-95a6-7f7e52820b70","metadata":{},"outputs":[],"source":["def train(env, agent, n_episodes):\n","    \n","    total_reward = -1000*np.ones(n_episodes)\n","    print('Episode 1')\n","    for i in range(n_episodes):\n","        \n","        if ((i+1) % 100) == 0:\n","            clear_output(wait=True)\n","            print('Episode', i+1)\n","            \n","        state, info = env.reset()\n","        \n","        states = [state]\n","        actions = []\n","        rewards = [0] # rewards[0] is never used.\n","        \n","        terminated = truncated = False\n","        \n","        while not terminated and not truncated:\n","            action = agent.act(state)\n","            actions.append(action)\n","            state, reward, terminated, truncated, info = env.step(action)\n","            \n","            states.append(state)\n","            rewards.append(reward)\n","            \n","        if truncated:\n","            print('An episode was truncated, so the training stop at episode', i)\n","            return (total_reward, True)\n","        \n","        agent.learn(states, actions, rewards)\n","        total_reward[i] = np.sum(rewards)\n","        \n","    return (total_reward, False)"]},{"attachments":{},"cell_type":"markdown","id":"b1a8a4ec-1950-4903-9ced-c44fd3bfc13e","metadata":{"deletable":false,"editable":false},"source":["### A reamrk on truncation"]},{"attachments":{},"cell_type":"markdown","id":"76cbf6be-c07d-41e0-b25b-298f7220034a","metadata":{"deletable":false,"editable":false},"source":["One problem with the given environment is that with a deterministic policy, the agent will never reach the goal. Even with a policy that is close to deterministic (lets say 99.9% for left and 0.01% for right) it may take very long time for the agent to reach the goal. For this reason, `ShortCorridor-v0` will truncate the episode if the goal is not reach within 1000 steps. \n","\n","This however means that we were not able to finish the episode, and therefore we cannot compute returns. For this reason `train` will stop the training if an episode is truncated.\n","\n","Below you will see that larger step sizes usually leads to faster learning, but also increases the risk that the agent gets stuck with an almost deterministic policy and thus never learns the optimal policy. "]},{"attachments":{},"cell_type":"markdown","id":"eabe304d-448c-4cbd-9064-18c6c1f5bbd9","metadata":{"deletable":false,"editable":false},"source":["## 3.1 Without a baseline <a id=\"sec3_1\">"]},{"attachments":{},"cell_type":"markdown","id":"e8f406c1-2766-481f-b71a-63b3ebe8aefa","metadata":{"deletable":false,"editable":false},"source":["We first create an environment to train in "]},{"cell_type":"code","execution_count":null,"id":"745d698b-50d5-4f33-927e-4d2be609d9d5","metadata":{},"outputs":[],"source":["env_train = gym.make('ShortCorridor-v0')"]},{"attachments":{},"cell_type":"markdown","id":"59ee93c0-a59f-4ecd-a7be-28508c35fbaa","metadata":{"deletable":false,"editable":false},"source":["Finally we run the training. This may take a few seconds, so wait until it says `Finished`. "]},{"cell_type":"code","execution_count":null,"id":"5410a814-e24b-4707-9a3e-d6c2a364541e","metadata":{},"outputs":[],"source":["n_episodes = 1000\n","alpha_theta = 2**(-13)\n","policy = PolicySC()\n","valuefunc = ValueFunctionSC()\n","agent = REINFORCE(policy, valuefunc, alpha_theta=alpha_theta, baseline=False)\n","total_reward, truncated = train(env_train, agent, n_episodes)\n","print('Finished')"]},{"attachments":{},"cell_type":"markdown","id":"8d0ddd38-871b-497a-9f49-50b0c4de74d9","metadata":{"deletable":false,"editable":false},"source":["We now plot the total reward received in every episode. The yellow dashed line shows the optimal average reward. Note that you can higher total reward than the optimal average reward in individual episodes but on average it is not possible to do better. "]},{"cell_type":"code","execution_count":null,"id":"5e8041c4-f8e8-4d4d-af1d-7451b2f382fa","metadata":{},"outputs":[],"source":["plt.plot(total_reward) \n","plt.axhline(y=-11.6, color='y', linestyle='-.', label='v*(s0)')\n","plt.xlabel('Episode')\n","plt.ylabel('G')\n","plt.legend(loc='best')\n","plt.ylim(-100, 0)\n","plt.grid(True)"]},{"attachments":{},"cell_type":"markdown","id":"fb86d638-8eed-4538-8af4-4c170d892bac","metadata":{"deletable":false,"editable":false},"source":["It may also be of interest to see the what probabilities the agent has learned for each action. "]},{"cell_type":"code","execution_count":null,"id":"b021fc02-2ce6-4e61-841b-74fd01c96e7a","metadata":{},"outputs":[],"source":["print('Estimated theta:', agent.policy.theta)\n","print('Probability for LEFT: %.2f' % ( agent.policy.a_probs(0)[LEFT]))\n","print('Probability for RIGHT: %.2f' % ( agent.policy.a_probs(0)[RIGHT]))"]},{"attachments":{},"cell_type":"markdown","id":"8817f067-70a4-43a3-82d4-832949998b58","metadata":{"deletable":false,"editable":false},"source":["This can be compared with the initial probabilities (0.95 for `LEFT` and 0.05 for `RIGHT`) and the optimal probabilities (0.41 for `LEFT` and 0.59 for `RIGHT`)."]},{"attachments":{},"cell_type":"markdown","id":"b4ac4969-63e8-40cf-96c3-dd42fc68491b","metadata":{"deletable":false,"editable":false},"source":["__Task__: \n","1. Re-run the code cells above a few times to see how the results varies, both when it comes to rewards and the estimated probabilities after 1000 time steps. \n","\n","2. Try to change the step-length $\\alpha_\\theta$. Try at least $\\alpha_\\theta = 2^{-14}, 2^{-13}, 2^{-12}, 2^{-9}$. For each choice, re-run the code a few times. \n","\n","3. Compare your results with that of Figure 13.1 in the textbook. Note that the results in the textbook show the average results over 100 independent runs. Can you explain why $\\alpha = 2^{-12}$ gives much worse average result than $2^{-13}$ and $2^{-14}$ in the figure? (The remark below may be relevant when you reason about this).\n","\n","4. It may also be of interest to train for more episodes. Try e.g. using $\\alpha_\\theta = 2^{-14}$ with 10 000 episodes. (Note, even if you find the optimal probabilities, the total reward in each episode is stochastic. The yellow line shows the best you can to on average).\n","\n","__Remark:__ We have tested to run this experiment 1000 times with different $\\alpha_\\theta$. With $\\alpha_\\theta = 2^{-12}$ an episode was truncated in 40 of these runs.  With $\\alpha=2^{-13}$ there was a truncated episode in only 5 of the runs. With $2^{-14}$ no episode was truncated. \n"]},{"attachments":{},"cell_type":"markdown","id":"e3cfe1d2-d980-4726-bae0-a82860635da6","metadata":{"deletable":false,"editable":false},"source":["## 3.2 With baseline <a id=\"sec3_2\">"]},{"attachments":{},"cell_type":"markdown","id":"b7c90040-81a8-47c7-9d0e-e79e393e433d","metadata":{"deletable":false,"editable":false},"source":["We will again train in the `ShortCorridor-v0`-environment."]},{"cell_type":"code","execution_count":null,"id":"21a0a769-8819-4c5e-b384-c327903e209f","metadata":{},"outputs":[],"source":["env_train = gym.make('ShortCorridor-v0')"]},{"cell_type":"code","execution_count":null,"id":"423afc63-8bca-4026-be13-26f8964caab1","metadata":{},"outputs":[],"source":["n_episodes = 1000\n","alpha_theta = 2**(-9)\n","alpha_w = 2**(-6)\n","policy = PolicySC()\n","valuefunc = ValueFunctionSC()\n","agent = REINFORCE(policy, valuefunc, alpha_theta=alpha_theta, baseline=True)\n","total_reward, truncated = train(env_train, agent, n_episodes)\n","print('Finished')"]},{"attachments":{},"cell_type":"markdown","id":"beaf9845-eddc-4afc-b629-a130514acaf4","metadata":{"deletable":false,"editable":false},"source":["Plotting the rewards per episode"]},{"cell_type":"code","execution_count":null,"id":"d477ef78-2fa1-4f55-a5a2-31e22f5f28eb","metadata":{},"outputs":[],"source":["plt.plot(total_reward) \n","plt.axhline(y=-11.6, color='y', linestyle='-.', label='v*(s0)')\n","plt.xlabel('Episode')\n","plt.ylabel('G')\n","plt.legend(loc='best')\n","plt.ylim(-100, 0)\n","plt.grid(True)"]},{"cell_type":"code","execution_count":null,"id":"77def1a8-bc1c-48c7-8da4-067d6cef10ba","metadata":{},"outputs":[],"source":["print('Estimated theta:', agent.policy.theta)\n","print('Probability for LEFT: %.2f' % ( agent.policy.a_probs(0)[LEFT]))\n","print('Probability for RIGHT: %.2f' % ( agent.policy.a_probs(0)[RIGHT]))"]},{"attachments":{},"cell_type":"markdown","id":"c0c0ce5a-49f0-4f06-b155-d91f3d907354","metadata":{"deletable":false,"editable":false},"source":["__TASK__:\n","\n","1. Experiment with the step sizes. And number of episodes. Compare your results with Figure 13.2 in the textbook (note that this one shows the average over 100 runs)"]},{"attachments":{},"cell_type":"markdown","id":"e271ee4c-2808-4ff6-bf87-7568a65a146d","metadata":{"deletable":false,"editable":false},"source":["# 4. One-step Actorc-Critic <a id=\"sec4\">"]},{"attachments":{},"cell_type":"markdown","id":"0710193b-b010-43bb-9280-e60555d10811","metadata":{"deletable":false,"editable":false},"source":["We will now implement a one-step actor-critic agent. See slides in Lecture 9 or the textbook Chapter 13.5 (note, we here use $\\gamma = 1$)."]},{"attachments":{},"cell_type":"markdown","id":"24757c27-1611-46dd-808a-2ce6b5040d50","metadata":{"deletable":false,"editable":false},"source":["__TASK__: Implement the learn function below. Note that we will still use the classes `PolicySC` and `ValueFunctionSC` for parameterization of the policy and value function."]},{"cell_type":"code","execution_count":null,"id":"67baa983-d8d8-41da-8768-dc67c9a74901","metadata":{},"outputs":[],"source":["class ActorCritic:\n","    \n","    def __init__(self, policy, valuefunc, alpha_theta=(2**-13), alpha_w = (2**-6)):\n","        \n","        self.alpha_theta = alpha_theta\n","        self.alpha_w = alpha_w\n","        \n","        self.policy = policy\n","        self.valuefunc = valuefunc\n","        \n","    def act(self, state):\n","        return self.policy.act(state)\n","        \n","    def learn(self, state, action, reward, state_next, terminated):\n","        \n","        # TODO Implement the learn method for actor-critic\n","\n","\n","\n","        \n","\n"]},{"attachments":{},"cell_type":"markdown","id":"29a78c1d-b0f1-4090-8331-c33c9c2b3dc9","metadata":{"deletable":false,"editable":false},"source":["Since we do not have to wait until the end of the episode when we use the one-step actor-critic, we need a new `trainAC` function for training the agent. "]},{"cell_type":"code","execution_count":null,"id":"e93039ab-b0ac-4f24-88dc-f29eecbd8219","metadata":{},"outputs":[],"source":["def trainAC(env, agent, n_episodes):\n","    \n","    total_reward = -1000*np.ones(n_episodes)\n","    print('Episode 1')\n","    for i in range(n_episodes):\n","        \n","        if ((i+1) % 100) == 0:\n","            clear_output(wait=True)\n","            print('Episode', i+1)\n","            \n","        state, info = env.reset()\n","        \n","        terminated = truncated = False\n","        total_reward[i] = 0\n","        while not terminated and not truncated:\n","            action = agent.act(state)\n","            state_next, reward, terminated, truncated, info = env.step(action)\n","            \n","            total_reward[i] += reward\n","            \n","            agent.learn(state, action, reward, state_next, terminated)\n","            state = state_next\n","            \n","        if truncated:\n","            print('An episode was truncated, so the training stop at episode', i)\n","            return (total_reward, True)\n","        \n","    return (total_reward, False)"]},{"attachments":{},"cell_type":"markdown","id":"d066dded-8abe-44b6-b4f1-27a6c84f3363","metadata":{"deletable":false,"editable":false},"source":["Let us now train the actor-critic agent"]},{"cell_type":"code","execution_count":null,"id":"1af1c387-98d2-4ee6-8646-d3ca5e10e995","metadata":{},"outputs":[],"source":["env_train = gym.make('ShortCorridor-v0')"]},{"cell_type":"code","execution_count":null,"id":"5d44b518-e446-42f1-bd96-271eeebc66a5","metadata":{},"outputs":[],"source":["n_episodes = 1000\n","alpha_theta = 2**(-13)\n","alpha_w = 2**(-6)\n","policy = PolicySC()\n","valuefunc = ValueFunctionSC()\n","agent = ActorCritic(policy, valuefunc, alpha_theta=alpha_theta, alpha_w = alpha_w)\n","total_reward, truncated = trainAC(env_train, agent, n_episodes)\n","print('Finished')"]},{"cell_type":"code","execution_count":null,"id":"a1794e35-741a-4ab3-a909-81e2ee4fa920","metadata":{},"outputs":[],"source":["plt.plot(total_reward) \n","plt.axhline(y=-11.6, color='y', linestyle='-.', label='v*(s0)')\n","plt.xlabel('Episode')\n","plt.ylabel('G')\n","plt.legend(loc='best')\n","plt.ylim(-100, 0)\n","plt.grid(True)"]},{"cell_type":"code","execution_count":null,"id":"494550f7-4faf-423d-95a8-590c2979e40a","metadata":{},"outputs":[],"source":["print('Estimated theta:', agent.policy.theta)\n","print('Probability for LEFT: %.2f' % ( agent.policy.a_probs(0)[LEFT]))\n","print('Probability for RIGHT: %.2f' % ( agent.policy.a_probs(0)[RIGHT]))"]},{"attachments":{},"cell_type":"markdown","id":"72a1442d-41f0-4f4f-ab6a-dad6511e78cc","metadata":{"deletable":false,"editable":false},"source":["__TASK__: Try different step sizes, compare with results from REINFORCE with baseline. Remember to re-run several times to see how the results vary."]},{"cell_type":"code","execution_count":5,"id":"5752e685","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Softmax with theta = [1 0] and x = [1 1] : [0.73105858 0.26894142]\n","New theta: [2.61364853 4.38635147]\n","Softmax with theta = [2.61364853 4.38635147] and x = [1 1] : [0.14520651 0.85479349]\n"]}],"source":["# Values of softmax with certain paramerters\n","def softmax(x, theta):\n","    return np.exp(x*theta)/np.sum(np.exp(x*theta))\n","\n","theta = np.array([1, 0])\n","x = np.array([1, 1])\n","\n","print('Softmax with theta =', theta, 'and x =', x, ':', softmax(x, theta))\n","\n","reward = 6\n","theta = theta + 1 * reward * (x - softmax(x, theta))\n","print('New theta:', theta)\n","print('Softmax with theta =', theta, 'and x =', x, ':', softmax(x, theta))"]},{"cell_type":"code","execution_count":22,"id":"6d45a7a9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x:  [1 0]\n","derivative1:  [ 0.5 -0.5]\n","New theta: [ 1.5 -1.5]\n","Softmax with theta = [ 1.5 -1.5] and x = [1 0] : [0.95257413 0.04742587]\n","x:  [0 1]\n","derivative1:  [-0.81757448  0.81757448]\n","New theta: [-3.40544686  3.40544686]\n","Softmax with theta = [-3.40544686  3.40544686] and x = [0 1] : [0.0011005 0.9988995]\n"]}],"source":["theta = np.array([0, 0])\n","x = np.array([1, 0])\n","x2 = np.array([0, 1])\n","\n","reward = 3\n","print(\"x: \", x)\n","derivative =  x - (softmax(x, theta)*x + softmax(x2, theta)*x2)\n","\n","print(\"derivative1: \", derivative)\n","\n","theta = theta + 1 * reward * derivative\n","print('New theta:', theta)\n","print('Softmax with theta =', theta, 'and x =', x, ':', softmax([1,1], theta))\n","\n","reward = 6\n","x = np.array([0, 1])\n","x2 = np.array([1, 0])\n","print(\"x: \", x)\n","derivative =  x - (softmax(x, theta)*x + softmax(x2, theta)*x2)\n","\n","print(\"derivative1: \", derivative)\n","\n","theta = theta + 1 * reward * derivative\n","print('New theta:', theta)\n","print('Softmax with theta =', theta, 'and x =', x, ':', softmax([1,1], theta))\n","\n","# probably something wrong with my softmax function...\n"]},{"cell_type":"code","execution_count":15,"id":"0665ed9c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["W1: [8.1 8.1]\n","theta1: [-3.6  0. ]\n","[-2.57412066e-06  5.14824132e-06]\n","W2: [ -4.77 -17.64]\n","theta2: [-3.59997055e+00 -5.88958807e-05]\n"]}],"source":["# Assignment 4 Part 2\n","w = np.array([0, 0])\n","w = w + 0.9 * 9 * np.array([1, 1])\n","print(\"W1:\", w)\n","\n","theta = np.array([0, 0])\n","theta = theta + 0.8 * 9 * (np.array([1, 1]) - 0.5* (np.array([1, 1]) + np.array([2, 1])))\n","\n","print(\"theta1:\", theta)\n","\n","G = 10\n","v = np.array([1, 2]) @ w\n","delta = 10 - v\n","\n","w = w + 0.9 * delta * (np.array([1, 2]))\n","pi1 = np.exp(w[0]) / (np.exp(w[0]) + np.exp(w[1]))\n","pi2 = np.exp(w[1]) / (np.exp(w[0]) + np.exp(w[1]))\n","\n","theta = theta + 0.8 * delta *  (np.array([1, 4]) - (pi1 * np.array([1, 4]) + pi2 * np.array([2, 2])))\n","print( (np.array([1, 4]) - (pi1 * np.array([1, 4]) + pi2 * np.array([2, 2]))))\n","\n","print(\"W2:\", w)\n","print(\"theta2:\", theta)\n"]},{"cell_type":"code","execution_count":9,"id":"dac589c8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["W: [ -4.77 -17.64]\n","theta: [ -3.6  -11.44]\n","pi_right: 1.1570236974691711e-10\n","pi_left: 0.9999999998842976\n","ln_grad_pi: [2.22044605e-16 2.00000000e+00]\n"]}],"source":["# Assignment 4 Part 2 Continuation\n","# Constants\n","alpha_theta = 0.8\n","alpha_w = 0.9\n","\n","# Set start weights\n","w = np.array([0, 0])\n","theta = np.array([0, 0])\n","\n","# Starting x and z (when at state 1)\n","s = 1\n","x = np.array([1, s]) # [1, s]\n","z_right = np.array([s, s**2]) # [s, s^2]\n","z_left = np.array([2, s]) # [2, s]\n","pi_right = np.exp(w @ z_right) / (np.exp(w @ z_right) + np.exp(w @ z_left))\n","pi_left = np.exp(w @ z_left) / (np.exp(w @ z_right) + np.exp(w @ z_left))\n","\n","# Set gradients\n","grad_w = np.array([1, s])\n","ln_grad_pi = z_right - (pi_right * z_right + pi_left * z_left)\n","\n","# Set values of G and delta\n","G = -1 + 10\n","delta = G - (x @ w)\n","\n","# Update weights\n","w = w + alpha_w * delta * grad_w\n","theta = theta + alpha_theta * delta * ln_grad_pi\n","\n","# Step two\n","# Starting x and z (when at state 2)\n","s = 2\n","x = np.array([1, s]) # [1, s]\n","z_right = np.array([s, s**2]) # [s, s^2]\n","z_left = np.array([2, s]) # [2, s]\n","pi_right = np.exp(theta @ z_right) / (np.exp(theta @ z_right) + np.exp(theta @ z_left))\n","pi_left = np.exp(theta @ z_left) / (np.exp(theta @ z_right) + np.exp(theta @ z_left))\n","\n","# Set gradients\n","grad_w = np.array([1, s])\n","ln_grad_pi = z_right - (pi_right * z_right + pi_left * z_left)\n","\n","# Set values of G and delta\n","G = 10\n","delta = G - (x @ w)\n","\n","# Update weights\n","w = w + alpha_w * delta * grad_w\n","theta = theta + alpha_theta * delta * ln_grad_pi\n","\n","print(\"W:\", w)\n","print(\"theta:\", theta)\n","\n","# Pi left and right\n","pi_right = np.exp(theta @ z_right) / (np.exp(theta @ z_right) + np.exp(theta @ z_left))\n","pi_left = np.exp(theta @ z_left) / (np.exp(theta @ z_right) + np.exp(theta @ z_left))\n","\n","print(\"pi_right:\", pi_right)\n","print(\"pi_left:\", pi_left)\n","\n","\n","# Set gradients\n","grad_w = np.array([1, s])\n","ln_grad_pi = z_right - (pi_right * z_right + pi_left * z_left)\n","print(\"ln_grad_pi:\", ln_grad_pi)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ba474c43","metadata":{},"outputs":[],"source":["# Assignment 4 Part 3\n","\n","s = 2\n","x_mu = np.array([1, s, s**3]) # [1, s, s^3]\n","x_sigma = np.array([s - 1, (s-1)**2])\n","theta_mu = np.array([3, 3, 3])\n","theta_sigma = np.array([1, 1])\n","\n","mu = theta_mu @ x_mu\n","sigma = np.exp(theta_sigma @ x_sigma)\n","\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":5}
