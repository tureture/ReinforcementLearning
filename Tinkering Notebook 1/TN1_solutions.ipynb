{"cells": [{"cell_type": "markdown", "id": "2dcbc2c4-bbaa-430a-aed9-5dfcd6e462cd", "metadata": {"editable": false, "deletable": false}, "source": ["# Tinkering Notebook 1 - Introduction\n", "\n", "In this notebook we will first introduce Gymnasium, a package that will be used throughout the course. \n", "\n", "In the last part you will implement your first RL-algorithm to solve the `MultiarmedBandit` problem. "]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# Table of content\n", "* ### [1. Imports](#sec1)\n", "* ### [2. Some important concepts in RL](#sec2)\n", "* ### [3. Going to the Gymnasium](#sec3)\n", "* ### [4. The Taxi environment](#sec4)\n", " * #### [4.1 Alternative rendering modes](#sec4_1)\n", " * #### [4.2 Driving the Taxi](#sec4_2)\n", "* ### [5. The MountainCar](#sec5)\n", "* ### [6. Multi-armed bandits](#sec6)\n", " * #### [6.1 The environment](#sec6_1)\n", " * #### [6.2 Learn](#sec6_2)\n", " * #### [6.3 Act and explore](#sec6_3)\n", " * #### [6.4 Training the agent](#sec6_4)\n"]}, {"cell_type": "markdown", "id": "78176752-499f-40a2-a4e3-8cab36a26542", "metadata": {"editable": false, "deletable": false}, "source": ["# 1. Imports <a id=\"sec1\">"]}, {"cell_type": "markdown", "id": "3892e517-647c-4472-a215-fde8c7f3383b", "metadata": {"editable": false, "deletable": false}, "source": ["In this notebook the following packages will be used. "]}, {"cell_type": "code", "execution_count": null, "id": "4130f358-5d91-4597-8e5c-c4cf14210b15", "metadata": {}, "outputs": [], "source": ["import gymnasium as gym\n", "import gym_RLcourse \n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."]}, {"cell_type": "markdown", "id": "f99bc554-c85b-4990-b222-a008190ac4f3", "metadata": {"editable": false, "deletable": false}, "source": ["# 2. Some important concepts in RL <a id=\"sec2\">"]}, {"cell_type": "markdown", "id": "3e902c92-8eb5-45dc-b51c-264bb8dfcf3c", "metadata": {"editable": false, "deletable": false}, "source": ["RL is a family of modern machine learning techniques for learning how to make sequential decisions using feedback from real and/or simulated environments.\n", "\n", "In RL the agent (e.g. computer program) interacts with an environment and gets rewards. The environment could be a physical or chemical system, resource management, traffic light control, advertisement network, a computer game or many other things. The goal of the agent is typically to maximize the cumulative sum of rewards over some number of sequential actions. In order to do so, the agent learns from observations in order to improve its future actions.\n", "\n", "Some important concepts, that we will explore in this notebook, are\n", "* __Agent__: The learner and decision maker.\n", "* __Environment__:  What the agent interacts with.\n", "* __State__: A state $s \\in \\mathcal{S}$ is a succinct representation of the environments current state.\n", "* __Action__: The agent can take actions $a \\in \\mathcal{A}$ in order to change the state of the environment.\n", "* __Observation__: After each action the agent receives an observation of the environment. For most of the course we will assume that the agent observes the state $s$.\n", "* __Policy__: Rules for how the agent chooses the next action given the current state, $a = \\pi(s)$.\n", "* __Reward__: An immediate reward $R(s,a)$ that the agent gets for taking action $a$ in state $s$."]}, {"cell_type": "markdown", "id": "fee5452c-93fa-4f2f-86d4-769bbacab3de", "metadata": {"editable": false, "deletable": false}, "source": ["# 3. Going to the Gymnasium <a id=\"sec3\">"]}, {"cell_type": "markdown", "id": "b2a9f6a9-510d-42d4-9b07-9e4abedacb4d", "metadata": {"editable": false, "deletable": false}, "source": ["In RL the agent typically needs to interact with the environment in order to learn how to act. That is, unlike in supervised learning, the agent must be able to train by trying out different actions and see how the environment react. \n", "\n", "This is what the [Gymnasium](https://gymnasium.farama.org/) is for. It is a Python-package that contains a wide variety of environments in which you can test different RL-algorithms. The package [RLcourse](https://github.com/magni84/gym_RLcourse) adds some extra environments that implement different examples from the textbook. \n", "\n", "We will make use of Gymnasium throughout the notebooks in this course. To learn about some basic usage, and test that your installation works as it should, we will take a look at the relatively simple Taxi environment. "]}, {"cell_type": "markdown", "id": "e0a85577-9c10-4790-93ce-37738e39c39e", "metadata": {"tags": [], "editable": false, "deletable": false}, "source": ["# 4. The Taxi environment <a id=\"sec4\">"]}, {"cell_type": "markdown", "id": "33923e8d-418c-4899-8ff5-95bb180bb0aa", "metadata": {"editable": false, "deletable": false}, "source": ["To get an environment object that we can interact with use `gym.make()'"]}, {"cell_type": "code", "execution_count": null, "id": "9fa316cb-faed-4fc1-a863-d52110742e87", "metadata": {}, "outputs": [], "source": ["env = gym.make(\"Taxi-v3\", render_mode=\"human\")"]}, {"cell_type": "markdown", "id": "56662345-a5e6-4692-856f-bd70d0f86447", "metadata": {"editable": false, "deletable": false}, "source": ["`render_mode` specifies how the state of the environment should be visualized. For most environments, using `human` renders the environment graphically in a separate window. Furthermore it will automatically update the visualization whenever the environments state changes.\n", "\n", "Ok, so now we have created an environment object. To initialize the environment we use `reset()`. "]}, {"cell_type": "code", "execution_count": null, "id": "75825dee-ea3f-45a8-8fc4-b523dd00a6f8", "metadata": {}, "outputs": [], "source": ["state, info = env.reset()\n", "print(\"Initial state:\", state)\n", "print(\"Info:\", info)"]}, {"cell_type": "markdown", "id": "958c1043-6c55-4072-b4f8-7fa0b6b794ad", "metadata": {"editable": false, "deletable": false}, "source": ["`reset()` initialize the environment and returns the initial state (as well as some information mainly used for debugging). If you use `render_mode = \"human\"` this will also open a window showing the current state of the environment. (See below for how to use alternative rendering modes.)\n", "\n", "To close this window, you must use `env.close()`. If you accidentally overwrite `env` before closing it, the window will not be possible to close until you either `make` and `reset` a new environment with `render_mode=\"human\"` or you restart your Jupyter kernel."]}, {"cell_type": "code", "execution_count": null, "id": "0f9133c9-78d5-4432-84cf-3f90cc127f03", "metadata": {}, "outputs": [], "source": ["env.close()"]}, {"cell_type": "markdown", "id": "b13bf5b7-5170-42a9-bd80-c0c40d766566", "metadata": {"editable": false, "deletable": false}, "source": ["## 4.1 Alternative rendering modes <a id=\"sec4_1\">"]}, {"cell_type": "markdown", "id": "04ede35e-cbaf-4c28-93c1-67118212f7ff", "metadata": {"editable": false, "deletable": false}, "source": ["As mentioned above, most environments allow some different rendering modes. The Taxi-environment for example also has the modes `ansi` and `rgb_array`. \n", "\n", "With `ansi` you get a formatted text string that you can print for a visual representation. "]}, {"cell_type": "code", "execution_count": null, "id": "902758d7-29d0-4b5a-a4dd-7f664a99af48", "metadata": {}, "outputs": [], "source": ["env = gym.make(\"Taxi-v3\", render_mode=\"ansi\")\n", "state, info = env.reset()\n", "rep = env.render()\n", "print(rep)"]}, {"cell_type": "markdown", "id": "1d477d10-cb63-4b51-bec1-3ca8e8d0b5c5", "metadata": {"editable": false, "deletable": false}, "source": ["With `rgb_array` you get an RGB array representing the image. With this you can plot the visual representation yourself using e.g. `matplotlib`. This can be useful if you for some reason can't open an external window (e.g. when you use Google Colab) or if you do not need to update the rendering every time the state changes (as `human` does)."]}, {"cell_type": "code", "execution_count": null, "id": "77d1b971-417b-4034-95d5-183344e941c8", "metadata": {}, "outputs": [], "source": ["env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n", "state, info = env.reset()\n", "rep = env.render()\n", "plt.imshow(rep)"]}, {"cell_type": "markdown", "id": "57eb715c-46f9-4f5b-a8b4-0f2b5629106c", "metadata": {"editable": false, "deletable": false}, "source": ["Later when we start training agents, we typically do not want to render the environment while training the agent since this slows down the training process. In such case you can just `make` the environment without specifying a `render_mode`. "]}, {"cell_type": "markdown", "id": "0a8eafc6-8cb4-4f4b-b0fc-e6d9931ce209", "metadata": {"editable": false, "deletable": false}, "source": ["## 4.2 Driving the Taxi <a id=\"sec4_2\">"]}, {"cell_type": "markdown", "id": "da5bb668-40f5-4303-972d-9139c24d3d69", "metadata": {"editable": false, "deletable": false}, "source": ["So lets try to play around with the Taxi."]}, {"cell_type": "code", "execution_count": null, "id": "af61b359-ec5b-4b1d-805e-88754dc6cd89", "metadata": {}, "outputs": [], "source": ["env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n", "state, info = env.reset()\n", "print(\"Initial state:\", state)"]}, {"cell_type": "markdown", "id": "3fb28e89-258d-421f-9d92-cdfdbfaab27c", "metadata": {"editable": false, "deletable": false}, "source": ["The objective in this environment is for the taxi to first drive to the passenger and pick her up, then drive her to the destination and drop her off. An RL-agent should thus find a policy $a = \\pi(s)$ that for a given state $s$ returns an action $a$ in such a way that the objective is achieved.  \n", "\n", "The set of all possible states is called the __state space__ $\\mathcal{S}$, and the set of all possible actions is called the __action space__ $\\mathcal{A}$. Lets see how these looks for the Taxi-environment."]}, {"cell_type": "code", "execution_count": null, "id": "1b612efc-12d9-4008-beb3-910f5f28bfcb", "metadata": {}, "outputs": [], "source": ["print(\"State space:\", env.observation_space)\n", "print(\"Action space:\", env.action_space)"]}, {"cell_type": "markdown", "id": "fa25303a-9f10-472e-a043-7b82b9261ed1", "metadata": {"editable": false, "deletable": false}, "source": ["__State space__: The state space contains 500 discrete states, so each state is a number between 0 and 499. Here each state corresponds to a position of the taxi (25 possibilities), the passengers position (5 possibilities, including picked up) and the destination (4 possibilities). Hence, there are $25\\times5 \\times 4 = 500$ possible states.\n", "\n", "***Remark***: Note that we actually asked for the `observation_space` and not the state space. As mentioned above, we will for most of the course assume that the observation space and the state space are the same. However, in some RL-problems the full state cannot be observed, so the space of possible states may not be the same as the space of possible observations. For example, the complete state of an inverted pendulum consists of both the angle and angular velocity, but maybe only the angle is measured directly.\n", "\n", "__Action space__: The action space contains 6 discrete actions, so each action should be a number between 0 and 5. In the Taxi-environment, each action can be interpreted as follows:"]}, {"cell_type": "code", "execution_count": null, "id": "0c506171-92a7-47e7-b637-ff5af61ab392", "metadata": {}, "outputs": [], "source": ["DOWN = 0\n", "UP = 1\n", "RIGHT = 2\n", "LEFT = 3\n", "PICKUP = 4\n", "DROPOFF = 5"]}, {"cell_type": "markdown", "id": "b9f0380f-f8a8-4ce5-96ce-7b10aaf6de8f", "metadata": {"editable": false, "deletable": false}, "source": ["Note however, that when the RL-agent starts to learn, it will not know the meaning of each state and action, instead it has to try out different actions to see how this changes the state. \n", "\n", "Let us now try to move the Taxi:"]}, {"cell_type": "code", "execution_count": null, "id": "b8e4a24e-89b2-4ca4-9424-42f489eb1bf3", "metadata": {}, "outputs": [], "source": ["state, reward, terminated, truncated, info = env.step(UP) \n", "print(\"New state:\",state)\n", "print(\"Reward:\", reward)\n", "print(\"Terminated:\", terminated)\n", "print(\"Truncated:\", truncated)\n", "print(\"Info:\", info)"]}, {"cell_type": "markdown", "id": "0431f022-b669-4c0b-87e5-dcc9524318ff", "metadata": {"editable": false, "deletable": false}, "source": ["If it was possible, the taxi should now have moved one step up (if it started at the top row it will not move). \n", "\n", "The `step`-function returns the following information:\n", "\n", "* __state__: The new state of the environment.\n", "* __reward__: The immediate reward. In the taxi-environment the reward for illegal \"pickup\" or \"dropoff\" is -10, successfully delivering the passenger gives +20, and any other action gives -1. To maximize the total reward you must thus deliver the passenger with as few actions as possible. \n", "* __terminated__: There are two different ways a Gymnasium environment can finish. The first is that it terminates due to the fact that we reach a terminal state. In the Taxi environment there is only one terminal state, namely the state where the passenger has been dropped off at the correct destination. \n", "* __truncated__: In Gymnasium environments there may also be a fixed time limit that is not actually part of the environment (part of the Markov Decision Process). If an environment finishes due to this type of time limit, then truncated is set to True.\n", "* __info__: Just as `reset()`, `step()` also return some info that is mainly used for debugging purposes."]}, {"cell_type": "markdown", "id": "6b29110c-49b7-4b94-a697-2ab6c85cfb8d", "metadata": {"editable": false, "deletable": false}, "source": ["So how can we solve the Taxi problem? We basically need to figure out what the optimal action is in each of the 500 states. With discrete states it is thus possible to create an array with 500 entries, and in each entry we specify the optimal action. \n", "\n", "__In Lecture 3__ we will see how the optimal action can be found using dynamic programming if the possible transitions in the environment are known. \n", "\n", "__In Lecture 4 - Lecture 5__ we will see how the optimal actions can be found even if nothing about the environment is known by letting the agent explore how the environment reacts to different actions. \n", "\n", "For now, lets close the Taxi environment and instead look at a MountainCar."]}, {"cell_type": "code", "execution_count": null, "id": "d34bca4a-aaf1-4d70-9194-391967d462ef", "metadata": {}, "outputs": [], "source": ["env.close()"]}, {"cell_type": "markdown", "id": "adac90c9-054b-4eef-835d-fb9ede7a3e1f", "metadata": {"editable": false, "deletable": false}, "source": ["# 5. The MountainCar <a id=\"sec5\">"]}, {"cell_type": "code", "execution_count": null, "id": "9ec1ae3e-0d56-4228-83bc-d6ebda8587f6", "metadata": {}, "outputs": [], "source": ["env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n", "state, info = env.reset()\n", "print(\"Initial state:\", state)"]}, {"cell_type": "markdown", "id": "34a83049-89c0-4081-aa2c-dbdb9eb5cf57", "metadata": {"editable": false, "deletable": false}, "source": ["In this environment the goal is to get the car to the flag in as few steps as possible. "]}, {"cell_type": "code", "execution_count": null, "id": "d2d23d2b-b1cc-47e0-b312-69587b3e195f", "metadata": {}, "outputs": [], "source": ["print(\"State space:\", env.observation_space)\n", "print(\"Action space:\", env.action_space)"]}, {"cell_type": "markdown", "id": "cf9bfe6b-5b76-46d4-a044-fa4b5668fe44", "metadata": {"editable": false, "deletable": false}, "source": ["*  __Action space__: We have three discrete actions, 0 - push left, 1 - no push, 2 - push right.\n", "* __State space__: `Box` represents a continuous space, in this case with 2 dimensions. The first element of the state is the position of the car and can go between -1.2 and +0.6. The second element of the state is the velocity of the car and can go between -0.07 and +0.07.\n", "* __Reward__: The reward given by the environment is -1 for each action. \n", "* __Terminal state__: The environments terminates if a state with position greater than or equal to 0.5 is reached (0.5 is the position of the flag). Thus, to maximize total reward the agent should reach the flag in as few steps as possible. \n", "* __Truncation__: If a terminal state is not reached within 200 time steps, the environment will stop anyway. Note that this is not actually part of the environment, it is just to ensure that the environment will stop even if a policy that never reaches a terminal state is used. \n", "\n", "Lets first try to solve this environment by the simple policy: Always push the car right (action = 2)."]}, {"cell_type": "code", "execution_count": null, "id": "6af1bf78-01bd-4ccc-b5cb-01dd6c210907", "metadata": {}, "outputs": [], "source": ["def policy(state):\n", "    # Possible solution \n", "    # If car moves left, push left \n", "    # If car moves right, push right \n", "    if state[1] <= 0: \n", "        action = 1 \n", "    else: \n", "        action = 2 \n", "        \n", "    return action  "]}, {"cell_type": "code", "execution_count": null, "id": "8a5bad40-bdc8-4c94-a86f-c00cb5a10af3", "metadata": {}, "outputs": [], "source": ["env = gym.make('MountainCar-v0', render_mode=\"human\")\n", "state, info = env.reset()\n", "time_step = 0\n", "total_reward = 0\n", "terminated = False\n", "truncated = False\n", "while not terminated and not truncated:\n", "    action = policy(state)\n", "    state, reward, terminated, truncated, info = env.step(action)\n", "    total_reward += reward\n", "    env.render()\n", "    \n", "    clear_output(wait=True)\n", "    print(\"State:\", state)\n", "    print(\"Action:\", action)\n", "    print(\"Total reward:\", total_reward)\n", "    \n", "env.close()"]}, {"cell_type": "markdown", "id": "d7f5747d-076c-4940-af5d-7a1a2f314022", "metadata": {"tags": [], "editable": false, "deletable": false}, "source": ["Unfortunately the car does not have the momentum to overcome gravity, and thus it never reaches the flag. The agent must learn that the car needs to gain momentum by going up the left hill."]}, {"cell_type": "markdown", "id": "7444f95e-efcd-4b50-9c0f-2d5dc23e2e17", "metadata": {"editable": false, "deletable": false}, "source": ["This environment is harder than the Taxi-enivronment for RL-agents. There are two reasons for this:\n", "1. The state-space is continuous (infinitely many states). Hence, it is not possible to build up a table with information about all states. In RL this can be solved using function approximations. This will be discussed in the second half of the course.\n", "\n", "2. The agent always gets the immediate reward -1 for each action until it reaches the flag. So all actions looks equally bad if we do not reach the flag before the environment truncates at 200 steps, and it is very unlikely that the car reaches the flag by only using e.g. random actions. So how can it learn about the goal? We will see two possible solutions in the course: We can try to re-engineer the reward so that the agent is encouraged to go up the left slope first. But this requires that we already knew that this is a good idea. More generally, we can encourage the agent to try to reach states that has not been seen before.\n", "\n", "Even though the problem is hard for an RL-agent that knows nothing about the environment, it is relatively easy for an engineer that knows the basics of the environment to find a policy that (most of the time) reaches the flag within 200 steps. The strength of RL is that the agent can learn this only by interacting with the environment. However, when we know more about the environment it can still be much more efficient to let an engineer figure out a good policy, and then potentially use RL to improve this policy.\n"]}, {"cell_type": "markdown", "id": "cced594e-6a4d-4d72-b8a7-7d0f81d0e110", "metadata": {"editable": false, "deletable": false}, "source": ["***\n", "### Task:\n", "a) Try to construct a policy that only uses the current velocity (`state[1]`) to determine the action. The goal is to reach the flag within 200 steps most of the time. Implement your policy in the function `policy` above and try it. Remember to execute the cell with the function after you have made your changes. *Hint:* It is actually enough to check in which direction the car is currently moving (if velocity is positive or negative) to reach the flag within 200 time steps most of the time. \n", "\n", "b) Implement the policy that is on the [leaderboard](https://github.com/openai/gym/wiki/Leaderboard) for MountainCar-v0: ($p$ is position,  $v$ is velocity, and the state is $s = \\begin{bmatrix} p & v \\end{bmatrix}^\\top$)\n", "$$\n", "\\pi(s) = \\begin{cases} \\text{Right} & \\text{if } \\min\\{-0.09( p + 0.25)^2 + 0.03, 0.3(p + 0.9)^4 - 0.008\\} \\leq v \\leq -0.07(p+0.38)^2 + 0.07 \\\\ \\text{Left} & \\text{otherwise} \\end{cases}\n", "$$\n", "\n", "***"]}, {"cell_type": "markdown", "id": "5e35fecc-7920-41b6-859c-99451e1c211a", "metadata": {"editable": false, "deletable": false}, "source": ["# 6. Multi-armed bandits <a id=\"sec6\">"]}, {"cell_type": "markdown", "id": "ee6d6b3c-5723-49c0-b8da-75502c21eb78", "metadata": {"editable": false, "deletable": false}, "source": ["This part of the notebook is based on Chapter 2 in the textbook. It will let you play around with some important RL-concepts by trying out a relatively simple RL-algorithm. To get a more complete discussion, it is recommended to read Chapter 2.1-2.6 in the textbook."]}, {"cell_type": "markdown", "id": "30e132a3-e63b-47a7-8a19-c4b4f4c22a65", "metadata": {"editable": false, "deletable": false}, "source": ["## 6.1 The environment <a id=\"sec6_1\">"]}, {"cell_type": "markdown", "id": "679e963a-07d4-4e4b-adf6-78b01e6cc0ce", "metadata": {"editable": false, "deletable": false}, "source": ["We consider 10 slot machines (one-armed bandits). You will repeatedly choose one of the slot machines, and each time you receive a random reward from a stationary distribution that depends on which slot machine you choose."]}, {"cell_type": "code", "execution_count": null, "id": "39a62384-c505-4574-9847-938f41514e59", "metadata": {}, "outputs": [], "source": ["env = gym.make(\"MultiarmedBandits-v0\")\n", "state, info = env.reset()"]}, {"cell_type": "code", "execution_count": null, "id": "ee0bf616-9103-43e9-8352-90dc9bc71eac", "metadata": {}, "outputs": [], "source": ["print(\"State space:\", env.observation_space)\n", "print(\"Action space:\", env.action_space)"]}, {"cell_type": "markdown", "id": "f95cffde-77a1-4204-aeed-28154620ff1a", "metadata": {"editable": false, "deletable": false}, "source": ["__State space__: There is just one state in this environment, since the slot machines do not change due to your actions. This simplifies the RL-problem substantially, since we do not have to take into account how an action affects the state, we can concentrate on what immediate reward each action gives. \n", "\n", "__Action space__: $\\mathcal{A} = \\{ 0, 1, \\ldots, 9\\}$, so action $a=0$ means that we try slot machine $0$, $a=1$ means we try slot machine $1$ etc.\n", "\n", "__Reward__: You get a random $R$ reward depending on the action you choose. Every time you `reset` the environment, the expected reward for each action will change. \n", "\n", "Lets try one of the slot machines:"]}, {"cell_type": "code", "execution_count": null, "id": "95091a43-782c-47d4-8a93-b80351b1d59a", "metadata": {}, "outputs": [], "source": ["action = 0\n", "state, reward, terminated, truncated, info = env.step(action)\n", "print(\"Reward:\", reward)"]}, {"cell_type": "markdown", "id": "9ab8d761-3502-4f15-8604-bdc8f4e88856", "metadata": {"editable": false, "deletable": false}, "source": ["We now define the value of an action, $q_*(a)$, to be the expected reward when action $a$ is chosen, i.e.\n", "\n", "$$\n", "q_*(a) = \\mathbb{E}[ R | a].\n", "$$\n", "\n", "Since there are 10 different actions, we represent $q_*(a)$ as an array with 10 elements in Python. In the `MultiarmedBandits` environment the true values are stored in `env.values`. "]}, {"cell_type": "code", "execution_count": null, "id": "a30df82e-8a09-4dc7-8d15-ee9ebcb99b18", "metadata": {}, "outputs": [], "source": ["print(\"True values:\\n\", env.values)"]}, {"cell_type": "markdown", "id": "9e48fcd8-9ad1-458b-a63d-11fbde79c47b", "metadata": {"editable": false, "deletable": false}, "source": ["The action that maximizes the expected reward is thus the action with largest value. If we know the true values $q_*(a)$ we can find that action with\n", "$$\n", "A = \\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} q_*(a),\n", "$$\n", "or in code"]}, {"cell_type": "code", "execution_count": null, "id": "498c0e8f-4607-4988-9581-6a03a66081f1", "metadata": {}, "outputs": [], "source": ["optimal_action = np.argmax(env.values)\n", "print(\"Optimal action:\", optimal_action)"]}, {"cell_type": "markdown", "id": "c5f3a909-2792-4b41-a08a-89044c7b645b", "metadata": {"editable": false, "deletable": false}, "source": ["Let us now test using the best action and the worst action 1000 times each to see the difference."]}, {"cell_type": "code", "execution_count": null, "id": "a8b6d520-3b95-4042-9b4c-ead7f4295ec4", "metadata": {}, "outputs": [], "source": ["optimal_action = np.argmax(env.values)\n", "worst_action = np.argmin(env.values)\n", "\n", "rewards_optimal = np.zeros(1000)\n", "rewards_worst = np.zeros(1000)\n", "\n", "for i in range(1000):\n", "    state, reward, terminated, truncated, info = env.step(optimal_action)\n", "    rewards_optimal[i] = reward\n", "    \n", "    state, reward, terminated, truncated, info = env.step(worst_action)\n", "    rewards_worst[i] = reward"]}, {"cell_type": "code", "execution_count": null, "id": "c7e826c2-a2e2-4dcf-97ee-15e1bde1a532", "metadata": {}, "outputs": [], "source": ["# Plot the rewards\n", "plt.plot(rewards_optimal, label=\"Optimal action\")\n", "plt.plot(rewards_worst, label=\"Worst action\")\n", "plt.legend()\n", "plt.ylabel(\"Reward\")\n", "\n", "# Print average reward as well as true value \n", "print(\"Optimal action. Average reward: %.2f, True value: %.2f\"\n", "      % (np.mean(rewards_optimal), env.values[optimal_action]))\n", "\n", "print(\"Worst action. Average reward: %.2f, True value: %.2f\"\n", "      % (np.mean(rewards_worst), env.values[worst_action]))"]}, {"cell_type": "markdown", "id": "408d28fb-fbe2-406a-9e23-db1ccf71299c", "metadata": {"editable": false, "deletable": false}, "source": ["So, if we know the true value of each action we can directly find the optimal action. However, the question is how an agent can **learn** what the optimal action is without knowing the true values. \n", "\n", "Note from the results above that if we have tested the two actions 1000 times, the average reward we received is relatively close to the true value, and by looking at the average rewards we can clearly determine which action is better. This idea will be used below to learn the optimal action. "]}, {"cell_type": "markdown", "id": "6babb284-1c4a-4397-a5b5-0e771c5594cc", "metadata": {"editable": false, "deletable": false}, "source": ["## 6.2 Learn <a id=\"sec6_2\">"]}, {"cell_type": "markdown", "id": "f90136df-92c6-499d-99ac-c81e9e8891e3", "metadata": {"editable": false, "deletable": false}, "source": ["To find an estimate of the true values $q_*(a)$ from observations, assume that we have taken $n-1$ actions and observed the reward for each action. To get an estimate $Q_n(a)$ of the true values, we compute the average reward received from action $a$ so far. \n", "\n", "That is, when $n-1$ actions have been taken, we can compute an estimate of $q_*(a)$ as\n", "\n", "$$\n", "Q_n(a) = \\frac{ \\text{ sum of rewards when $a$ taken prior to $n$} }{\\text{number of times $a$  taken prior to $n$}} \\quad \\text{(see eq 2.1 in textbook)},\n", "$$\n", "\n", "where we let $Q_n(a) = 0$ if the action $a$ was not taken prior to the $n$th action. In particular, $Q_1(a) = 0$ for all $a$.\n", "\n", "Instead of recomputing the sums every time a new action is taken, we can update this sums incrementally. Equation (2.3) in the textbook shows how to do this. That is, if the $n$th action taken is $A_n$, and the received reward is $R_n$, then $Q_{n+1}(A_n)$ can be computed from $Q_n(A_n)$ as\n", "\n", "\n", "$$\n", "Q_{n+1}(A_n) = Q_{n}(A_n) + \\frac{1}{N(A_n)}(R_n - Q_n(A_n))\n", "$$\n", "where $N(A_n)$ is the number of times that $A_n$ has been taken so far. For all $a \\neq A_{n}$ we have $Q_{n+1}(a) = Q_{n}(a)$ since we did not receive any new information about this action.\n", "\n", "If we initialize the estimate to $Q_1(a) = 0$ for all $a \\in \\mathcal{A}$, then both expressions for $Q_n(a)$ are equivalent. For implementation the second expression is quite useful. Here we only need to create an array with 10 elements (one for each possible action) and whenever action $a$ is taken and the reward is observed, we update element $a$ in the array. \n", "Pseudo-code for this can be written as\n", "\n", "*Initialize:*\n", "\n", "For all $a \\in \\mathcal{A}$, \n", "  * $\\quad Q(a) \\leftarrow 0$\n", "  * $\\quad N(a) \\leftarrow 0$\n", "  \n", "*Learn:*\n", "\n", "When action $A$ is taken with received reward $R$, update the estimates:\n", "  * $N(A) \\leftarrow N(A) + 1$\n", "  * $Q(A) \\leftarrow Q(A) + \\frac{1}{N(A)} (R - Q(A))$"]}, {"cell_type": "markdown", "id": "e988e34d-4938-4d20-9102-887aa7f21602", "metadata": {"editable": false, "deletable": false}, "source": ["## 6.3 Act and explore <a id=\"sec6_3\">"]}, {"cell_type": "markdown", "id": "c63dc3b9-122f-4417-a458-baadf29e8792", "metadata": {"editable": false, "deletable": false}, "source": ["Given an estimate $Q(a)$, how should the agent choose the next action? \n", "\n", "A straightforward answer is to pick the one that maximizes $Q(a)$. This is in RL called the *greedy* choice, picking the action that according to the current estimates seems best.\n", "\n", "However, assume that we initialize $Q(a)$ to zero for all actions. Lets say that the first action we take is $a=0$ and we receive a positive reward, so now $Q(0) > 0$, while $Q(a) = 0$ for all $a \\neq 0$. The greedy choice is now $a=0$, so we choose this action again. As long as $Q(0)>0$ we will continue to choose $a=0$ and we will never learn about the other actions.\n", "\n", "The problem is that if we always act greedily with respect to the current estimate, we may never explore other possible actions. A simple, but often quite effective, way of adding exploration is to let the agent use the greedy option most of the time, but with probability $\\varepsilon$ take a random action.\n", "This is called an $\\varepsilon$-greedy policy. In this way the agent will always continue to explore different possibilities. The $\\varepsilon$-greedy policy given an estimate $Q(a)$ can be written as\n", "\n", "$$\n", " \\begin{cases} \\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} Q(a) & \\text{with probability } 1-\\varepsilon \\\\ \\text{random action} & \\text{with probability } \\varepsilon\\end{cases}\n", "$$\n", "\n", "When several actions have an estimate equal to the maximum, the agent can break ties any way it wants (e.g. randomly). "]}, {"cell_type": "markdown", "id": "cdd63443-866d-4372-93fc-4d301264901c", "metadata": {"editable": false, "deletable": false}, "source": ["## 6.4 Training the agent <a id=\"sec6_4\">"]}, {"cell_type": "markdown", "id": "0fece867-6ddf-40f2-bc09-05b4160e5178", "metadata": {"editable": false, "deletable": false}, "source": ["Below we define a class `Agent` that does not learn anything and always choose a random action. In the tasks below you will update this code to implement learning and an $\\varepsilon$-greedy policy. "]}, {"cell_type": "code", "execution_count": null, "id": "a7057929-cb31-4f80-8d5c-9b1085f77d4f", "metadata": {}, "outputs": [], "source": ["class Agent():\n", "    \n", "    def __init__(self, epsilon=0, nr_arms=10):\n", "        self.epsilon = epsilon\n", "        self.nr_arms = nr_arms\n", "        self.N = np.zeros(nr_arms)\n", "        self.Q = np.zeros(nr_arms)\n", "        \n", "    def learn(self, action, reward):\n", "        self.N[action] += 1 \n", "        self.Q[action] += 1/self.N[action] * (reward-self.Q[action]) \n", "    \n", "    def act(self):\n", "        if (np.random.rand() > self.epsilon): \n", "            action = np.argmax(self.Q) \n", "        else: \n", "            action = np.random.choice(self.nr_arms) \n", "        return action"]}, {"cell_type": "markdown", "id": "b1f65681-fccb-4d93-a658-d3f31bc8a946", "metadata": {"editable": false, "deletable": false}, "source": ["We now let this agent take 10 000 actions, and compute the total reward. "]}, {"cell_type": "code", "execution_count": null, "id": "cafc0b51-4398-4c2d-b094-d6845f98d648", "metadata": {}, "outputs": [], "source": ["env = gym.make(\"MultiarmedBandits-v0\")\n", "state, info = env.reset()\n", "agent = Agent(epsilon = 0)\n", "nr_actions = 10000\n", "rewards = np.zeros(nr_actions)\n", "for t in range(nr_actions):\n", "    action = agent.act()\n", "    state, reward, terminated, truncated, info = env.step(action)\n", "    rewards[t] = reward\n", "    agent.learn(action, reward)\n", "\n", "print('Total reward:',  np.around(np.sum(rewards),2))\n", "print('True values:\\n', np.around(env.values,2))\n", "print('Estimated values:\\n', np.around(agent.Q,2))"]}, {"cell_type": "markdown", "id": "dd757f7c-20a3-4234-a538-6b5c24cb0e94", "metadata": {"editable": false, "deletable": false}, "source": ["Since the agent does not learn anything, all estimated values will be equal to zero. Your task is now to implement the learning and acting according to the discussion above. "]}, {"cell_type": "markdown", "id": "1fc0a2db-ce85-4c1b-93a7-c1d349fdbfba", "metadata": {"editable": false, "deletable": false}, "source": ["***\n", "### Task\n", "\n", "a) Change the `Agent`-class in order to implement an $\\varepsilon$-greedy agent. To do this, change the `learn`-method to implement the update of $Q$ given an action with corresponding reward. Then implement the $\\varepsilon$-greedy policy in the `act`-function. (See discussion above for pseudo-code). ___Hint:___ `np.random.rand() > self.epsilon` has a probability of `1 - self.epsilon` to be `True`.\n", "\n", "b) Try to re-run the code above a few times with your agent using $\\varepsilon = 1$. In this case, if your code is correct, the estimated values and the true values should be similar (but not exactly equal).\n", " \n", "c) Try to re-run the code above a few times with your agent using $\\varepsilon = 0$. Try to explain the results. \n", "\n", "d) Try to re-run the code above with your agent using $\\varepsilon = 0.01$ and $\\varepsilon =0.5$. Do your estimated values improve as you increase $\\varepsilon$? Why? What is the drawback of increasing $\\varepsilon$?\n", "\n", "e) When you have tested that your implementation work, you can run the code below the reproduce plots like in Figure 2.2 in the textbook. Here we do 2000 episodes (with different bandit problems) where the agent take 1000 actions each episode.The average reward at different time steps is then tested. Try to use different $\\varepsilon$. (Running this code may take some minute). Compare your results with those of Figure 2.2 in the textbook.\n", "***"]}, {"cell_type": "code", "execution_count": null, "id": "628030e3-9bdf-4be9-9219-f0abfdd4be4b", "metadata": {}, "outputs": [], "source": ["epsilon = 0.1 # Change this to try different values\n", "rewards = np.zeros((2000, 1000)) \n", "optimal = np.zeros((2000, 1000)) # Using optimal action?\n", "\n", "for i in range(2000):\n", "    agent = Agent(epsilon = epsilon)\n", "    env.reset()     \n", "    optimal_action = np.argmax(env.values)\n", "    \n", "    for t in range(1000):\n", "        action = agent.act()\n", "        \n", "        if action == optimal_action:\n", "            optimal[i,t] = 1\n", "        else:\n", "            optimal[i,t] = 0\n", "            \n", "        state, reward, terminated, truncated, info = env.step(action)\n", "        rewards[i,t] = reward\n", "        \n", "        agent.learn(action, reward)"]}, {"cell_type": "code", "execution_count": null, "id": "8106f431-6784-4686-a656-c2998d7ff838", "metadata": {}, "outputs": [], "source": ["# Plot average reward in each time step\n", "mean_rewards = np.mean(rewards, 0)\n", "plt.plot(mean_rewards)\n", "plt.xlabel(\"Steps\")\n", "plt.ylabel(\"Average reward\")"]}, {"cell_type": "code", "execution_count": null, "id": "49bd6188-1153-4e81-89d5-f2e296030a29", "metadata": {}, "outputs": [], "source": ["# Plot how often optimal action is choosen in %\n", "mean_optimal = np.mean(optimal,0)\n", "plt.plot(mean_optimal)\n", "plt.xlabel(\"Steps\")\n", "plt.ylabel(\"% Optimal action\")"]}, {"cell_type": "code", "execution_count": null, "id": "7d2b26b8-1931-4f21-b277-de74dd02fe15", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.16"}}, "nbformat": 4, "nbformat_minor": 5}