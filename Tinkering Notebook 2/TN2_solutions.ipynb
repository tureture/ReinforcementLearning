{"cells": [{"cell_type": "markdown", "id": "d8e28032-37dc-4dab-ab18-384ccdbd0d68", "metadata": {"editable": false, "deletable": false}, "source": ["# Tinkering notebook 2: Markov Decision Processes and Dynamic Programming\n", "In this notebook we will see how some of the content of Lecture 2 - Lecture 3 works in practice. \n", "\n", "We will start by a repetition of Markov Decision Processes (MDPs) and value functions. \n", "Then we introduce the two GridWorld environments that will be used in the notebook (Example 4.1 in the textbook and FrozenLake). \n", "\n", "We then use dynamic programming to estimate the value function $v_\\pi(s)$ for a given policy $\\pi$ and then use policy improvement to find the optimal policy. Finally we try value iteration to directly find the optimal value function $v_*(s)$ and the corresponding optimal policy. "]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# Table of content\n", "* ### [1. Imports](#sec1)\n", "* ### [2. Markov Decision Processes and Value Functions](#sec2)\n", " * #### [2.1 The Bellman Equations](#sec2_1)\n", " * #### [2.2 Example: Study or Facebook?](#sec2_2)\n", " * #### [2.3 *Analytical solution to the Bellman equation](#sec2_3)\n", "* ### [3. Helper functions](#sec3)\n", "* ### [4. The environments](#sec4)\n", " * #### [4.1 The Frozen Lake](#sec4_1)\n", "* ### [5. MDPs and the Bellman equations](#sec5)\n", " * #### [5.1 Test your code on Example 4.1 (GridWorld)](#sec5_1)\n", "* ### [6. Policy Evaluation](#sec6)\n", " * #### [6.1 In place updates](#sec6_1)\n", "* ### [7. Policy Iteration](#sec7)\n", "* ### [8. Value iteration](#sec8)\n"]}, {"cell_type": "markdown", "id": "580c1138-82bf-4af0-b413-0a8299cea67b", "metadata": {"editable": false, "deletable": false}, "source": ["# 1. Imports <a id=\"sec1\">"]}, {"cell_type": "code", "execution_count": null, "id": "5f7df59c-4105-44e8-a82e-40dd9458e5a7", "metadata": {}, "outputs": [], "source": ["# Packages needed for this notebook\n", "import gymnasium as gym\n", "import gym_RLcourse\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."]}, {"cell_type": "markdown", "id": "bd0fbf5b-1ddb-42c6-aa20-ff6e34c1e667", "metadata": {"editable": false, "deletable": false}, "source": ["# 2. Markov Decision Processes and Value Functions <a id=\"sec2\">"]}, {"cell_type": "markdown", "id": "023404f2-f844-4b46-80af-ce6a3f40dbf0", "metadata": {"editable": false, "deletable": false}, "source": ["An MDP consist of a state space $\\mathcal{S}$, an action space $\\mathcal{A}$, a reward set $\\mathcal{R}$ and a transition function $p(s', r | s, a)$. We define the return as \n", "$$\n", "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n", "$$\n", "where $\\gamma$ is the discount rate.\n", "\n", "**State-value function:**\n", "$$\n", "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[ G_t | S_t = s]\n", "$$\n", "The expected return starting from state $s \\in \\mathcal{S}$ and following policy $\\pi$.\n", "\n", "**Action-value function ($Q$-value)**:\n", "$$\n", "q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[ G_t | S_t = s, A_t = a]\n", "$$\n", "The expected return starting from state $s \\in \\mathcal{S}$, then taking action $a \\in \\mathcal{A}$ and then following policy $\\pi$."]}, {"cell_type": "markdown", "id": "e2504400-0709-41e6-921e-e90766cfee88", "metadata": {"editable": false, "deletable": false}, "source": ["## 2.1 The Bellman Equations <a id=\"sec2_1\">"]}, {"cell_type": "markdown", "id": "8ee4fbad-0afa-43ad-805f-69f4d68826ed", "metadata": {"editable": false, "deletable": false}, "source": ["We have seen that $v_{\\pi}(s)$ is the solution to the Bellman equation\n", "$$\n", "\\begin{aligned}\n", "v_{\\pi}(s) &= \\mathbb{E}_{\\pi}[ R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s] \\\\\n", "&= \\sum_{a} \\pi(a|s) \\sum_{r} \\sum_{s'} p(s', r | s, a) [ r + \\gamma v_\\pi(s') ] = \\sum_{a} \\pi(a|s) q_{\\pi}(s,a)\n", "\\end{aligned}\n", "$$\n", "where\n", "$$\n", "q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[ G_t | S_t = s, A_t = a] = \\sum_{r}\\sum_{s'} p(s',r | s,a) [ r + \\gamma v_{\\pi}(s')]\n", "$$"]}, {"cell_type": "markdown", "id": "bc0cbf19-b8df-4d25-9cc6-615b0a2b63bb", "metadata": {"editable": false, "deletable": false}, "source": ["## 2.2 Example: Study or Facebook? <a id=\"sec2_2\">"]}, {"cell_type": "markdown", "id": "8e5a6f17-41d6-4635-bc9f-b581769b9e21", "metadata": {"editable": false, "deletable": false}, "source": ["In Lecture 2 we looked at the example MDP bellow.\n", "<img src=\"example.png\">\n", "\n", "It has four states, $\\mathcal{S} = \\{ K_0, K_1, K_2, Pass \\}$. The state $Pass$ is a terminal state, so the episode will end if this state is reached (alternatively, if we reach $Pass$ we will stay there forever and receive 0 future return).\n", "\n", "In each non-terminal state we can choose between `Study` or `Facebook`, $\\mathcal{A} = \\{ Study, Facebook\\}$. The red nodes corresponds to actions, and the labels on the edges gives immediate rewards (green) and transition probabilities (black)."]}, {"cell_type": "markdown", "id": "8d1f0fd6-f727-4dc7-89ac-c5e76f773653", "metadata": {"editable": false, "deletable": false}, "source": ["***\n", "### Task:\n", "Assume that we use the policy $\\pi(a|s) = 0.5$ for all states and actions, and the discount factor $\\gamma = 0.9$.  \n", "\n", "In Lecture 2 we saw that the state-value function (rounded to two decimals) is\n", "$$\n", "v_{\\pi}(K_0) = 3.00, \\quad v_{\\pi}(K_1) = 4.78, \\quad v_{\\pi}(K_2) = 7.84.\n", "$$\n", "1. Verify that this satisfies the Bellman equation for all states (when rounded to two decimals)! \n", "\n", "You can use the code-block bellow to carry out your computations.\n", "***"]}, {"cell_type": "code", "execution_count": null, "id": "44151557-0549-424e-ac66-7d5fa3b6bc64", "metadata": {}, "outputs": [], "source": ["gamma = 0.9\n", "vK0 = 0.5*(0 + gamma*3) + 0.5*(-1+gamma*4.78) \n", "vK1 = 0.5*(0 + gamma*(0.5*3 + 0.5*4.78)) + 0.5*(-1 + gamma*7.84) \n", "vK2 = 0.5*(0 + gamma*(0.5*4.78 + 0.5*7.84)) + 0.5*(10 + gamma*0) \n", "print(\"v(K0) = %.2f, v(K1) = %.2f, v(K2) = %.2f\" % (vK0, vK1, vK2))"]}, {"cell_type": "markdown", "id": "0af14a61-acab-44b8-bde6-3da1b103a569", "metadata": {"editable": false, "deletable": false}, "source": ["## 2.3 *Analytical solution to the Bellman equation <a id=\"sec2_3\">"]}, {"cell_type": "markdown", "id": "ff44318d-cabd-48f4-942e-ff69a20ecd54", "metadata": {"editable": false, "deletable": false}, "source": ["Note that the Bellman equation can be written as\n", "$$\n", "v_{\\pi}(s) = \\mathbb{E}_{\\pi}[ R_{t+1} | S_{t} = s] + \\gamma\\sum_{a, s'} \\pi(a|s)p(s' | s, a) v_{\\pi}(s').\n", "$$\n", "So the value of $s$ is the average immediate reward plus the discounted average value of the next state."]}, {"cell_type": "markdown", "id": "c89bd8bc-8a6f-4bb3-9ff9-a7be5ced2334", "metadata": {"editable": false, "deletable": false}, "source": ["**Example: From the state $K_1$**:\n", "    \n", "* In $s = K_1$ the expected immediate reward is $-0.5$, since we choose `Facebook` with probability 0.5 (reward 0) and `Study` with probability 0.5 (reward -1). \n", "* There is a 0.5 probability that the action is `Facebook`, and then there is a 50/50 chance of going either to $K_0$ or $K_1$. Hence the total probability of going to $K_0$ and $K_1$ are both 0.25 ($0.5 \\times 0.5$). There is also a 0.5 probability for the action `Study` which will move us to $K_2$. Finally there is 0 probability of reaching $Pass$. \n", "\n", "Summarizing this we get\n", "$$\n", "v_{\\pi}(K_1) = -0.5 + \\gamma [ 0.25 v_{\\pi}(K_0) + 0.25 v_{\\pi}(K_1) + 0.5 v_{\\pi}(K_2) + 0 v_{\\pi}(Pass)]\n", "$$\n", "If we define a vector with all state-values\n", "$$\n", "V_{\\pi} = \\begin{bmatrix} v_{\\pi}(K_0) \\\\ v_{\\pi}(K_1) \\\\ v_{\\pi}(K_2) \\\\ v_{\\pi}(Pass) \\end{bmatrix}\n", "$$\n", "we can write this as\n", "$$\n", "v_{\\pi}(K_1) = \\underbrace{-0.5}_{r_1} + \\gamma \\underbrace{\\begin{bmatrix} 0.25 & 0.25 & 0.5 & 0 \\end{bmatrix}}_{p_1} V_\\pi\n", "$$\n", "Note that the elements of $p_1$ are the probability of going from $K_1$ to each of the other states when we follow the policy."]}, {"cell_type": "markdown", "id": "54972b36-844b-46e0-bbd8-4423abc4d256", "metadata": {"editable": false, "deletable": false}, "source": ["**Putting it all together:**\n", "\n", "If we do the same for all states, we get an equation on the form \n", "$$\n", "V_{\\pi} = R + \\gamma P V_{\\pi} \\iff (I - \\gamma P) V_{\\pi} = R\n", "$$\n", "where $R$ is the vector of expected immediate rewards for each state, and $P \\in \\mathbb{R}^{4 \\times 4}$ where element $P_{i,j}$ is the probability of moving from the $i$th state to the $j$th state when we follow the policy.\n", "\n", "Assuming that $\\gamma < 1$ then $I - \\gamma P$ is always invertible, so there is a unique solution \n", "$$\n", "V_{\\pi} = (I - \\gamma P)^{-1} R.\n", "$$"]}, {"cell_type": "markdown", "id": "98b3fc41-2fec-433a-be0e-e0528cd48bcc", "metadata": {"editable": false, "deletable": false}, "source": ["***\n", "### Task \n", "Fill in the correct values of $R$ and $P$ in the code below, and see if you find the same solution as in the slides of Lecture 2. You can also play around with the discount rate, and/or try to compute $R$, $P$ and then $V_{\\pi}$ for another policy.\n", "\n", "**Note**: The state $Pass$ is a bit special, since it is a terminating state. Hence, when we reach \"Pass the exam\" we will not receive anymore rewards, and we will stay in this state forever (the probability of going to any other state is 0). Hence\n", "$$\n", "v_{\\pi}(Pass) = 0 + \\gamma \\begin{bmatrix} 0 & 0 & 0 & 1 \\end{bmatrix} V_{\\pi}\n", "$$\n", "***"]}, {"cell_type": "code", "execution_count": null, "id": "ad5bef54-69d1-43ce-8f32-17aa47188cd9", "metadata": {}, "outputs": [], "source": ["discount = 0.9 # gamma\n", "R = np.zeros((4,1))\n", "P = np.zeros((4,4))\n", "\n", "# Enter the expected immediate reward for each state\n", "# Those computed in the text above are already filled in.\n", "R[0] = -.5 \n", "R[1] = -.5 # For K_1\n", "R[2] = 5 \n", "R[3] = 0 # For \"Pass exam\"\n", "\n", "# Enter the probabilities going from state i to state j\n", "P[0] = [0.5, 0.5, 0, 0] \n", "P[1] = [0.25, 0.25, 0.5, 0] # for i=1 (K_1)\n", "P[2] = [0, 0.25, 0.25, 0.5] \n", "P[3] = [0, 0, 0, 1] # for i=3 (Pass the exam) \n", "\n", "# Solve the Bellman equation \n", "V = np.linalg.inv(np.eye(4) - discount*P)@R # V = (I - discount*P)^-1 * R\n", "print(V)"]}, {"cell_type": "markdown", "id": "ab422e0f-7eec-4046-8eab-f904d9930d6b", "metadata": {"editable": false, "deletable": false}, "source": ["# 3. Helper functions <a id=\"sec3\">"]}, {"cell_type": "markdown", "id": "72ec2d64-b009-41b1-bd99-5f1f51184c75", "metadata": {"editable": false, "deletable": false}, "source": ["We will now see how dynamic programming can be used to find the values and optimal actions in an MDP. \n", "\n", "First we will define two functions that will be used throughout the notebook. \n", "\n", "The class `RandomAgent` implements an agent with a policy $\\pi(a|s)$. The method `act` will take the state $s$ as an argument, and then sample an action according to the probabilities $\\pi(a|s)$. \n", "\n", "The probabilities are implemented in a table `probs` of size $|\\mathcal{S}| \\times |\\mathcal{A}|$, so that `probs[s][a]` $= \\pi(a|s)$. Here we implement an agent that (initially) chooses the action with a uniform probability, so all actions are equally likely in each state. However, by changing `probs` different policies can be implemented."]}, {"cell_type": "code", "execution_count": null, "id": "0e23f39f-49ba-4618-ad45-7a7c2f8533d9", "metadata": {}, "outputs": [], "source": ["class RandomAgent():\n", "    \n", "    def __init__(self, nA=4, nS=16):\n", "        self.nA = nA # Number of actions\n", "        self.nS = nS # Number of states\n", "        \n", "        # Uniform probabilites in each state.\n", "        # That is, in each of the nS states\n", "        # each of the nA actions has probability\n", "        # 1/nA.\n", "        self.probs = np.ones((nS,nA))/nA \n", "\n", "    def act(self, state):\n", "        action = np.random.choice(self.nA, p=self.probs[state]) \n", "        return action # a random policy"]}, {"cell_type": "markdown", "id": "fe66873e-bc6a-40ce-9a82-574bdf4d03b9", "metadata": {"editable": false, "deletable": false}, "source": ["We also define a function to run an environment with a given agent."]}, {"cell_type": "code", "execution_count": null, "id": "578f8090-d2b4-40a0-b2fc-e3569576bc16", "metadata": {}, "outputs": [], "source": ["def run_agent(env, agent):\n", "    state, info = env.reset()\n", "    time_step = 0\n", "    total_reward = 0\n", "    truncated = False\n", "    terminated = False\n", "    while not truncated and not terminated:\n", "        action = agent.act(state);\n", "        state, reward, terminated, truncated, info = env.step(action)\n", "        total_reward += reward\n", "        time_step += 1\n", "        \n", "        clear_output(wait=True)\n", "        print(\"Time step:\", time_step)\n", "        print(\"State:\", state)\n", "        print(\"Action:\", action)\n", "        print(\"Total reward:\", total_reward)\n", "        \n", "    if truncated:\n", "        print(\"The environment was truncated even though a terminal state was not reached.\")\n", "    elif terminated:\n", "        print(\"A terminal state was reached.\")"]}, {"cell_type": "markdown", "id": "5fa075ba-215f-44f8-b9a0-e6ce8c6881a1", "metadata": {"editable": false, "deletable": false}, "source": ["# 4. The environments <a id=\"sec4\">"]}, {"cell_type": "markdown", "id": "84807ca8-40fc-4218-a88a-caf2109290f9", "metadata": {"editable": false, "deletable": false}, "source": ["In this notebook we will try the methods on different GridWorld environments. \n", "These environments are $4\\times 4$ gridworlds. (It is possible to use larger grids, but in this notebook we will stay with $4 \\times 4$ gridworlds)\n", "\n", "<img src=\"grid.png\" width=200>\n", "\n", "The agent can be in one of the 16 positions, so the state space is\n", "$$\n", "\\mathcal{S} = \\{ 0, 1, 2, \\ldots, 15\\},\n", "$$\n", "see the figure for the meaning of each state. \n", "\n", "The action space contains four actions, \n", "$$\n", "\\mathcal{A} = \\{ 0, 1, 2, 3\\}\n", "$$\n", "corresponding to "]}, {"cell_type": "code", "execution_count": null, "id": "2d8d10c7-87d6-442f-b153-d261a9e0fa16", "metadata": {}, "outputs": [], "source": ["LEFT = 0\n", "DOWN = 1\n", "RIGHT = 2\n", "UP = 3"]}, {"cell_type": "markdown", "id": "6e107c61-20d0-4c5f-acd7-d140a9d8aa98", "metadata": {"editable": false, "deletable": false}, "source": ["Let us look at Example 4.1 in the textbook. \n", "\n", "The player (here represented by a smiley) should go to one of the goals (which are supposed to look like ice creams with three cherries on top, but Per is not very good at drawing...)\n", "\n", "___Note:___ If you prefer to use a different rendering mode, like `ansi`, also edit `run_agent` above by adding `print(env.render())` and potentially add `time.sleep(0.05)` at the end of the function."]}, {"cell_type": "code", "execution_count": null, "id": "0a7bff46-823b-4e76-9f12-53aa413b7938", "metadata": {}, "outputs": [], "source": ["env = gym.make(\"GridWorld-v0\", render_mode=\"human\")\n", "state, info = env.reset()\n", "print(\"State space:\", env.observation_space)\n", "print(\"Action space:\", env.action_space)"]}, {"cell_type": "markdown", "id": "f165d661-b8f6-4478-acbf-b4da9da577ac", "metadata": {"editable": false, "deletable": false}, "source": ["**The transition probabilities**: The environments used in this notebook have their transition probabilities $p(s', r | s, a)$ stored in `env.P`. Specifically, `env.P[s][a]` gives a list where each element is `(probability, next_state, reward, terminating)`. So, if you take action `a` in state `s`, then with probability `probability` you will move to `next_state` and get the reward `reward`. `terminating` tells us if `next_state` will terminate the episode or not. "]}, {"cell_type": "code", "execution_count": null, "id": "291055b7-0138-4db1-a961-22ef89cb84ce", "metadata": {}, "outputs": [], "source": ["s = 6\n", "a = LEFT\n", "print(env.P[s][a])\n", "for p, next_s, reward, _ in env.P[s][a]: # Go through all possible transitions\n", "    print(\"With probability\", p, \"you will move to state\", next_s, \n", "          \"and get the reward\", reward)"]}, {"cell_type": "markdown", "id": "2a8f0a97-9e3f-42c7-89e9-88a5dc346d5c", "metadata": {"editable": false, "deletable": false}, "source": ["Try a few sates and actions above and see that the results agree with your understanding of the environment. "]}, {"cell_type": "markdown", "id": "a69dcdef-5686-4d9a-98c4-1a472bd020b0", "metadata": {"editable": false, "deletable": false}, "source": ["**Testing the environment:** To test the environment we create an agent that moves completely randomly. Then we use this agent on the environment. `run_agent` will continue until a terminating state (one of the \"ice creams\") is reached or the episode is truncated after 100 time steps. "]}, {"cell_type": "code", "execution_count": null, "id": "8c73ee45-4658-4e50-9144-a8a9dea2a4b5", "metadata": {}, "outputs": [], "source": ["agent = RandomAgent()\n", "run_agent(env, agent)"]}, {"cell_type": "markdown", "id": "94c69069-1d44-414d-94c7-2f9d82cb9815", "metadata": {"editable": false, "deletable": false}, "source": ["**The optimal policy:** Note that `RandomAgent` choose actions according to the probabilities specified in `agent.probs`. By changing `agent.probs` we can thus change the policy of the agent. \n", "\n", "That is `agent.probs[s][a]` gives the probability of the agent choosing action `a` when it is in state `s`. \n", "\n", "**Task:** Implement the optimal policy for the environment. This policy is shown in Figure 4.1 in the textbook. Note there are several optimal actions in a given state, then you can choose between them with e.g. equal probability. "]}, {"cell_type": "code", "execution_count": null, "id": "d9c51a07-63bc-44b0-87d9-09b725ede999", "metadata": {}, "outputs": [], "source": ["agent.probs = np.zeros((16, 4))\n", "\n", "# Note that in each state the total\n", "# probability must add upp to 1.0.\n", "\n", "# You do not have to give any probabilities\n", "# for state 0 or 15, since these\n", "# are terminating states. \n", "\n", "# Row 1\n", "agent.probs[1][LEFT] = 1.0\n", "agent.probs[2][LEFT] = 1.0\n", "agent.probs[3][[LEFT, DOWN]] = 0.5 # Pick between them with equal probability\n", "\n", "# Row 2\n", "agent.probs[4][UP] =  1.0 \n", "agent.probs[5][[LEFT, UP]] = 0.5 \n", "agent.probs[6][[DOWN, LEFT]] = 0.5 \n", "agent.probs[7][DOWN] = 1.0 \n", "\n", "# Row 3\n", "agent.probs[8][UP] = 1.0 \n", "agent.probs[9][[UP, RIGHT]] = 0.5 \n", "agent.probs[10][[DOWN, RIGHT]] = 0.5 \n", "agent.probs[11][DOWN] = 1.0 \n", "\n", "# Row 4 \n", "agent.probs[12][[UP, LEFT]] = 0.5 \n", "agent.probs[13][RIGHT] = 1.0 \n", "agent.probs[14][RIGHT] = 1.0 \n"]}, {"cell_type": "code", "execution_count": null, "id": "15ff3564-87db-4a4f-93ff-92c190b1073c", "metadata": {}, "outputs": [], "source": ["state, info = env.reset()\n", "run_agent(env, agent)"]}, {"cell_type": "markdown", "id": "389465a5-7006-4d99-a51a-4d487e4b1c8a", "metadata": {"editable": false, "deletable": false}, "source": ["Run the agent a few times to see that it seems to be acting optimally. "]}, {"cell_type": "markdown", "id": "6006b88d-4eb1-42f6-9de4-03c2c3450efa", "metadata": {"editable": false, "deletable": false}, "source": ["## 4.1 The Frozen Lake <a id=\"sec4_1\">"]}, {"cell_type": "markdown", "id": "935c38de-56e3-4b94-a567-d14c4cfcb46c", "metadata": {"editable": false, "deletable": false}, "source": ["The environment `FrozenLake-v1` [(link)](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) is similar to `GridWorld-v0`, but it is stochastic. "]}, {"cell_type": "code", "execution_count": null, "id": "07c607bd-d5ae-43c7-852c-8ec016aa6e75", "metadata": {}, "outputs": [], "source": ["env = gym.make('FrozenLake-v1', render_mode=\"human\")\n", "state, info = env.reset()\n", "print(\"State space:\", env.observation_space)\n", "print(\"Action space:\", env.action_space)"]}, {"cell_type": "markdown", "id": "a379a263-eee2-4014-b6e8-b14834107f4a", "metadata": {"editable": false, "deletable": false}, "source": ["The objective is to reach the goal without falling into a hole. However, the ice is slippery so the movement will only partially depend on your action. \n", "\n", "**Terminal states:** All holes and the goal are terminal states. If you fall into a hole you get no reward, but if you reach the goal you get a reward. \n", "\n", "**Reward:**\n", " All actions not leading to the goal state gives a reward of 0. If you get to the goal you get reward $+1$. Hence, to maximize the total reward the agent much reach the goal state without falling into a hole.\n", " "]}, {"cell_type": "markdown", "id": "284aaba4-db9e-4c06-9c82-c24f0f964bd1", "metadata": {"editable": false, "deletable": false}, "source": ["**Dynamics:** As mentioned above, this is a stochastic environment. As in the previous example, the transition probabilities are stored in `env.P`. "]}, {"cell_type": "code", "execution_count": null, "id": "9ca48613-7ebb-4925-ab35-96ed2c6a1cbf", "metadata": {}, "outputs": [], "source": ["s = 0\n", "a = RIGHT \n", "print(env.P[s][a])\n", "for p, next_s, reward, _ in env.P[s][a]:\n", "    print(\"With probability %.2f you will move to state %d and get reward %.1f.\" % (p, next_s, reward))"]}, {"cell_type": "markdown", "id": "90b62e70-f04c-44d0-a322-94e420976dad", "metadata": {"editable": false, "deletable": false}, "source": ["We see that the environment is stochastic in this case, since the action RIGHT in state 0 may take the agent either to state 4 (down), state 1 (right) or state 0 (stay), due to the slippery surface.\n", "\n", "We can also try to run this environment using the random policy:"]}, {"cell_type": "code", "execution_count": null, "id": "dfdbbeaa-c198-4bfd-bb0f-0180ce4065f8", "metadata": {}, "outputs": [], "source": ["agent = RandomAgent()\n", "run_agent(env, agent)"]}, {"cell_type": "markdown", "id": "30757229-6ef3-4839-9c73-10b919b06f9a", "metadata": {"editable": false, "deletable": false}, "source": ["We can see that typically the agent ends up in one of the holes, and thus the total reward is typically 0 (but the expected total reward is positive, since there is a non-zero probability that we reach the goal)."]}, {"cell_type": "code", "execution_count": null, "id": "59404243-1f3c-4d20-8a37-fa8b39781ffc", "metadata": {}, "outputs": [], "source": ["env.close()"]}, {"cell_type": "markdown", "id": "90a1b478-4dd6-4985-9690-17393d226d91", "metadata": {"editable": false, "deletable": false}, "source": ["# 5. MDPs and the Bellman equations <a id=\"sec5\">"]}, {"cell_type": "markdown", "id": "f7a5781b-43a0-4c7e-b981-fc6f3d8fec57", "metadata": {"editable": false, "deletable": false}, "source": ["In this section we will study the Bellman equations for the value function a bit closer. Remember that we defined the return as \n", "$$\n", "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n", "$$\n", "and the state-value function as \n", "$$\n", "v_\\pi(s) = \\mathbb{E}[ G_t | S_{t} = s].\n", "$$\n", "The Bellman equation for the state-value function is then \n", "$$\n", "v_\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{r} \\sum_{s'} p(s', r | s, a) [ r + \\gamma v_\\pi(s') ] = \\sum_{a} \\pi(a|s) q_{\\pi}(s,a)\n", "$$\n", "where\n", "$$\n", "q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[ G_t | S_t = s, A_t = a] = \\sum_{r}\\sum_{s'} p(s',r | s,a) [ r + \\gamma v_{\\pi}(s')]\n", "$$\n", "is the action-value function."]}, {"cell_type": "markdown", "id": "8c519ceb-288c-4b38-a192-434c823e92ba", "metadata": {"editable": false, "deletable": false}, "source": ["**Implementation:**\n", "In the code we will represent the state-value function $v_\\pi(s)$ as a vector $v$ with one element for each state in $\\mathcal{S}$.\n", "\n", "We will now implement functions for computing the right-hand side of the Bellman equation. \n", "\n", "**Task:**\n", "Complete `compute_action_value` and `Bellman_RHS`. Make sure that you understand the code.\n", "\n", "We start by a function that computes the action values $q_{\\pi}(s,a)$ given the state-value function $v_\\pi(s)$."]}, {"cell_type": "code", "execution_count": null, "id": "2bb5ce62-0bba-4f8a-9465-4d918da19c26", "metadata": {}, "outputs": [], "source": ["def compute_action_value(env, discount, s, a, v):\n", "    \n", "    action_value = 0\n", "    \n", "    # Loop through all possible (s', r) pairs\n", "    for p, next_s, reward, _ in env.P[s][a]:\n", "        action_value += p * (reward + discount*v[next_s]) \n", "    \n", "    return action_value"]}, {"cell_type": "markdown", "id": "8a89b62f-cd36-4570-87a7-b276c551f5cf", "metadata": {"editable": false, "deletable": false}, "source": ["With the action values, we can now compute $\\sum_{a} \\pi(a|s) q_{\\pi}(s,a)$ (the expected action value) to get the right-hand side (RHS) of the Bellman equation.\n", "\n", "For this we use `agent.probs[s][a]` $=\\pi(a|s)$, see discussion in \"Helper Functions\"."]}, {"cell_type": "code", "execution_count": null, "id": "3f80c285-042f-4bf9-9eff-ef4f982952dd", "metadata": {}, "outputs": [], "source": ["def Bellman_RHS(env, discount, agent, s, v):\n", "    \n", "    state_value = 0\n", "    \n", "    for a in range(env.action_space.n):\n", "        # Loop through all possible actions\n", "        state_value += agent.probs[s][a]*compute_action_value(env, discount, s, a, v) \n", "    \n", "    return state_value"]}, {"cell_type": "markdown", "id": "24806206-fe62-48b1-bea5-402d2946c049", "metadata": {"editable": false, "deletable": false}, "source": ["Finally we implement a function that, given a value function, computes the right-hand side of the Bellman equation for all states."]}, {"cell_type": "code", "execution_count": null, "id": "bd7ee8c6-4611-4cb5-9175-47ef973ae296", "metadata": {}, "outputs": [], "source": ["def Bellman_RHS_all(env, discount, agent, v0):\n", "    # v0 is the given value function\n", "    # v will be the right-hand side of the Bellman equation\n", "    # If v0 is indeed the value function, then we should get v = v0.\n", "    \n", "    v = np.zeros(env.observation_space.n)\n", "    \n", "    for s in range(env.observation_space.n):\n", "        v[s] = Bellman_RHS(env, discount, agent, s, v0)\n", "    \n", "    return v"]}, {"cell_type": "markdown", "id": "b44f19d3-f12b-4c35-af4e-694be7ab90d4", "metadata": {"tags": [], "editable": false, "deletable": false}, "source": ["## 5.1 Test your code on Example 4.1 (GridWorld) <a id=\"sec5_1\">"]}, {"cell_type": "markdown", "id": "467e0465-b78b-4b98-b324-f2ba25abaebb", "metadata": {"editable": false, "deletable": false}, "source": ["For Example 4.1 we will consider the state-value $v_{\\pi}(s)$ for the policy when each action is taken with equal probability. The discount rate is\n", "$$\n", "\\gamma = 1\n", "$$\n", "The value function for this policy is given in Figure 4.1 in the textbook (the lower left), and it is"]}, {"cell_type": "code", "execution_count": null, "id": "2b5e7182-985b-4ff1-b7cd-58243c7b508d", "metadata": {}, "outputs": [], "source": ["v = np.array([[0, -14, -20, -22], \n", "             [-14, -18, -20, -20],\n", "             [-20, -20, -18, -14], \n", "             [-22, -20, -14, 0]]).ravel()\n", "\n", "print(\"v as vector:\", v)\n", "print(\"v as matrix:\\n\", v.reshape(4,4))\n", "\n", "# ravel turns the matrix into an array,\n", "# and with reshape we print it as a matrix again so that it is easier to read."]}, {"cell_type": "markdown", "id": "038c91b6-5441-4730-88b3-82f3b1ccf45e", "metadata": {"editable": false, "deletable": false}, "source": ["We can now use our code to see if this value function really satisfy the Bellman equation:"]}, {"cell_type": "code", "execution_count": null, "id": "050c583d-984d-4c6f-a48e-83c2313268ee", "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-v0')\n", "agent = RandomAgent()\n", "discount = 1\n", "v_new = Bellman_RHS_all(env, discount, agent, v)\n", "print('Right-hand side of Bellman equation:\\n', v_new.reshape(4,4))"]}, {"cell_type": "markdown", "id": "0be2a825-bd48-45d2-ae6d-8a078ced9bad", "metadata": {"editable": false, "deletable": false}, "source": ["**Task:** `v` is the true value-function for the policy $\\pi$ implemented in `agent`. Hence, `v_new` (the right-hand side of the Bellman equation) should be equal to `v`. \n", "\n", "If `v_new` is not equal to `v`, go back and fix your code for `compute_action_value` and `Bellman_RHS`. Remember to re-run the code cells after you have changed the code!"]}, {"cell_type": "markdown", "id": "ec5ea371-fa65-4e30-9559-2bcb71f3e792", "metadata": {"editable": false, "deletable": false}, "source": ["# 6. Policy Evaluation <a id=\"sec6\">"]}, {"cell_type": "markdown", "id": "9d51c490-6482-47ee-a065-7dbd87069173", "metadata": {"editable": false, "deletable": false}, "source": ["In Lecture 3 we learned that one way of solving the Bellman equation, is to start from an initial guess and then repeatedly update the value function by applying the right-hand side of the Bellman equation. \n", "\n", "Below is one way to implement this. The iteration will stop when the maximum change in $v$ is less than `tol` (tolerance) or the number of iterations are `max_iter`.\n", "\n", "***Note:*** For this code to work properly, your implementation of `compute_action_value` and `Bellman_RHS` must be correct. So make sure that you have tested your code first!\n", "\n", "**Task:** Make sure that you understand the code!"]}, {"cell_type": "code", "execution_count": null, "id": "22a699dd-0ff3-4631-8217-eb62cdffdd88", "metadata": {}, "outputs": [], "source": ["def policy_evaluation(env, discount, agent, v0, max_iter=1000, tol=1e-6):\n", "    \n", "    v_old = v0\n", "    \n", "    for i in range(max_iter):\n", "        v_new = Bellman_RHS_all(env, discount, agent, v_old)\n", "        \n", "        if np.max(np.abs(v_new-v_old)) < tol:\n", "            break\n", "            \n", "        v_old = v_new\n", "        \n", "    return v_new"]}, {"cell_type": "markdown", "id": "2b66c274-d11f-4892-95a0-0e64b2658df9", "metadata": {"editable": false, "deletable": false}, "source": ["Lets try this on the `GridWorld-v0` example, with the uniformly random policy. We start with an initial guess $v_{\\pi}(s) = 0$ for all $s$."]}, {"cell_type": "code", "execution_count": null, "id": "2a1c8743-0fa7-4a4d-bf14-f77267926060", "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-v0')\n", "agent = RandomAgent()\n", "discount = 1\n", "v0 = np.zeros((env.observation_space.n))\n", "\n", "v = policy_evaluation(env, discount, agent, v0)\n", "print(v.reshape(4,4))"]}, {"cell_type": "markdown", "id": "2bd7cb35-8f78-4f7c-8d5a-6b6948755c4c", "metadata": {"editable": false, "deletable": false}, "source": ["Is this (approximately) the same as the true value function in Figure 4.1 $(k=\\infty)$?\n", "If you do not find the correct value function, make sure that your code in `compute_action_value` and `Bellman_RHS` is correct!\n", "\n", "To replicate the other parts of Figure 4.1, you can set `max_iter` in order to see how the value function looks after a few iterations."]}, {"cell_type": "code", "execution_count": null, "id": "df78ea23-0288-4bc8-86c0-2ba1012c82ef", "metadata": {}, "outputs": [], "source": ["v0 = np.zeros((env.observation_space.n))\n", "v = policy_evaluation(env, discount, agent, v0, max_iter=10)\n", "print(v.reshape(4,4))"]}, {"cell_type": "markdown", "id": "7e02343e-0ffe-4ccc-ac08-7e5e984cab1b", "metadata": {"editable": false, "deletable": false}, "source": ["**Task:** Use `policy_evaluation` to compute the value function for `FrozenLake-v0` when the uniformly random policy is used. Use $\\gamma = 1$."]}, {"cell_type": "code", "execution_count": null, "id": "c8124854-d787-482a-9b0e-ab030806c022", "metadata": {}, "outputs": [], "source": ["env = gym.make('FrozenLake-v1')\n", "agent = RandomAgent()\n", "discount = 1\n", "# Write code for computing the state-value function\n", "v0 = np.zeros(env.observation_space.n) \n", "v = policy_evaluation(env, discount, agent, v0) \n", "print(v.reshape(4,4))"]}, {"cell_type": "markdown", "id": "ebb2ab32-0b7d-475a-a19c-e18d65e23683", "metadata": {"editable": false, "deletable": false}, "source": ["## 6.1 In place updates <a id=\"sec6_1\">"]}, {"cell_type": "markdown", "id": "aa114f4e-5767-4357-9e38-ebc91ca646f2", "metadata": {"editable": false, "deletable": false}, "source": ["As mentioned in Lecture 3 and in the textbook, the policy evaluation is often implemented using in place updates.\n", "\n", "This can both simplify implementation, since we do not keep two separate arrays, and it can also speed up convergence.\n", "\n", "**Task:** Complete the code in `policy_evaluation_ip`. Pseudo-code can be found in the textbook in the box on page 75. Then test your code on `GridWorld-v0` with the uniform policy to see that you still get the correct value function.\n", "\n", "***Note:*** You have already written a function that computes the right-hand side of the Bellman equation!"]}, {"cell_type": "code", "execution_count": null, "id": "84b5d50a-5839-4afa-8596-f4407a531feb", "metadata": {}, "outputs": [], "source": ["def policy_evaluation_ip(env, discount, agent, v0, max_iter=1000, tol=1e-6):\n", "    \n", "    v = v0\n", "    \n", "    for i in range(max_iter): # Loop\n", "        delta = 0\n", "        for s in range(env.observation_space.n):\n", "            vs = v[s]\n", "            \n", "            v[s] = Bellman_RHS(env, discount, agent, s, v) \n", "            \n", "            delta = np.max([delta, np.abs(vs-v[s])])\n", "            \n", "        if (delta < tol): # Until delta < tol\n", "            break\n", "            \n", "    return v    "]}, {"cell_type": "code", "execution_count": null, "id": "b72e31d5-dc24-4db1-817c-9e56ef41343c", "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-v0')\n", "agent = RandomAgent()\n", "discount = 1\n", "\n", "v0 = np.zeros(16)\n", "v = policy_evaluation_ip(env, discount, agent, v0)\n", "print(v.reshape(4,4))"]}, {"cell_type": "markdown", "id": "087b5fb6-ae64-4a96-9807-2be0210e9bb4", "metadata": {"editable": false, "deletable": false}, "source": ["# 7. Policy Iteration <a id=\"sec7\">"]}, {"cell_type": "markdown", "id": "2a87b230-898a-4637-b94f-6ec55ee2b23e", "metadata": {"editable": false, "deletable": false}, "source": ["Now when we have code for evaluating a policy, it is time to see how it can be improved. Remember that the idea is to act greedily with respect to $v_{\\pi}(s)$. That is, given $v_{\\pi}(s)$ we can compute $q_{\\pi}(s,a)$, and then the greedy (improved) policy is\n", "$$\n", "\\pi'(s) = \\text{argmax}_{a} q_{\\pi}(s,a)\n", "$$\n", "We have already written code for computing $q_{\\pi}(s,a)$ for a given $v_{\\pi}(s)$, so the only thing we have to do now is to implement the maximization.\n", "\n", "`greedy_policy` will return `a_probs` which encode a policy that is greedy with respect to `v`. That is `a_probs[s][a]` $= \\pi'(a|s)$. Make sure that you understand the code."]}, {"cell_type": "code", "execution_count": null, "id": "69aac234-7097-4a30-8bb7-e5fca2a9d77f", "metadata": {}, "outputs": [], "source": ["def greedy_policy(env, discount, agent, v):\n", "    \n", "    # The new policy will be a_probs\n", "    # We start by setting all probabilities to 0\n", "    # Then when we have found the greedy action in a state, \n", "    # we change the probability for that action to 1.0.\n", "    \n", "    a_probs = np.zeros((env.observation_space.n, env.action_space.n)) \n", "    \n", "    for s in range(env.observation_space.n):\n", "        \n", "        action_values = np.zeros(env.action_space.n)\n", "        \n", "        for a in range(env.action_space.n):\n", "            # Compute action value for all actions\n", "            action_values[a] = compute_action_value(env, discount, s, a, v)\n", "            \n", "        a_max = np.argmax(action_values) # A greedy action\n", "        a_probs[s][a_max] = 1.0 # Always choose the greedy action!\n", "        \n", "    return a_probs"]}, {"cell_type": "markdown", "id": "192241af-3efe-466b-b4e3-b46e5a2fef42", "metadata": {"editable": false, "deletable": false}, "source": ["Lets try to improve the policy on `GridWorld-v0`."]}, {"cell_type": "code", "execution_count": null, "id": "90babd0e-72d8-405d-b210-87ad11ce58bd", "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-v0', render_mode=\"human\")\n", "agent = RandomAgent()\n", "discount = 1\n", "\n", "# We first evaluate the policy\n", "v = np.zeros(env.observation_space.n)\n", "v = policy_evaluation(env, discount, agent, v)"]}, {"cell_type": "code", "execution_count": null, "id": "4649e60e-c244-4575-88e4-8b18d4c18a4c", "metadata": {}, "outputs": [], "source": ["v_old = v\n", "\n", "# And then we improve the policy (act greedy w.r.t v)\n", "agent.probs = greedy_policy(env, discount, agent, v)\n", "\n", "# We can also evaluate the new policy \n", "v = policy_evaluation(env, discount, agent, v)\n", "\n", "print(\"Value of initial policy:\")\n", "print(v_old.reshape(4,4))\n", "print(\"\\nValue of improved policy:\")\n", "print(v.reshape(4,4))"]}, {"cell_type": "markdown", "id": "c2e87b19-ed51-4020-b86b-7b8006b8a7ae", "metadata": {"editable": false, "deletable": false}, "source": ["Assuming that your implementation of `compute_action_value` is correct, \n", "we can clearly see that the improved policy has higher value in every state. In fact, the policy is now an optimal policy. To see this, you can try to rerun the second cell above and note that the policy does not improve anymore.\n", "\n", "**Policy iteration:** However, it is not the case for all environments that the policy will converge to the optimal in just one improvement. Typically we have to improve the policy several times until it finally converge to the optimal policy. \n", "\n", "Finally, we can try to run the agent with the improved policy."]}, {"cell_type": "code", "execution_count": null, "id": "a19630f8-c9d9-456a-8ea2-2e4246e784a3", "metadata": {}, "outputs": [], "source": ["run_agent(env, agent)"]}, {"cell_type": "markdown", "id": "dc594d33-7423-4aff-bddc-2a5ac7f036a3", "metadata": {"editable": false, "deletable": false}, "source": ["**Task:** Find an optimal policy for `FrozenLake-v1`. (This time it is not enough to do policy improvement only one time to reach an optimal policy. So do several improvements!)"]}, {"cell_type": "code", "execution_count": null, "id": "89d40e74-bb27-4165-8cec-7a17a06ed454", "metadata": {}, "outputs": [], "source": ["env = gym.make('FrozenLake-v1', render_mode='human')\n", "agent = RandomAgent()\n", "discount = 1\n", "# Enter code here\n", "v_old = np.zeros(env.observation_space.n) \n", "for i in range(1000): \n", "    v = policy_evaluation(env, discount, agent, v_old) \n", "    \n", "    if (np.max(np.abs(v-v_old))<1e-6): \n", "        break \n", "        \n", "    v_old = v \n", "    agent.probs = greedy_policy(env, discount, agent, v) \n", "\n", "print(v.reshape(4,4)) \n"]}, {"cell_type": "markdown", "id": "daec2b6e-05d8-4a94-97ed-d5c20fdf3a1e", "metadata": {"editable": false, "deletable": false}, "source": ["When you have found a new policy you can try it out on the environment. \n", "\n", "___Note:___ You can see from above that the value of each state is strictly less than one even for the optimal policy. Since you get a total reward of $+1$ if you reach the goal, and total reward 0 if you fall into a hole, this means that even for the optimal policy there is a non-zero probability that you will fall into a hole. However, the probability that you reach the goal is much higher for the optimal policy than it was for the uniformly random policy. "]}, {"cell_type": "code", "execution_count": null, "id": "56c9885a-9215-45b0-b751-df7423013328", "metadata": {}, "outputs": [], "source": ["run_agent(env, agent)"]}, {"cell_type": "markdown", "id": "f88c751c-b167-409f-9642-390dee57ef07", "metadata": {"editable": false, "deletable": false}, "source": ["# 8. Value iteration <a id=\"sec8\">"]}, {"cell_type": "markdown", "id": "3eacf48f-e515-4935-a015-737f9257624b", "metadata": {"editable": false, "deletable": false}, "source": ["In the value iteration we instead start from the Bellman optimality equation\n", "\n", "$$\n", "v_{*}(s) = \\max_{a} q_{\\pi_*}(s,a) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_{*}(s')]\n", "$$\n", "\n", "We start with an initial guess $v_0$ and then we repeatedly compute the right-hand side of this equation, until we converge to the optimal state-value function. When we have the optimal state-value function $v_*$, we can take any policy that is greedy w.r.t $v_*$ and this will give us an optimal policy. \n", "\n", "**Task 1:** Complete the code below. Pseudo-code for the algorithm can be found on page 83 in the textbook. Note that the code for computing the action-value given $v_{\\pi}$ has already been implemented above.\n", "\n", "The `value_iteration` function will (if implemented correctly) give back the optimal value function. \n", "\n", "**Task 2:** Also add some code for computing the optimal policy given this, and try it on `FrozenLake-v0` and/or `GridWorld-v0`."]}, {"cell_type": "code", "execution_count": null, "id": "e79871ca-72b5-4852-9433-6fe35a1c69f2", "metadata": {"tags": []}, "outputs": [], "source": ["def value_iteration(env, discount, agent, v0, max_iter=1000, tol=1e-6):\n", "    \n", "    v = v0\n", "    \n", "    for i in range(max_iter): # Loop\n", "        delta = 0\n", "        for s in range(env.observation_space.n):\n", "            vs = v[s]\n", "            \n", "            action_values = np.zeros(env.action_space.n) \n", "            \n", "            for a in range(env.action_space.n): \n", "                action_values[a] = compute_action_value(env, discount, s, a, v) \n", "            \n", "            v[s] = np.max(action_values) \n", "            \n", "            delta = np.max([delta, np.abs(vs-v[s])])\n", "            \n", "        if (delta < tol): # Until delta < tol\n", "            break\n", "            \n", "    print(i) \n", "            \n", "    return v    "]}, {"cell_type": "code", "execution_count": null, "id": "068bd729-839a-4d93-b8a3-9a423c05210e", "metadata": {}, "outputs": [], "source": ["env = gym.make('FrozenLake-v1', render_mode=\"human\")\n", "agent = RandomAgent()\n", "discount = 1\n", "\n", "v0 = np.zeros(env.observation_space.n) \n", "v = value_iteration(env, discount, agent, v0) \n", "agent.probs = greedy_policy(env, discount, agent, v) \n", "print(v.reshape(4,4)) \n"]}, {"cell_type": "markdown", "id": "d7cdf188-10ad-4fe8-80a9-7f60f82dbb25", "metadata": {"editable": false, "deletable": false}, "source": ["**Checking your results:** If everything is done correctly, the value iteration should find the same optimal value function as before. "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.16"}}, "nbformat": 4, "nbformat_minor": 5}