{"cells": [{"cell_type": "markdown", "id": "ef71d5c6-d0e1-4a78-9224-aa76727f3d3f", "metadata": {"editable": false, "deletable": false}, "source": ["# Tinkering Notebook 4: Planning and Learning\n", "\n", "In this notebook we look at the methods from Chapter 8 of the textbook. \n", "\n", "This notebook focuses on model based reinforcement learning and integration of learning and planning. Another important theme in the notebook is the exploitation-exploration trade-offs.  We will implement Dyna-Q and Dyna-Q+ and run these on the various maze environments of Example 8.1, 8.2 and 8.3.\n", "    \n", "Note that in this notebook we do not perform averaging over runs,  hence we typically have considerable amount of randomness in our results. This makes it difficult to draw general conclusions about the effect of parameters. You may want to perform averaging over runs after your initial experiments to be able to understand the effect of parameters better. "]}, {"cell_type": "markdown", "metadata": {"editable": false, "deletable": false}, "source": ["# Table of content\n", "* ### [1. Imports](#sec1)\n", "* ### [2. The maze environments](#sec2)\n", "* ### [3. DYNA-Q](#sec3)\n", " * #### [3.1 Example: Maze](#sec3_1)\n", "* ### [4. DynaQ+](#sec4)\n", " * #### [4.1 Example: Blocking Maze](#sec4_1)\n", " * #### [4.2 Example: Shortcut Maze](#sec4_2)\n"]}, {"cell_type": "markdown", "id": "67341bef-27ca-4a45-a11c-f1feaba0a105", "metadata": {"editable": false, "deletable": false}, "source": ["# 1. Imports <a id=\"sec1\">"]}, {"cell_type": "code", "execution_count": null, "id": "e07ea0fc-3549-4e92-869c-d58f69ea1c12", "metadata": {}, "outputs": [], "source": ["import gymnasium as gym\n", "import gym_RLcourse\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."]}, {"cell_type": "markdown", "id": "1cb7b9d1-f2a4-4534-b42a-bb9a9f94f857", "metadata": {"editable": false, "deletable": false}, "source": ["# 2. The maze environments <a id=\"sec2\">"]}, {"cell_type": "markdown", "id": "fd97c288-e5bd-45ea-9972-2b3737193661", "metadata": {"editable": false, "deletable": false}, "source": ["In this notebook we will use the environments in Example 8.2 and Example 8.3 in the textbook. These are implemented in `GridWorld-Maze-v0` and you can select between the different mazes by setting `map_name`:\n", "\n", "* `map_name = \"Figure 8.2\"` (default) gives the map in Figure 8.2.\n", "* `map_name = \"Figure 8.4a\"` gives the left map in Figure 8.4. \n", "* `map_name = \"Figure 8.4b\"` gives the right map in Figure 8.4.\n", "* `map_name = \"Figure 8.5a\"` same as Figure 8.4b. \n", "* `map_name = \"Figure 8.5b\"` gives the right map in Figure 8.5."]}, {"cell_type": "code", "execution_count": null, "id": "e305097e-8563-4bd6-9f66-ff60664a9eab", "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.2\", render_mode=\"human\")\n", "state, info = env.reset()\n", "print('State space:', env.observation_space)\n", "print('Action space:', env.action_space)"]}, {"cell_type": "markdown", "id": "31e2db12-d7f6-4b93-8893-05cbe6d80d92", "metadata": {"editable": false, "deletable": false}, "source": ["There are 54 states, and 4 actions corresponding to "]}, {"cell_type": "code", "execution_count": null, "id": "f04dff60-a733-4b4c-a91d-56a80800add5", "metadata": {}, "outputs": [], "source": ["WEST = 0\n", "SOUTH = 1\n", "EAST = 2\n", "NORTH = 3"]}, {"cell_type": "markdown", "id": "772d7908-a39f-4778-947f-16c2cf9bcfc9", "metadata": {"editable": false, "deletable": false}, "source": ["As usual we move around in the environment using `env.step`. "]}, {"cell_type": "code", "execution_count": null, "id": "bf13fa6d-fd51-4237-883f-2b9f0906b48b", "metadata": {}, "outputs": [], "source": ["env.step(NORTH)"]}, {"cell_type": "markdown", "id": "71b8c46c-d747-4b1b-8c2b-d47331d5b846", "metadata": {"editable": false, "deletable": false}, "source": ["**Reward:** The reward is 0 for every action, except when the goal state is reached and a reward of +1 is received. "]}, {"cell_type": "markdown", "id": "e94e6e57-20bb-495a-900b-84dbbd5cd264", "metadata": {"tags": [], "editable": false, "deletable": false}, "source": ["# 3. DYNA-Q <a id=\"sec3\">"]}, {"cell_type": "markdown", "id": "17ce1065-4fda-4dca-a625-29cde9f416e1", "metadata": {"tags": [], "editable": false, "deletable": false}, "source": ["We now implement Tabular Dyna-Q. \n", "  \n", "The class `DynaQ` can be called to run either plain Dyna-Q or DynaQ+. \n", "\n", "If `dynaq_plus = True` then Dyna-Q+ is used, otherwise Dyna-Q is used. The Dyna-Q+ version is not fully implemented, but you will complete it later in the notebook. For now we focus on Dyna-Q. \n", "      \n", "__Task-DQ:__ Assuming `dynaq_plus=False` examine the class below and relate it to DynaQ. Especially look at `learn()`, `update_Q()` and `update_model()` and `plan()` and relate the implementation to Dyna-Q. Note that the comments on top of these functions indicate which lines of the pseudo-code on page 164 the code implements. "]}, {"cell_type": "code", "execution_count": null, "id": "eae30033-025b-4126-bce3-85bcef53c155", "metadata": {}, "outputs": [], "source": ["class DynaQ:\n", "\n", "    def __init__(self, n_states, n_actions, gamma, alpha, epsilon, n=0, dynaq_plus = False, kappa=1e-3):\n", "        self.n_states = n_states\n", "        self.n_actions = n_actions\n", "        self.gamma = gamma\n", "        self.alpha = alpha\n", "        self.epsilon = epsilon\n", "        self.n = n\n", "        self.dynaq_plus = dynaq_plus\n", "        self.kappa = kappa\n", "        \n", "        self.Q = np.zeros((n_states, n_actions)) # Estimated Q-function\n", "        self.visited = np.zeros((n_states, n_actions)) # 1 if [s,a] has been visited.\n", "        self.model = {}\n", "        \n", "        # Extra information needed for DynaQ-plus\n", "        if dynaq_plus:\n", "            self.time = 0 # Internal counter of time\n", "            self.last_visit = np.zeros((n_states, n_actions)) # Last visit to state/action-pair\n", "            for state in range(n_states):\n", "                # See footnote on page 168\n", "                self.visited[state] = 1\n", "                for action in range(n_actions):\n", "                    self.model[state, action] = state, 0\n", "        \n", "        \n", "    # Tabular DynaQ step (b)\n", "    def act(self, state):\n", "        # Epsilon-greedy policy\n", "        if np.random.rand() > self.epsilon: \n", "            # Greedy action\n", "            # Break ties randomly\n", "            a_max = np.where(self.Q[state,:] == np.max(self.Q[state,:]))[0]\n", "            action = np.random.choice(a_max) \n", "        else: \n", "            # Random action\n", "            action = np.random.choice(self.n_actions)\n", "        \n", "        return action\n", "            \n", "    # Tabular DynaQ step (d) (the Q-learning update)\n", "    def update_Q(self, state, action, reward, state_next):\n", "        Q_next = np.max(self.Q[state_next, :])\n", "        self.Q[state, action] += self.alpha*(reward + self.gamma*Q_next - self.Q[state,action])\n", "        \n", "    # Tabular DynaQ step (e)\n", "    def update_model(self, state, action, reward, state_next):    \n", "        self.visited[state, action] = 1 # We have now seen this state\n", "        self.model[state, action] = state_next, reward\n", "        \n", "        if self.dynaq_plus:\n", "            self.last_visit[state, action] = self.time\n", "            \n", "        \n", "    # Tabular DynaQ step (f)\n", "    def plan(self):\n", "        #All visisted state-action pairs:\n", "        v_states, v_actions = np.nonzero(self.visited)\n", "        \n", "        #Pick out self.n random visisted pairs\n", "        idx = np.random.choice(len(v_states), self.n)\n", "        \n", "        for i in idx:\n", "            state = v_states[i]\n", "            action = v_actions[i]\n", "            [state_next, reward] = self.model[state, action]\n", "            \n", "            if self.dynaq_plus:\n", "                time_passed = self.time - self.last_visit[state, action]\n", "                # TODO modify reward\n", "            \n", "            self.update_Q(state, action, reward, state_next)\n", "    \n", "    # Tabular DynaQ calls (d), (e) and (f)\n", "    def learn(self, state, action, reward, state_next):\n", "        self.update_Q(state, action, reward, state_next)\n", "        self.update_model(state, action, reward, state_next)\n", "        self.plan()    \n", "        \n", "        if self.dynaq_plus:\n", "            self.time += 1"]}, {"cell_type": "markdown", "id": "81ec8041-b7af-4991-9435-2d623e43e0f8", "metadata": {"editable": false, "deletable": false}, "source": ["We now introduce `train()` function for ease of experimentation. Note that the function returns the length of episodes in `length_episodes` and rewards over all time steps `rewardA`. \n"]}, {"cell_type": "code", "execution_count": null, "id": "74c01791-6c0f-4e76-932d-5976545bf5e7", "metadata": {}, "outputs": [], "source": ["def train(env, agent, nEpisode):\n", "    episode_length = np.inf*np.ones(nEpisode)\n", "    rewardA = []\n", "    \n", "    for iEpisode in range(nEpisode):\n", "        t = 0  # step index within episode\n", "        state, info = env.reset()\n", "        terminated = False\n", "        truncated = False\n", "        \n", "        while not truncated and not terminated:\n", "            action = agent.act(state)\n", "            state_next, reward, terminated, truncated, info = env.step(action)\n", "            agent.learn(state, action, reward, state_next)\n", "            rewardA.append(reward)\n", "            t += 1\n", "            state = state_next\n", "            \n", "        episode_length[iEpisode] = t\n", "        \n", "    print('Finished')\n", "    return episode_length, rewardA"]}, {"cell_type": "markdown", "id": "cd5d9015-1305-496e-ade5-6b9d05a0a703", "metadata": {"editable": false, "deletable": false}, "source": ["We now define three functions that can be used to illustrate the behavior of the agent. \n", "\n", "`plot_episode_length` plots the number of step the agent takes before it finish for each episode during the training. This can be compared with Figure 8.2 in the textbook.\n", "\n", "`plot_cumulative_rewards` plot the cumulative rewards similar to what is done in Figure 8.4 in the textbook. \n", "\n", "`test_policy` takes an environment `env` (assuming `render_mode=\"human\"`) and lets the `agent` run through one episode.\n", "\n", "`plot_policy` gives a visualization of the agents current policy similar to that in Figure 8.3. "]}, {"cell_type": "code", "execution_count": null, "id": "4676b499-ab8b-495a-8db8-c3e81eea5d95", "metadata": {}, "outputs": [], "source": ["def plot_episode_length(episode_length): # for plots similar to Figure 8.2\n", "    plt.plot(episode_length, label='DYNA-Q')\n", "    plt.xlabel('Episodes')\n", "    plt.ylabel('Steps per Episode')\n", "    plt.legend(loc='best')\n", "    plt.ylim(-5, 800)\n", "    plt.grid(True)\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "29f12961-ed47-4f89-9c96-d5fa8a3f46b9", "metadata": {}, "outputs": [], "source": ["def plot_cumulative_rewards(rewards1, rewards2): # for plots similar to Figure 8.4\n", "    rewards1_cumulative = np.cumsum(rewards1)\n", "    rewards2_cumulative = np.cumsum(rewards2)\n", "    rewards2_cumulative = np.array(rewards2_cumulative) + rewards1_cumulative[-1]\n", "    rewards_cumulative = np.concatenate([rewards1_cumulative, rewards2_cumulative])\n", "    episode_change = len(rewards1)\n", "    plt.plot(rewards_cumulative)\n", "    plt.plot([episode_change, episode_change], [0, rewards_cumulative[-1]], '--', color='black')\n", "    plt.xlabel('Time Steps')\n", "    plt.ylabel('Cumulative Reward')\n", "    plt.grid(True)\n", "    plt.show()\n", "    \n", "    return rewards_cumulative"]}, {"cell_type": "code", "execution_count": null, "id": "3a972c8e-ed52-41a2-90ca-e4309a4b9a6e", "metadata": {}, "outputs": [], "source": ["def test_policy(env, agent, max_steps=50, render=True):\n", "    state, info = env.reset()\n", "    step = 0\n", "    terminated = False\n", "    truncated = False\n", "    while not terminated and not truncated and step<max_steps:\n", "        action = agent.act(state)\n", "        state, reward, terminated, truncated, info = env.step(action)\n", "        step += 1\n", "        if render:\n", "            clear_output(wait=True)\n", "            # Show some information\n", "            print(\"Time step:\", step)\n", "            print(\"Action:\", action)\n", "            print(\"Reward:\", reward)\n", "            \n", "    if terminated == True:\n", "        print(\"Finished after\", step, \" steps.\")\n", "    else:\n", "        print(\"Did not finish within\", step, \"steps.\")"]}, {"cell_type": "code", "execution_count": null, "id": "2e2751ca-51ac-4fc4-afe0-ff25c3b8c95f", "metadata": {}, "outputs": [], "source": ["def plot_policy(env, agent):\n", "    directions = {0: ' W ', 1: ' S ', 2: ' E ', 3: ' N '}\n", "    for state in range(env.observation_space.n):\n", "        q = agent.Q[state, :]\n", "        \n", "        if np.ravel(env.desc)[state] == b'W':\n", "            # If it is a \"wall\"-state\n", "            action = ' # '\n", "        elif np.ravel(env.desc)[state] == b'G':\n", "            # If it is a \"goal\"-state\n", "            action = ' G '\n", "        elif np.count_nonzero(q == q[0]) == len(q):\n", "            # If all actions have the same value\n", "            action = ' - '\n", "        else:\n", "            action = directions[np.argmax(q)]\n", "            \n", "        print(action, end='')\n", "        position = np.unravel_index(state, [env.nrow, env.ncol])\n", "        if position[1] == env.ncol -1:\n", "            print('')\n", "    print('')\n", "        "]}, {"cell_type": "markdown", "id": "f9225880-dda3-4be5-aea0-c2c18a791007", "metadata": {"editable": false, "deletable": false}, "source": ["## 3.1 Example: Maze <a id=\"sec3_1\">"]}, {"cell_type": "markdown", "id": "fd3f259d-5fe4-4232-af18-228d91e3667b", "metadata": {"editable": false, "deletable": false}, "source": ["We create the DynaMaze environment of Example 8.2 and the Dyna-Q agent.\n", "\n", "**Task-DM:** What is the length of the episode under an optimal deterministic strategy in the DynaMaze environment?\n", "\n", "Let us create a training environment (without rendering) and an agent that uses $n=5$ planing steps."]}, {"cell_type": "code", "execution_count": null, "id": "ed9dee71-12d2-41d2-b1fc-6ee2ee59e4ac", "metadata": {}, "outputs": [], "source": ["env_train = gym.make(\"GridWorld-Maze-v0\")\n", "agent = DynaQ(env.observation_space.n, env.action_space.n, gamma=0.95, alpha=0.1, epsilon=0.1, n=5)"]}, {"cell_type": "markdown", "id": "c036164e-9f2f-4b33-8f04-fc9f16030247", "metadata": {"editable": false, "deletable": false}, "source": ["We now let the agent train on the environment for 50 episodes. Wait until you see `Finished`."]}, {"cell_type": "code", "execution_count": null, "id": "daeea705-1c6c-491f-833b-abf288e23c2f", "metadata": {}, "outputs": [], "source": ["episode_length, rewardA = train(env_train, agent, nEpisode = 50)"]}, {"cell_type": "markdown", "id": "50d67bfe-12f8-450b-ae53-3c3389aa9f2b", "metadata": {"editable": false, "deletable": false}, "source": ["Let us now plot the total number of steps per episode, similar to Figure 8.2. However, there are two differences compared to Figure 8.2. First, we have only trained the agent once and the results may vary a bit every time you re-train the agent. Figure 8.2 on the other hand show the average when the agent is trained many times. Furthermore, we here plot the number of steps also for the first episode, while Figure 8.2 starts at episode 2."]}, {"cell_type": "code", "execution_count": null, "id": "34cc61bf-b0f1-4c22-ade4-2f9239b8a289", "metadata": {}, "outputs": [], "source": ["plot_episode_length(episode_length)"]}, {"cell_type": "markdown", "id": "b3301d3f-e4ee-47bc-9d86-46d976d56a08", "metadata": {"editable": false, "deletable": false}, "source": ["If you want to see your trained agent in action, you can use:"]}, {"cell_type": "code", "execution_count": null, "id": "7622793f-6847-4843-9458-7215a89cd779", "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.2\", render_mode=\"human\")\n", "test_policy(env, agent)"]}, {"cell_type": "markdown", "id": "20b2a454-718b-4db7-a367-e33e8154aafd", "metadata": {"editable": false, "deletable": false}, "source": ["We can also see what greedy policy the agent currently has found:"]}, {"cell_type": "code", "execution_count": null, "id": "9ed72004-c99f-4077-a7e1-21429990de5e", "metadata": {}, "outputs": [], "source": ["plot_policy(env_train, agent)"]}, {"cell_type": "markdown", "id": "19fffc86-7241-4c61-9974-d903c17db093", "metadata": {"editable": false, "deletable": false}, "source": ["**Task-DQ1**: Vary the number of planning steps `n` and plot steps per episode again. Repeat your experiments with the same parameters multiple times. Compare your results to Figure 8.2."]}, {"cell_type": "code", "execution_count": null, "id": "28dd8279-229b-42f1-aba7-acc732594c7c", "metadata": {}, "outputs": [], "source": ["agent = DynaQ(env.observation_space.n, env.action_space.n, gamma=0.95, alpha=0.1, epsilon=0.1, n=5)\n", "episode_length, rewardA = train(env_train, agent, nEpisode = 50)\n", "plot_episode_length(episode_length)"]}, {"cell_type": "markdown", "id": "478bebae-48fb-4420-8717-6e2d55e51bdc", "metadata": {"editable": false, "deletable": false}, "source": ["**Task-DQ2:** Look at the evolution of the policy during training. You can do this by running one episode at a time, and then plot the policy. Vary the number of planning steps `n` (remember that with `n=0` you get $Q$-learning). How do your results compare to the discussion in Example 8.1 of the textbook?"]}, {"cell_type": "code", "execution_count": null, "id": "8ed880be-90be-4cc4-b1dd-bf5f27f94f7d", "metadata": {}, "outputs": [], "source": ["agent = DynaQ(env.observation_space.n, env.action_space.n, gamma=0.95, alpha=0.1, epsilon=0.1, n=5)"]}, {"cell_type": "code", "execution_count": null, "id": "9c33a9fb-75f0-4427-972e-67fe0d7d4e44", "metadata": {}, "outputs": [], "source": ["episode_length, rewardA = train(env_train, agent, nEpisode = 1)\n", "plot_policy(env_train, agent)"]}, {"cell_type": "markdown", "id": "3b711e36-6ed0-4108-a05c-52f29a8373ed", "metadata": {"editable": false, "deletable": false}, "source": ["To train for one more episode, re-run the second code block (if you re-run the first code block then your agent re-starts from scratch)."]}, {"cell_type": "markdown", "id": "7dd7f75e-fb1d-45a5-af7f-36f07d3950ef", "metadata": {"editable": false, "deletable": false}, "source": ["**Task-DQ3**: Recall your answer to Task-DM (the optimal number of steps). Even after obtaining the optimal policy, the agent may use a higher number of steps to reach the goal. Why? If you have trained the agent long enough to have an optimal policy, you can test it a few times with the code below. Does it use the same number of steps every time?"]}, {"cell_type": "code", "execution_count": null, "id": "1df3f0ab-5954-4e32-938f-22084fff317c", "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.2\", render_mode=\"human\")\n", "test_policy(env, agent)"]}, {"cell_type": "markdown", "id": "0e18964b-9651-49e9-9176-0fdf200fb871", "metadata": {"editable": false, "deletable": false}, "source": ["# 4. DynaQ+ <a id=\"sec4\">"]}, {"cell_type": "markdown", "id": "899edde9-6a15-42ae-a09f-edb640f19682", "metadata": {"editable": false, "deletable": false}, "source": ["We now consider DynaQ+ (page 168). Note that this is a variant of Dyna-Q and no separate algorithm is provided on the page on page 168. You need to read the text and the footnote to understand how Dyna-Q+ is different from Dyna-Q. \n", "\n", "__Task-DQ+ 1:__ In `DynaQ()` class above, $\\color{blue}{\\text{the reward modification is not implemented for  DynaQ+}}$. In particular, we have not implemented the following statement from the book: \"... if the modeled reward for a transition is $r$, and the transition has not been tried in $\\tau$ time steps, then planning updates are done as if that transition produced a reward of $r + \\kappa \\sqrt{\\tau}$, for some small $\\kappa$\". Complete the implementation. The location of the code you need to fill in can be found by searching for the \"TODO\" in this file. \n", "\n", "__Task-DQ+ 2:__ Relate the idea behind Dyna-Q+ to the exploitation-exploration trade-off."]}, {"cell_type": "markdown", "id": "827a2eda-bcf4-4481-8412-c33c0adebb60", "metadata": {"editable": false, "deletable": false}, "source": ["## 4.1 Example: Blocking Maze <a id=\"sec4_1\">"]}, {"cell_type": "markdown", "id": "bfe59009-7bca-447a-9e63-e50e1720a346", "metadata": {"editable": false, "deletable": false}, "source": ["We now focus on Example 8.2. Here we will switch between two different environments. When the agent is training it will use the `map_name=\"Figure 8.4a` for 20 episode:"]}, {"cell_type": "code", "execution_count": null, "id": "061bd4d9-97a9-46f3-939f-389f4f604c14", "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.4a\", render_mode=\"human\")\n", "state, info = env.reset()\n", "print('State space:', env.observation_space)\n", "print('Action space:', env.action_space)"]}, {"cell_type": "markdown", "id": "99160717-2be0-4fae-a285-526f075e2e81", "metadata": {"editable": false, "deletable": false}, "source": ["After this the environment is changed to `map_name=\"Figure 8.4b\"`, and the agent continues the training. "]}, {"cell_type": "code", "execution_count": null, "id": "f1855e41-746f-4fd9-bbf9-9bc836b8c59a", "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.4b\", render_mode=\"human\")\n", "state, info = env.reset()\n", "print('State space:', env.observation_space)\n", "print('Action space:', env.action_space)"]}, {"cell_type": "markdown", "id": "e31fe0dc-0e39-4293-8a61-8fa8ba886911", "metadata": {"editable": false, "deletable": false}, "source": ["To perform the training, you can use the code below."]}, {"cell_type": "code", "execution_count": null, "id": "d4f390cf-a255-4eb1-8835-b5a369cc4029", "metadata": {}, "outputs": [], "source": ["agent = DynaQ(env.observation_space.n, env.action_space.n, gamma=0.95, alpha=0.5, epsilon=0.1, n=20, dynaq_plus=True, kappa=0.001)\n", "env_train = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.4a\")\n", "steps_episodes1, rewards1 = train(env_train, agent, nEpisode=20)\n", "env_train = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.4b\")\n", "steps_episodes2, rewards2 = train(env_train, agent, nEpisode=20)\n", "print('Both finished')"]}, {"cell_type": "markdown", "id": "307a1c56-3cea-440d-bbaf-18fb506b9584", "metadata": {"editable": false, "deletable": false}, "source": ["We now plot the cumulative reward, similar to Figure 8.5 in the textbook (note however that we switch environment after 20 episodes instead of after 1000 time steps, and that Figure 8.5 shows the average over many runs). \n", "\n", "The dashed line shows when the environment switched. "]}, {"cell_type": "code", "execution_count": null, "id": "5c4aee18-e9b4-4418-82a1-de6832d9a949", "metadata": {}, "outputs": [], "source": ["plot_cumulative_rewards(rewards1, rewards2);"]}, {"cell_type": "markdown", "id": "03961340-0a39-4e6e-a25a-64fc296d98ca", "metadata": {"editable": false, "deletable": false}, "source": ["You will probably see that after the environment switch, the line goes flat for some time. This is because the agent first learned to use the short path but this is now blocked. Hence, in the first episode after the environment switch, the agent has to figure out a new way to the goal and thus the first episode after the switch will take longer time. \n", "\n", "It can be of interest to see how many time steps the agent took in the first and last episode before and after the switch. "]}, {"cell_type": "code", "execution_count": null, "id": "c925f03f-1487-403c-bc47-abfd50e91664", "metadata": {}, "outputs": [], "source": ["print(\"First episode BEFORE switch:\", steps_episodes1[0], \"steps.\")\n", "print(\"Last episode BEFORE switch:\", steps_episodes1[-1], \"steps.\")\n", "print(\"First episode AFTER switch:\", steps_episodes2[0], \"steps.\")\n", "print(\"Last episode AFTER switch:\", steps_episodes2[-1], \"steps.\")"]}, {"cell_type": "markdown", "id": "c27d0d39-ae49-4ef2-9b10-ee8ee04248e0", "metadata": {"editable": false, "deletable": false}, "source": ["**Task-BM1:** If you run the experiment a few times, you will see that the first episode is often shorter than the first episode after the switch. Why is that?\n", "\n", "**Task-BM2:** Try the experiments a few times with DynaQ instead of DynaQ+ (set `dynaq_pluse = False`). Do you see a difference in the number of steps for the first episode after the change? \n", "\n", "**Task-BM3:** Experiment with different values of $\\kappa$ in DynaQ+. Try extremely large and small values. How does $\\kappa$ change the results? Why?\n", "\n", "**Task-BM4:** Below are two code blocks. In the first block the agent is trained on Figure 8.4a, and then the policy learned is used on Figure 8.4b. In the second block the agent continue training, but now on Figure 8.4b. It trains for one episode, and then the policy used on Figure 8.4b. You can re-run the second code block several times the let the agent train more on the new environment and see how the policy evolves. Try different values of $\\kappa$ and $n$."]}, {"cell_type": "code", "execution_count": null, "id": "e0a03707-3806-4b3e-8e9f-81156076ab00", "metadata": {}, "outputs": [], "source": ["agent = DynaQ(env.observation_space.n, env.action_space.n, gamma=0.95, alpha=0.5, epsilon=0.1, n=20, dynaq_plus=False, kappa=0.001)\n", "env_train = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.4a\")\n", "steps_episodes1, rewards1 = train(env_train, agent, nEpisode=20)\n", "\n", "# Run the last policy from before the switch on the environment after the switch\n", "env = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.4b\", render_mode=\"human\")\n", "test_policy(env, agent, max_steps=30)"]}, {"cell_type": "code", "execution_count": null, "id": "65ea6468-7768-45e4-ac00-79337e198fa0", "metadata": {}, "outputs": [], "source": ["env_train = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.4b\")\n", "steps_episodes2, rewards2 = train(env_train, agent, nEpisode=1)\n", "\n", "# Run the policy when it has trained on the new environment for one episode\n", "env = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.4b\", render_mode=\"human\")\n", "test_policy(env, agent, max_steps=30)\n"]}, {"cell_type": "markdown", "id": "41f701e7-90c3-48b0-bc57-e13248f14f94", "metadata": {"editable": false, "deletable": false}, "source": ["## 4.2 Example: Shortcut Maze <a id=\"sec4_2\">"]}, {"cell_type": "markdown", "id": "094bc639-39ed-4bc4-8b8e-a1a6d06a15c5", "metadata": {"editable": false, "deletable": false}, "source": ["We now consider Example 8.3. Here the agent trains on Figure 8.5a for 100 episodes:"]}, {"cell_type": "code", "execution_count": null, "id": "7c99295f-1fac-4946-89c7-80981fd34646", "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.5a\", render_mode=\"human\")\n", "state, info = env.reset()\n", "print('State space:', env.observation_space)\n", "print('Action space:', env.action_space)"]}, {"cell_type": "markdown", "id": "dda9a14d-b9c9-4e40-82ca-41f28da5655e", "metadata": {"editable": false, "deletable": false}, "source": ["After this a shortcut opens up and Figure 8.5b is used instead for 100 episodes:"]}, {"cell_type": "code", "execution_count": null, "id": "b6b20471-7f60-4a90-8709-b808f2355fc9", "metadata": {}, "outputs": [], "source": ["env = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.5b\", render_mode=\"human\")\n", "state, info = env.reset()\n", "print('State space:', env.observation_space)\n", "print('Action space:', env.action_space)"]}, {"cell_type": "markdown", "id": "5c144eb7-2680-4b41-b439-ea76ecc689d6", "metadata": {"editable": false, "deletable": false}, "source": ["The training can be done with the following code:"]}, {"cell_type": "code", "execution_count": null, "id": "850dafe9-77a4-4818-aa0e-d3881d32dbce", "metadata": {}, "outputs": [], "source": ["agent = DynaQ(env.observation_space.n, env.action_space.n, gamma=0.95, alpha=0.5, epsilon=0.1, n=20, dynaq_plus=True, kappa=0.01)\n", "env_train = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.5a\")\n", "steps_episodes1, rewards1 = train(env_train, agent, nEpisode=100)\n", "\n", "# Uncomment the following if you want to see the agent in action with the policy\n", "# learnet from Figure 8.5a.\n", "#env = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.5b\", render_mode=\"human\")\n", "#test_policy(env, agent)\n", "\n", "env_train = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.5b\")\n", "steps_episodes2, rewards2 = train(env_train, agent, nEpisode=150)\n", "\n", "# Uncomment the following if you want to see the agent in action with the final policy.\n", "#env = gym.make('GridWorld-Maze-v0', map_name=\"Figure 8.5b\", render_mode=\"human\")\n", "#test_policy(env, agent)\n", "\n", "print('Both finished')"]}, {"cell_type": "code", "execution_count": null, "id": "168bcec9-d4cb-4e55-870f-c2d04a963c5d", "metadata": {}, "outputs": [], "source": ["plot_cumulative_rewards(rewards1, rewards2);"]}, {"cell_type": "code", "execution_count": null, "id": "19e5bd12-9b62-46b7-9154-fc65b2dda5b7", "metadata": {}, "outputs": [], "source": ["print(\"First episode BEFORE switch:\", steps_episodes1[0], \"steps.\")\n", "print(\"Last episode BEFORE switch:\", steps_episodes1[-1], \"steps.\")\n", "print(\"First episode AFTER switch:\", steps_episodes2[0], \"steps.\")\n", "print(\"Last episode AFTER switch:\", steps_episodes2[-1], \"steps.\")"]}, {"cell_type": "markdown", "id": "d7ac28d6-131b-4151-ae0b-16688196b8dd", "metadata": {"editable": false, "deletable": false}, "source": ["**Task-SM1:** Run the experiment above a few times with both DynaQ and DynaQ+ with different values of $n$. In the figure of cumulative reward, you will see that the agent learns to use the short-cut if the slope of the curve changes. If you want the see the policy just after the switch and the final policy in action uncomment the code for this in the code block above. \n", "\n", "**Task-SM2:** Try to vary $\\kappa$ in DynaQ. Try very large and very small values. Explain your results. (Note that the training is stochastic, so re-run your experiments a few time for each $\\kappa$)\n", "\n", "**Task-SM3:** The key behind DynaQ+ is that it encourage exploration of states that has not been seen for some time. Another way to get more exploration is of course to increase $\\varepsilon$. Can you make the agent detect the change in the environment faster by increasing $\\varepsilon$? What are potential drawbacks?\n", "\n", "**Task-SM4:** (\\*) Work on Exercise 8.4."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.16"}}, "nbformat": 4, "nbformat_minor": 5}