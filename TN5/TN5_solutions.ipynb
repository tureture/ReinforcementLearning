{"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"tags":[]},"source":["# Tinkering Notebook - Function Approximation \n","\n","**Note**: If the environment `Cooridor-v0` does not work, you have to update the package `gym_RLcourse`. See Studium for instructions. \n","\n","**Tile Coding, IMPORTANT**: For the example with `MountainCar` we will use the tile coding as features. For this we will use the Python code described on http://incompleteideas.net/tiles/tiles3.html in the notebook. You thus have to download the code at http://incompleteideas.net/tiles/tiles3.py-remove (select all, copy and save in a file `TildeCoding.py` in the same folder as this Jupyter Notebook). You can also read more about tile coding in Chapter 9.5.4. in the textbook."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["# Table of content\n","* ### [1. Imports](#sec1)\n","* ### [2. The corridor environment](#sec2)\n","* ### [3. Value-Function Approximation](#sec3)\n"," * #### [3.1 Linear Approximation with Polynomials and Fourier Basis ](#sec3_1)\n"," * #### [3.2 Monte-Carlo with value function approximation ](#sec3_2)\n"," * #### [3.3 Run the agent](#sec3_3)\n"," * #### [3.4 Discussions ](#sec3_4)\n","* ### [4. On-Policy Control with Function Approximation](#sec4)\n"," * #### [4.1 Linear Approximation with Tile Coding](#sec4_1)\n"," * #### [4.2 SARSA for Estimating Action-Value Function with Function Approximation](#sec4_2)\n"," * #### [4.3 The MountainCar Environment](#sec4_3)\n"," * #### [4.4 Discussions](#sec4_4)\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["# 1. Imports <a id=\"sec1\">"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Obtaining file:///Users/turehassler/ReinforcementLearning/TN5\n","\u001b[31mERROR: file:///Users/turehassler/ReinforcementLearning/TN5 does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":[]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import gym_RLcourse\n","import pickle # used to read pickle data\n","import TileCoding as tc\n","from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["# 2. The corridor environment <a id=\"sec2\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["In this notebook we will use the corridor environment described in Example 9.1 in the textbook. "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"ename":"NameNotFound","evalue":"Environment Corridor doesn't exist. ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m'\u001b[39;49m\u001b[39mCorridor-v0\u001b[39;49m\u001b[39m'\u001b[39;49m, n_starting_states\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, max_delta\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mState space:\u001b[39m\u001b[39m'\u001b[39m, env\u001b[39m.\u001b[39mobservation_space)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAction space:\u001b[39m\u001b[39m'\u001b[39m, env\u001b[39m.\u001b[39maction_space)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/envs/registration.py:592\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m         logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    587\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing the latest versioned environment `\u001b[39m\u001b[39m{\u001b[39;00mnew_env_id\u001b[39m}\u001b[39;00m\u001b[39m` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstead of the unversioned environment `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m         )\n\u001b[1;32m    591\u001b[0m     \u001b[39mif\u001b[39;00m spec_ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m         _check_version_exists(ns, name, version)\n\u001b[1;32m    593\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo registered env with id: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    595\u001b[0m _kwargs \u001b[39m=\u001b[39m spec_\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mcopy()\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/envs/registration.py:218\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m get_env_id(ns, name, version) \u001b[39min\u001b[39;00m registry:\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m _check_name_exists(ns, name)\n\u001b[1;32m    219\u001b[0m \u001b[39mif\u001b[39;00m version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[39mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/envs/registration.py:195\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    192\u001b[0m namespace_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m in namespace \u001b[39m\u001b[39m{\u001b[39;00mns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m ns \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m suggestion_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDid you mean: `\u001b[39m\u001b[39m{\u001b[39;00msuggestion[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m`?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m suggestion \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 195\u001b[0m \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mNameNotFound(\n\u001b[1;32m    196\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEnvironment \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt exist\u001b[39m\u001b[39m{\u001b[39;00mnamespace_msg\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m{\u001b[39;00msuggestion_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m )\n","\u001b[0;31mNameNotFound\u001b[0m: Environment Corridor doesn't exist. "]}],"source":["env = gym.make('Corridor-v0', n_starting_states=10, max_delta=2)\n","print('State space:', env.observation_space)\n","print('Action space:', env.action_space)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["**State Space:** There are `n_starting_states` non-terminal states, and two terminal states. \n","\n","**Action space:** You can go `max_delta` steps to the right or to the left, so there are in total `2*max_delta` actions.\n","\n","**Rewards:** The reward is 0 except when you reach a terminal state. In the left terminal state you get -1 and in the right terminal state +1.\n","\n","**Policy:** Below we will estimate the value function for this environment when the policy chooses between all actions with the same probability. "]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["Let's make a basic check on our installation for the corridor environment. When you run the code, you will see `n_starting_states` non-terminal states,  a terminal state on the left (TL) and another terminal state on the right (TR). Agent's location is indicated by `x` which is (approximately) halfway in the corridor at initialization. Other states are marked with o. "]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["state, info = env.reset()\n","env.render()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["# 3. Value-Function Approximation <a id=\"sec3\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We now consider the prediction problem using function approximation of the value function, $\\hat{v}(s,w)$, with the goal of obtaining  $\\hat{v}(s,w)\\approx v_\\pi(s)$. "]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["## 3.1 Linear Approximation with Polynomials and Fourier Basis  <a id=\"sec3_1\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We first implement a linear approximator that uses either  polynomials or Fourier basis as features. \n","      \n","__Task-VA1:__ Below, variable `method` determines whether we use Fourier features or polynomial features. Compare `construct_basis` with the equations in Section 9.5.1 and Section 9.5.2 to understand how the features are implemented. The code uses `lambda` expressions, which is just another way to define functions. You can read about `lambda` expressions in Section 4.7.6 from here https://docs.python.org/3/tutorial/controlflow.html. With this implementation, we obtain a dictionary of basis functions which we can then evaluate for any value of state. \n","\n","__Task-VA2:__ Find which methods/variables in the code give the following: i)  $x(s)$ for a given state ii) $\\hat{v}(s,w)$ for a given state. \n","    \n","    \n","__Task-VA3:__ Complete the `update` function so that it implements Eqn. 9.7. Remember that for a linear function approximator $\\hat{v}(s, w) = w^\\top x(s)$ we get $\\nabla \\hat{v}(s,w) = x(s)$. (Also see the exercises on function approximation, where this is proven in an exercise).\n","   \n","__Task-VA4:__ In the implementation below, we have a variable called `StateRange` which normalizes the values of the states. Why do you think this is useful?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LinearApproximator:\n","\n","    def __init__(self, order, method=\"Fourier\", alpha=1e-4, stateRange=1):\n","        self.alpha = alpha\n","        self.method = method\n","        self.order = order\n","        self.weights = np.zeros(order + 1)\n","        self.stateRange = stateRange\n","        self.construct_basis()\n"," \n","    def construct_basis(self):\n","        self.basis=[]\n","        if self.method == \"Polynomial\":\n","            for i in range(0, self.order + 1):\n","                self.basis += [lambda s, i=i: np.power(s, i)]\n","        if self.method == \"Fourier\":\n","            for i in range(0, self.order + 1):\n","                self.basis += [lambda s, i=i: np.cos(np.pi * s * i) ]\n","\n","    # return the values of features for a given state \n","    def features(self, state): \n","        state = state / self.stateRange  # normalize to [0, 1]\n","        return np.array([f(state) for f in self.basis])\n","\n","    # return the value of the approximation for the given state\n","    def value(self, state):  \n","        return np.dot(self.features(state), self.weights)\n","\n","    # update the weights\n","    def update(self, target, state):  # update the weights using the eqn 9.7 where U_t is the target\n","        self.weights += self.alpha * (target - self.value(state))*self.features(state)  \n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["## 3.2 Monte-Carlo with value function approximation  <a id=\"sec3_2\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We now implement the Gradient Monte Carlo Algorithm on Page 202 using a random policy (i.e. all possible actions have the same probability). \n","\n","In the below code we implement both the tabular method and function approximation. You can then choose method with \n","\n","* `method = \"Tabular\"` - gives the tabular method\n","* `method = \"Fourier\"` - gives function approximation with Fourier basis functions.\n","* `method = \"Polynomial\"` - gives function approximation with polynomial basis functions.\n","\n","**Task-MC**: Implement the missing parts of the `learn` method which are marked with TODO-STD. Note that `self.func_approx` is a `LinearApproximator` and has the methods `self.func_approx.update()` and `self.func_approx.value()`."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["class MCAgent:\n","    # Accepts 'plain' or any method that LinearApproximator accepts. If the method is 'plain',\n","    # no function approximation is performed. The 'plain' option is included for comparison purposes.\n","    \n","    def __init__(self, n_states, n_actions, gamma, method, order, alpha, stateRange):\n","        self.n_actions = n_actions\n","        self.V = np.zeros(n_states)\n","        self.S = np.zeros(n_states)\n","        self.N = np.zeros(n_states)\n","        self.n_states = n_states\n","        self.gamma = gamma\n","        self.method = method \n","        self.func_approx = LinearApproximator(order, method, alpha, stateRange)\n","\n","    def act(self, state):\n","        return np.random.choice(self.n_actions)\n","    \n","    def value(self, state):\n","        if self.method == \"Tabular\": \n","            return self.V[state]\n","        else:\n","            return self.func_approx.value(state)\n","\n","    def learn(self, states, actions, rewards):\n","        T = len(states) - 1\n","        if self.method == \"Tabular\":   #  function approximation is NOT performed\n","            G = 0\n","            for t in reversed(range(T)):\n","                G = self.gamma * G + rewards[t + 1]  # G_t\n","                self.N[states[t]] += 1\n","                self.V[states[t]] += 1 / self.N[states[t]] * (G - self.V[states[t]])\n","        else:  #  Function approximation is performed\n","            # TODO-STD \n","            # Implement the last two lines of the MC-algorithm on page 202 in the textbook. \n","            G=0  \n","            for i in reversed(range(T)): \n","                G = self.gamma * G + rewards[i + 1] \n","                self.func_approx.update(G, states[i]) \n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["## 3.3 Run the agent <a id=\"sec3_3\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We first create the environment that will be used for training. Since the training can take a long time if you use a large number of states and actions, we will first use `n_starting_states=100` and `max_delta=10`. When you are done experimenting, you can change this to `n_starting_states=1000` and `max_delta=100` to get the environment used in Example 9.1-9.2 and Figure 9.5 in the textbook. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env = gym.make('Corridor-v0', n_starting_states=100, max_delta=10)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We then create an agent that uses function approximation with Fourier basis functions. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stateRange = env.observation_space.n - 2 # Number of non-terminal states\n","# By changing method you can get \"Tabular\", \"Fourier\" or \"Polynomial\"\n","agent = MCAgent(env.observation_space.n, env.action_space.n, gamma=1, method=\"Fourier\", order=5, alpha=5e-5,\n","                stateRange=stateRange)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We now run the agent on the environment. __Note that this may take some time, wait until you see 'Finished'.__ "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nEpisode = 5000\n","max_nStep = int(2.0e6)\n","print('Run starts')\n","for iEpisode in range(1,nEpisode+1):\n","    if iEpisode % 500 == 0:\n","        print('Episode', iEpisode)\n","    state, info = env.reset()\n","    stateA = [state]\n","    action = agent.act(state)\n","    actionA = [action]\n","    rewardA = [0]\n","    terminated, truncated = False, False\n","    t_done = -1\n","    while not (terminated or truncated):\n","        state, reward, terminated, truncated, info = env.step(action)\n","        stateA.append(state)\n","        rewardA.append(reward)\n","        action = agent.act(state)\n","        actionA.append(action)\n","        if t_done > max_nStep:\n","            done = True\n","        t_done += 1\n","    agent.learn(stateA, actionA, rewardA)\n","print('Finished')"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We now compare the learned value function with the true value function. \n","\n","__Note:__ We here show the true value function for `n_starting_states=100` and `max_delta=10`, so it is not the same as the one in Figure 9.1 which has 1000 non-terminal states.\n","\n","__Task-TVF:__ Suggest a method to find the true value function using our knowledge of the environment. (That is, knowing all the transition probabilities etc)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open('RandomWalk_100.pickle', 'rb') as file: # load the pre-calculated true values\n","    v_true = pickle.load(file)\n","\n","# Get the values for all states from the agent\n","V = np.zeros(env.observation_space.n)\n","for i in range(0, len(V)):\n","    V[i] = agent.value(i)\n","    \n","plt.plot(v_true[1:-1],label='True')\n","plt.plot(V[1:-1],label='Prediction')\n","plt.grid(True)\n","plt.xlabel('States')\n","plt.ylabel('Value Function')\n","plt.ylim(-1.1, 1.1)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["## 3.4 Discussions  <a id=\"sec3_4\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We will now compare our results with the ones in Figure 9.5. Nevertheless, note that we  should not expect to obtain exactly the same results as the book. In particular, note the following: i) We do not have any averaging over multiple runs, hence there is considerable randomness in our results. (Hence, after your first experiments, changing the code to average over multiple runs can be a good idea )  ii) Our corridor is a scaled-down version of the one in the book. \n","\n","\n","__Task-PD1__:   An important aspect is the definition of $\\overline{VE}$ in the book, see Eqn 9.1, which we try to minimize. A typical realization of the state distribution under a random agent can be seen in Figure 9.1. According to the Figure 9.1, the value function estimates of which states affect $\\overline{VE}$ more? State whether you think this is a good way of defining the error.  \n","\n","__Task-PD2__:   Relate $\\overline{VE}$ with  $E_\\pi[(v_\\pi(S)âˆ’\\hat{v}(S,w))^2]$ defined in the lecture slides.\n","\n","__Task-PD3__:   Repeat the experiment with Fourier basis and Polynomial basis with the order of 5, 10 and 20  multiple times.  In terms of the general trends, are the results consistent with Figure 9.5? In particular, check the following: \n","<ul>\n","<li> The book states that polynomials do not provide satisfactory results in RL tasks. The results in Figure 9.5 support this claim: Fourier basis shows better performance than polynomials. Are our results consistent with these claims? </li>\n","<li> Do  higher orders provide significantly better predictions?</li>\n","</ul>\n","\n","__Task-PD4__: Run the usual MC agent without function approximation using the option `method = 'Tabular'`. How does the value function prediction found in this case compare with the predictions using approximation? Which method(s) are *better*? Do your conclusions depend on the length of the corridor and the maximum allowed movement? \n","\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["# 4. On-Policy Control with Function Approximation <a id=\"sec4\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We now look at the control problem using the function approximation of the the action-value function $\\hat{q}(s,a,w)$ with the aim of obtaining $\\hat{q}(s,a,w) \\approx q_*(s,a)$. \n","\n","The updates of the weights for the estimate $\\hat{q}$ is given in equation (10.1) in the textbook, and is\n","\n","$$ w_{t+1} = w_t + \\alpha [U_t - \\hat{q}(S_t, A_t, w_t)] \\nabla \\hat{q}(S_t, A_t, w_t)$$\n","    \n","We will refer to this equation as Eqn 10.1 below. "]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["## 4.1 Linear Approximation with Tile Coding <a id=\"sec4_1\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We now implement a linear approximator that uses tile coding. Since tile coding operates in a slightly different manner than Fourier basis/polynomial, we provide a new linear approximator class for ease of exposition. Note that here each feature $x_i(s,a)$ is binary, i.e., it is either 0 or 1. We say that the feature is _active_ if it is 1. \n","\n","This means that we only need to know what indices of $x(s,a)$ that are active for each state-action pair. To implement this we use `TileCoding.py`.\n","See Section 9.5.4 of the book and  http://incompleteideas.net/tiles/tiles3.html for details. \n","    \n","__Task-TC1:__ Examine the usage of tile coding and make sure that you understand how we use the indices of active tiles as features. In particular, check the method `indicesActiveTiles`.\n","\n","__Task-TC2:__ Consider the method `value`. Verify that this method gives the approximation $\\hat{q}(s,a, w) = \\sum_{i} w_i x_i(s,a)$. \n","\n","__Task-TC3:__ Tile coding is just another set of features. Explain why the `value` method looks different than the one for Fourier basis and polynomials.  \n","    \n","__Task-TC4:__ Complete the `update` function using Eqn 10.1. Note that this `update` function may look different than the `update` function we had for Fourier basis and polynomials. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["class LinearApproximatorTile:\n","    def __init__(self, nTiling=8, size=4096, alpha=0.3, stateRange=1):\n","        self.size = size\n","        self.nTiling = nTiling\n","        self.iht = tc.IHT(size)\n","        self.weights = np.zeros(size) \n","        self.alpha = alpha\n","        self.stateRange = stateRange\n","\n","        # scaling for the states, see ``Fleshing out the example\" section on http://incompleteideas.net/tiles/tiles3.html\n","        # For the mountain-car example also see footnote 1 on page 246 of SuttonBarto_2018\n","        scaleFactor = self.nTiling\n","        self.scale = [scaleFactor / s for s in self.stateRange]\n","\n","    # get indices of active tiles (i.e features)\n","    def indicesActiveTiles(self, state, action):\n","        scaledState = [s * scale for s, scale in zip(state, self.scale)]\n","        return tc.tiles(self.iht, self.nTiling, scaledState, [action])\n","\n","    # calculate q_app(state, action)\n","    def value(self, state, action):\n","        ind = self.indicesActiveTiles(state, action)\n","        return np.sum(self.weights[ind])\n","\n","    # update the weights \n","    def update(self, target, state, action): # update the weights using  Eqn 10.1\n","        features = np.zeros(self.size) \n","        for i in self.indicesActiveTiles(state, action): \n","            features[i] = 1 \n","        self.weights = self.weights + self.alpha*(target - self.value(state, action))*features \n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["## 4.2 SARSA for Estimating Action-Value Function with Function Approximation <a id=\"sec4_2\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["The following class implements the algorithm *Episodic semi-gradient Sarsa for estimating $q_*$*  on page 244 of the book. You can also look at the algorithm from the lecture slides of Lecture 7.\n","\n","**Task-Sarsa:** Complete the `learn` method. \n","\n","\n","***Note***: With function approximation we have to handle the case when $S_{t+1}$ is terminal as a special case. When we used tabular methods, we set the initial value of any terminal state to 0, and then we never updated it, so it would always be zero and there for $Q(S_{t+1}, A_{t+1}) = 0$ if $S_{t+1}$ is a terminal state. However, now when we use function approximation this is not true anymore, since changes in $w$ also affect the estimated value of the terminal states. Hence, the target in the update should be \n","\n","$$\n","U_t = \\begin{cases} R_{t+1} & \\text{if } S_{t+1} \\text{ is terminal} \\\\ R_{t+1} + \\gamma \\hat{v}(S_{t+1},A_{t+1},w) & \\text{otherwise}\\end{cases}\n","$$\n","\n","To be able to do this, we also pass the flag `terminated` to `learn`. So if `terminated == True` then `state_next` is a terminal state."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["class SARSA:\n","    def __init__(self, gamma, epsilon, alpha, stateRange):\n","        self.n_actions = 3  # 0,1,2:  reverse, stay, go forward\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.epsilon = epsilon\n","        self.func_approx = LinearApproximatorTile(nTiling=8, size=4096, alpha=alpha, stateRange=stateRange)\n","\n","    def act(self, state): \n","        if np.random.random_sample() <= self.epsilon:  # random action wp epsilon  \n","            action = np.random.choice(np.arange(0,self.actions))\n","        else:  # greedy action wp 1-epsilon\n","            # Get all action values\n","            Q = np.zeros(self.n_actions)\n","            for action in range(0, self.n_actions):\n","                Q[action] = self.func_approx.value(state, action)\n","                \n","            # Take greedy action, and choose randomly when there is a tie\n","            a_max = np.where(Q == np.max(Q))[0]\n","            action = np.random.choice(a_max) \n","        return action\n","\n","\n","    # Sutton, Barton pg.244\n","    def learn(self, state, action, reward, state_next, action_next, terminated): \n","        # TODO-STD Implement the weight updates according to Sarsa pseudo-code on page 244\n","        \n","        if terminated:  \n","            self.func_approx.update(reward, state, action) \n","        else: \n","            self.func_approx.update(reward + self.gamma*self.func_approx.value(state_next, action_next), state, action) \n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We now introduce the training function, and also a test function where you can test your trained agents policy."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(env, agent, nEpisode):\n","    total_reward_episodes = np.zeros(nEpisode)\n","    max_nStep = 1000\n","    print('Run starts')\n","    for iEpisode in range(nEpisode):\n","        if iEpisode % 50 == 0:\n","            print('Episode', iEpisode)\n","        terminated, truncated = False, False\n","        t = 0\n","        state, info = env.reset()\n","        action = agent.act(state)\n","        rewardA = [0]\n","        while not terminated and not truncated:\n","            state_next, reward, terminated, truncated, info = env.step(action) \n","            action_next = agent.act(state_next)\n","            agent.learn(state, action, reward, state_next, action_next, terminated)\n","            action = action_next\n","            state  = state_next\n","            t += 1\n","            if t > max_nStep:\n","                truncated = True\n","                \n","            total_reward_episodes[iEpisode] += reward\n","\n","    print('Finished')\n","    return total_reward_episodes\n","\n","def test_car(agent, env, render=True): \n","    state, info = env.reset()\n","    step = 0\n","    total_reward = 0\n","    terminated = False\n","    truncated = False\n","    while not terminated and not truncated:\n","        action = agent.act(state)\n","        state, reward, terminated, truncated, info = env.step(action)\n","        total_reward += reward\n","        step += 1\n","        \n","        if render:\n","            clear_output(wait=True)\n","            env.render()\n","            # Show some information\n","            print(\"Time step:\", step)\n","            print(\"Reward:\", reward)\n","            print(\"Total reward:\", total_reward)\n","    return total_reward"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["## 4.3 The MountainCar Environment <a id=\"sec4_3\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We will test our SARSA-agent with function approximation on the `MountainCar-v0` environment you have seen in previous notebooks. The environment is also described in Example 10.1 of the textbook. \n","\n","Let us first look at the environment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env = gym.make('MountainCar-v0', render_mode=\"human\")\n","print(\"State space:\", env.observation_space)\n","print(\"Action space:\", env.action_space)\n","\n","state, info = env.reset()\n","print(\"Initial state:\", state)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["The state space is continuous, and the first element corresponds to the position of the car, while the second element corresponds to the velocity of the car. We can see that the position can go between -1.2 and 0.6, while the velocity is between -0.07 and 0.07. We will use this information to re-scale the sate in in the linear function approximator. \n","\n","To prepare for training, we create a new environment `env_train` that does not render. Furthermore, the `MountainCar-v0` has as a standard a time limit that truncates the episode if the car did not reach the goal within 200 steps. To be able to compare with Example 10.1 in the textbook, we do not want this artificial time limit (see in Figure 10.2 that the first episode used more than 1000 time steps). Therefore we ad `.env` when we create the environment, since this will remove the time limit wrapper."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env_train = gym.make('MountainCar-v0').env\n","\n","# state limits for Mountain-Car Environment\n","min_position = -1.2\n","max_position = 0.6  \n","max_velocity = 0.07\n","min_velocity = -0.07\n","stateRange = [max_position - min_position, max_velocity - min_velocity]\n","\n","agent = SARSA(gamma=1, epsilon=0, alpha=0.5/8, stateRange=stateRange)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We now train. __Note that training may take some time, wait until you see 'Finished'.__"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["total_reward_episodes = train(env_train, agent, nEpisode = 500)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We can now plot the number of of time steps each episode during training took. You can compare this with Figure 10.2 in the textbook. \n","\n","Note that the number of time steps is `-total_reward` since the reward in each time step is -1."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(-total_reward_episodes)\n","plt.xlabel('Episode')\n","plt.ylabel('Steps per episode')\n","plt.ylim(0, 1000)\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["And if you want to see your agent in action you can use:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_car(agent, env)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["## 4.4 Discussions <a id=\"sec4_4\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false},"source":["We now compare our results with the figures in the book. Similar to the before, note the following: in this notebook we do not perform averaging over runs hence we typically have more randomness in our results compared to the book. We should keep this point in mind while interpreting our results. You are encouraged  to perform averaging over the runs after your initial experiments.\n","\n","__Task-CD1:__  Consider Figure 10.2 which shows how the episode length varies with different learning rates as the agent trains. Try different $\\alpha$ values for the agent. Do you observe the same type of behaviour? Explain the reason behind the general trend as $\\alpha$ increases/decreases. \n","\n","__Task-CD2:__ It may also be interesting to go back to Tinkering Notebook 3b, where you tested to solve `MountainCar-v0` using state aggregation, and compare the results with using tile coding."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
