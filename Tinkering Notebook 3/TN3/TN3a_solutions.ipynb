{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f532b7-25c6-4d42-94d6-06bf97364772",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Tinkering Notebook 3a: Model-free prediction\n",
    "\n",
    "In Tinkering Notebook 2 we saw different ways to find the value function $v_\\pi(s)$ given that we know the dynamics $p(s', r | s, a)$. In this notebook we will see how we can learn $v_\\pi(s)$ in a model-free way using experience. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d741d2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Table of content\n",
    "* ### [1. Imports](#sec1)\n",
    "* ### [2. Monte-Carlo Methods](#sec2)\n",
    " * #### [2.1 Bias and variance](#sec2_1)\n",
    " * #### [2.2 Constant step size and non-stationary case](#sec2_2)\n",
    "* ### [3. Monte-Carlo Prediction](#sec3)\n",
    "* ### [4. Temporal Differences Prediction (TD)](#sec4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efe6469-a041-427a-9fb1-7e8873418328",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 1. Imports <a id=\"sec1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f2e51-6aef-4cb7-a088-608a6831f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_RLcourse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ee66f-4398-4db2-9589-7896ed21b99a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 2. Monte-Carlo Methods <a id=\"sec2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25a241-e4aa-4caf-9235-e28869764a43",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In this section we will look at the example with two dice from Lecture 4. \n",
    "\n",
    "The main point of this section is to get a better feeling for the ideas around bias and variance, and also take a look at the difference between a constant step size or a step size that decrease over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d47a51-e438-40a4-ad0e-3a481f961793",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 2.1 Bias and variance <a id=\"sec2_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afbea66-2078-45d8-8a18-0aed7036711a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We start with an example of throwing two dice, and we let $G$ be the sum we get from the two dices. We are interested in finding $\\mathbb{E}[ G ]$.\n",
    "With hand calculations it can be shown that $\\mathbb{E}[G]=7$, but this can be quite cumbersome to compute.  \n",
    "\n",
    "Here we instead carry out $N=1000$ throws with the two dice and compute the average value $V$. Remember that we can compute this incrementally using (see Lecture 4)\n",
    "$$\n",
    "V \\leftarrow V + \\frac{1}{n} ( G - V).\n",
    "$$\n",
    "In the code below we also store and then plot the estimated $V$ after each throw. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5d618-62ea-41ce-92b5-0df20c274921",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000 # Total number of throws\n",
    "V = np.zeros(N+1) # Will be used to store the mean values\n",
    "\n",
    "# V[0] is the initial value. (Should be zero to get true empircal mean)\n",
    "# V[1] is the mean after we have thrown the dices once etc.\n",
    "\n",
    "for n in range(1,N+1):\n",
    "    dice1 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "    dice2 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "    G = dice1 + dice2\n",
    "    V[n] = V[n-1] + 1/n*(G-V[n-1])\n",
    "    \n",
    "plt.plot(range(1,N+1), V[1:])\n",
    "plt.plot([1,N], [7, 7]); # True E[G]\n",
    "plt.ylim([4,10])\n",
    "plt.xlabel(\"$n$ - Number of throws\")\n",
    "plt.ylabel(\"$V$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5b703-6f4a-46cf-ba18-e820058e7b64",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:** Re-run the code cell above a few times to see that the results are different every time. Note that the difference between each run is larger for small $n$ than large $n$.\n",
    "\n",
    "**Task:** You can try to increase $N$ in the code above, to see that $V$ really converge to 7 as $n \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10acfb5-5a04-441e-943b-4bf7125b8f31",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The reason that we get different results in each run is because the observations are random, so `V[n]` is random. \n",
    "\n",
    "**Bias:** Tells us how much the expected value of `V[n]` differs from the true value (7). In Lecture 4 we saw that the bias in this case is 0, so `V[n]` is an unbiased estimate for all $n$. That is: If you run the code above (infinitely) many times `V[n]` will on average be 7 for all $n$.\n",
    "\n",
    "**Variance:** Tell us how much `V[n]` will vary around the expected value if we re-run the code many times. From running the code above many times, we see that it varies more for small $n$ than for large $n$. This is consistent with the fact that the variance is $5.83/n$ (see Lecture 4), and thus decreases as $n$ increases.\n",
    "\n",
    "**Consistency:** The estimate is consistent since $V \\rightarrow \\mathbb{E}[G] = 7$ as $n\\rightarrow \\infty$. If you let $N=100000$ or even larger, you will see that this is indeed the case for this dice example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2129a-5d58-4908-b759-03eb88286887",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 2.2 Constant step size and non-stationary case <a id=\"sec2_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee945ee-0481-4e49-a589-393b1838975a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The incremental update used above can be written as \n",
    "$$\n",
    "V \\leftarrow V + \\alpha_n ( G - V)\n",
    "$$\n",
    "where $\\alpha_n = 1/n$. These types of updates will come back over and over again in the course. \n",
    "\n",
    "Sometimes we will use a constant $\\alpha \\in (0,1)$. In this section we will study constant $\\alpha$ in the simple two dice example.\n",
    "\n",
    "The effect of choosing constant $\\alpha$ is intuitively that we put less weight on observations that happened a long time ago. For example, with $\\alpha = 1$ we get $V \\leftarrow G$, i.e., we forget everything that happened before the last observation completely (see textbook Chapter 2.5 for more discussion on this).\n",
    "\n",
    "The code below is the same as in the previous section, but with a constant step size `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe6805-97be-4625-9551-2dd39047a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "N = 1000 # Total number of throws\n",
    "V = np.zeros(N+1) # Will be used to store the mean values\n",
    "\n",
    "for n in range(1,N+1):\n",
    "    dice1 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "    dice2 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "    G = dice1 + dice2\n",
    "    V[n] = V[n-1] + alpha*(G-V[n-1])\n",
    "    \n",
    "plt.plot(range(1, N+1),V[1:])\n",
    "plt.plot([1,N], [7, 7]); # True E[G]\n",
    "plt.ylim([4,10])\n",
    "plt.xlabel(\"$n$ - Number of throws\")\n",
    "plt.ylabel(\"$V$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb632d-8b49-426a-b320-58084dc8164a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:** Vary `alpha` between 0 and 1. Try at least 0.5, 0.1 and 0.01. Also compare the result with the previous section where we used $\\alpha_n = 1/n$.\n",
    "\n",
    "**Task:** Do the same as above, but with $N=10000$ to see if the estimate seems to converge as $n\\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd25b7-3fa6-412e-95ad-39dbac02905a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "From the test above you should be able to see that:\n",
    "1. The estimate never converge to a fixed number. The reason for this is that $\\alpha$ does not go to zero, and therefor the estimate will continue to change for each new observation even as $n \\rightarrow \\infty$. Another view: The larger $\\alpha$ is, the more we focus on just the last couple of observations.\n",
    "2. But for large $n$, the estimate will vary around the true value.\n",
    "3. Smaller step size means that it takes longer to get close to the true value, but on the other hand it does not vary as much around the true value for large $n$.\n",
    "\n",
    "So, to learn fast use large $\\alpha$, but to get the result as accurate as possible as $n \\rightarrow \\infty$ use small $\\alpha$. Using the a step-size schedule such as $\\alpha_n = 1/n$, combines these insights by letting $\\alpha_n$ be large for small $n$ and small for large $n$. \n",
    "\n",
    "But why would we ever use a constant $\\alpha$ then? Some good reasons may be:\n",
    "1. Easier to implement. In more advanced settings than just computing the mean value, it is not always obvious how a good step-size schedule $\\alpha_n$ should look.\n",
    "2. It may not be important that the estimate converge to a fixed number (just that it gets close enough to the true value).\n",
    "2. If the underlying probabilities may change, it is important to continue to learn! If we let $\\alpha_n \\rightarrow 0$, then new observations will not matter very much when $n$ is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660de2fe-e14c-4cf5-b382-033f24665e00",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the code below we first do `N_two = 1000` throws with two dice. After this one dice gets lost. So then we perform `N_one = 500` throws with only one dice (so the expected value changes from 7 to 3.5 after throw 1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba3a36-45eb-4773-a46a-7df3e6c8e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_two = 1000 # Total number of throws with 2 dices\n",
    "N_one = 500 # Total number of throws with 1 dice\n",
    "N = N_one+N_two\n",
    "V = np.zeros(N+1) # Will be used to store the mean values\n",
    "\n",
    "for n in range(1,N+1):\n",
    "    if n<=N_two:\n",
    "        dice1 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "        dice2 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "        G = dice1 + dice2\n",
    "    else:\n",
    "        G = np.random.randint(1,7)\n",
    "    \n",
    "    V[n] = V[n-1] + 1/n*(G-V[n-1])\n",
    "    \n",
    "plt.plot(V[1:])\n",
    "plt.plot([1, N_two, N_two, N], [7, 7, 3.5, 3.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac11721-5e2f-4c8a-9129-5564c8a3c441",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:** Change the code above to use a constant step-size `alpha` instead of `1/n`. Try different values of `alpha`. (At least 0.01, 0.1 and 0.5)\n",
    "\n",
    "It should be clear that using a constant `alpha` (where you put less weight on old observations) gives estimates that are much faster in detecting that the expected value has changed. In a real-world RL implementation, it may be important to be able to react to a change in the underlying environment (i.e. if $p(s', r | s, a)$ changes). It could be that we control a robot, but due to wear and tear the friction between the robot and the floor changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def07144-e99b-4f19-a1be-90000a19e03b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 3. Monte-Carlo Prediction <a id=\"sec3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a78d618-d5d6-4d93-940f-7e13f82e3320",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We will now try to find an estimate of $v_\\pi(s)$ using Monte-Carlo. \n",
    "\n",
    "In Monte-Carlo prediction, we let the agent run a full episode with policy $\\pi$ to get a trajectory\n",
    "$$\n",
    "S_0, R_1, S_1, R_2, S_2, R_3, \\ldots, S_{T-1}, R_T, S_T\n",
    "$$\n",
    "where $S_T$ is a terminal state (so no future rewards are received when this state is reached). \n",
    "\n",
    "For each state in the trajectory we then compute \n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t-1} R_T.\n",
    "$$\n",
    "Note that if we start from the end of the episode and go backwards, we can use the recursive relationship\n",
    "$$\n",
    "G_t = \\begin{cases}\n",
    "0 & \\text{if } t = T \\\\\n",
    "R_{t+1} + \\gamma G_{t+1}  &\\text{if } t<T\n",
    "\\end{cases}\n",
    "$$\n",
    "Finally we incrementally update the estimate $V(S_t)$ using \n",
    "$$\n",
    "V(S_t) \\leftarrow  V(S_t) + \\frac{1}{N(S_t)} (G_t - V(S_t))\n",
    "$$\n",
    "where $N(S_t)$ is the number of times we have updated the state $S_t$. \n",
    "\n",
    "Below we implement the every-visit version of the MC-algorithm. Make sure that you understand the code. To change it to the first-visit version, you would also have to add code so that you only update $V(S_t)$ if $t$ is the first time you are in state $S_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994a8f1-1188-445b-8e8c-d7ad0431a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAgent():\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, gamma):\n",
    "        self.n_actions = n_actions\n",
    "        self.V = np.zeros(n_states)\n",
    "        self.N = np.zeros(n_states)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Use a uniform random policy (all actions have equal probability)\n",
    "        return np.random.choice(self.n_actions)\n",
    "    \n",
    "    def learn(self, states, rewards):\n",
    "        T = len(states)\n",
    "        G = 0 # G_T = 0\n",
    "        \n",
    "        for t in reversed(range(T)): # From T-1 to 0\n",
    "            G = rewards[t+1] + self.gamma*G # G_t = R_{t+1} + gamma*G_{t+1}\n",
    "            \n",
    "            self.N[states[t]] += 1\n",
    "            self.V[states[t]] += 1/self.N[states[t]] * (G - self.V[states[t]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1088cf-b640-48fe-b42e-f383d6403b9a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To test our implementation, we use the `GymGrid-v0` environment that was also studied in Tinkering Notebook 2. With a uniform random policy and discount $\\gamma = 1$, we know that the state-value function for this environment will be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ce051-547e-4428-8c2b-c688655ce342",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function = np.array([ [0.,   -14., -20., -22.],\n",
    "                            [-14., -18., -20., -20.],\n",
    "                            [-20., -20., -18., -14.],\n",
    "                            [-22., -20., -14., 0.] ]).ravel() \n",
    "# ravel turn value_function into a flat array. To write it as a matrix use reshape.\n",
    "print(value_function.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4eb1b5-190e-4b19-953f-afba3623a884",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To try out the MC-algorithm, we run 1000 episodes using the uniform random policy implemented in `agent.act`. After each episode we use `agent.learn` to update our current estimate of the state-value function. We also keep track off the maximum absolute error in the estimate (which we can compute since we in this case know the true value function).\n",
    "\n",
    "We also save the history of the estimated values, so that we can see how the value in different states converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18280f21-18c2-47ed-8ee8-6602e5298aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "\n",
    "env = gym.make('GridWorld-v0') # the same as in Example 3.5\n",
    "agent = MCAgent(env.observation_space.n, env.action_space.n, gamma=1)\n",
    "\n",
    "error = np.zeros(n_episodes)\n",
    "values = np.zeros((n_episodes, env.observation_space.n))\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    state, info = env.reset()\n",
    "    states = []\n",
    "    rewards = [0] # rewards[0] is not used in MC, so can set it to anything\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not terminated and not truncated: # Run one episode with the agents policy\n",
    "        states.append(state) # Add state to list of states seen\n",
    "        action = agent.act(state) \n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward) # Add reward to list of rewards seen\n",
    "        \n",
    "    agent.learn(states, rewards) # Update value function\n",
    "    values[i,:] = agent.V\n",
    "    # Maximum difference between true and estimated value function:\n",
    "    error[i] = np.max(np.abs(agent.V - value_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d21fab-a5c0-4b66-a20f-c997093fa05e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We first plot the maximum absolute error after each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf524d-cc85-47f2-9b77-e3834d8dcb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error)\n",
    "plt.ylabel('Maximum error in estimates')\n",
    "plt.xlabel('Number of episodes');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa910006-bf4b-40c4-b2fe-512c9664bcb7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And then we plot the estimated value of state `s` after each episode. You can change `s` to look at different states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6426b0f-2bd5-4cb3-8f76-8947a5ec8768",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1\n",
    "# Plot estimated value in state s\n",
    "plt.plot(values[:,s], label='Estimated values')\n",
    "\n",
    "# Plot true value in state s\n",
    "plt.plot([0, n_episodes], [value_function[s], value_function[s]], label='True value')\n",
    "\n",
    "plt.xlabel('$n$ - number of episodes')\n",
    "plt.ylabel(\"Estimated $V(s)$ for $s={}$\".format({s}));\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26953afd-2c6a-4ffe-9b19-7c0ba002efd4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Tasks:\n",
    "1. Increase the number of episodes (e.g. to 10 000) you use in training, and see if you get better estimates.\n",
    "\n",
    "2. Try to run the MC-method with a constant step-size. (You have to change the code in `MCAgent` and remember to execute the code cell again when you have made your changes.)\n",
    "\n",
    "3. (*) Write a first-visit version of `MCAgent` and try it with the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e058c-1c5f-4769-a86c-92c4701bd852",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 4. Temporal Differences Prediction (TD) <a id=\"sec4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3456db-298e-4a62-bf50-716af09f8d5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We now implement an agent that learn using TD. A benefit of using TD is that we do not have to wait until the end of an episode to do the updates. \n",
    "\n",
    "**Task:** Implement the TD-update in the `learn`-method of `TDAgent`. Use a constant step-size `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafdd17-aab0-42f0-8260-fea72e64caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent():\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, gamma, alpha):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.V = np.zeros(n_states)\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Random\n",
    "        return np.random.choice(self.n_actions)\n",
    "    \n",
    "    def learn(self, state, action, reward, state_next):\n",
    "        self.V[state] += self.alpha*(reward + self.gamma*self.V[state_next] - self.V[state]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccc3203-ab40-40dd-a0e6-c67ca7714207",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We will again test our agent on the `GridWorld-v0` with environment with a uniform policy and discount $\\gamma = 1$, so the true value function is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6331df8-7dc7-4a99-a2eb-7dc34148b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function = np.array([ [0.,   -14., -20., -22.],\n",
    "                            [-14., -18., -20., -20.],\n",
    "                            [-20., -20., -18., -14.],\n",
    "                            [-22., -20., -14., 0.] ]).ravel() \n",
    "# ravel turn value_function into a flat array. To write it as a matrix use reshape.\n",
    "print(value_function.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4f510-7316-4238-b8bd-d3ee7cc7e8cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The code below runs 10 000 episodes. Note that we now have moved the `learn` inside the `while`-loop, since we do not have to wait for the episode to end. This also makes it possible to use the TD-method in continuing environments (that never terminates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b4b4d-4d94-4f86-9d90-51cce0f64d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000\n",
    "s = 5 # The state we save the history for\n",
    "error = np.zeros(n_episodes)\n",
    "values = np.zeros((n_episodes, env.observation_space.n))\n",
    "\n",
    "env = gym.make('GridWorld-v0') # the same as in Example 3.5\n",
    "agent = TDAgent(env.observation_space.n, env.action_space.n, alpha=0.01, gamma=1)\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not terminated and not truncated:\n",
    "        action = agent.act(state)\n",
    "        state_next, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.learn(state, action, reward, state_next)\n",
    "        state = state_next\n",
    "        \n",
    "    error[i] = np.max(np.abs(agent.V - value_function))\n",
    "    values[i, :] = agent.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18827539-d2c4-400d-8caf-98acf6119262",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error)\n",
    "plt.ylabel('Error in estimates')\n",
    "plt.xlabel('Number of episodes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c5fc0-543a-4cf6-8999-c17e1e1c3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1\n",
    "plt.plot(values[:, s])\n",
    "plt.plot([0, n_episodes], [value_function[s], value_function[s]])\n",
    "plt.xlabel('$n$ - number of episodes')\n",
    "plt.ylabel(\"Estimated $V(s)$ for $s={}$\".format({s}));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83065bdb-b024-488d-98bf-15a075e3a017",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Task:\n",
    "1. Try different step sizes `alpha`. Try to explain the difference between the results for different `alpha`. \n",
    "2. In the book it is stated that TD will have a lower variance than MC. Can you see this in your results? \n",
    "\n",
    "Note: Variance here refers to how much your results will vary if you re-run the experiment from scratch many times. If you compare the TD-method with the MC-method, you will note that the results from TD looks similar each time you re-run the code above, but for MC the result will, especially for small $n$, be quite different each time. \n",
    "\n",
    "However, we can also see that using the TD-method, on a single experiment, the estimate continues to vary when $n$ increases. This is due to the fact that we use a constant step length $\\alpha$, so the result does not converge to a final value. (See the discussion about the dice example in the beginning of the notebook). You will see the same effect in the MC-method if you tried to use a constant step size there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
