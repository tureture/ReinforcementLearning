{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1504ed4f-aa3f-4a5d-a0c3-21ea78827ee9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Tinkering Notebook 3b: Model-free control\n",
    "\n",
    "In this notebook we will test model-free control. We will implement and use SARSA and Q-learning. If you are interested you can also try to implement Monte Carlo-control, but this method will not work very well on the environments we study in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a48055",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Table of content\n",
    "* ### [1. Imports](#sec1)\n",
    "* ### [2. Termination vs truncation](#sec2)\n",
    " * #### [2.1 \\*How to handle actual time limits](#sec2_1)\n",
    "* ### [3. Helper functions](#sec3)\n",
    "* ### [4. SARSA](#sec4)\n",
    " * #### [4.1 SARSA on Example 3.5](#sec4_1)\n",
    " * #### [4.2 Example 6.5: Windy Grid World](#sec4_2)\n",
    "* ### [5. Q-learning](#sec5)\n",
    " * #### [5.1 Example 6.6](#sec5_1)\n",
    "* ### [6. A note on exploration](#sec6)\n",
    "* ### [7. \\* MountainCar-v0](#sec7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a263c-c771-4ad7-b489-df20a410bdb1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 1. Imports <a id=\"sec1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece07cce-a458-47b5-bb9e-852509abc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_RLcourse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07437ace-ccbd-4b1f-9729-216fd0599c70",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 2. Termination vs truncation <a id=\"sec2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d0246a-d8a2-48c6-bed4-b26ed8bdb06c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In an episodic MDP, the episode will end when (and only when) a terminal state is reached. In a continuing environment, there are no terminal states so it will never stop. Even in an episodic MDP the agent may, depending on the policy it uses, never reach a terminating state and thus continue forever. \n",
    "\n",
    "Of course, in practice we cannot continue running the agent forever. Therefore most environments in Gymnasium implement a time limit and stops even if a terminal state was not reached. In such cases it returns `truncated = True` when you call `env.step(a)`. \n",
    "\n",
    "This is mainly a problem in Monte-Carlo control. Here we first collect the data of a full episode before we do any learning. If the environment truncates the episode, we did not actually get a full episode trajectory! \n",
    "\n",
    "In this notebook we will look at SARSA and $Q$-learning. In these cases it is not a problem that the episode truncates, since we only use 1-step transitions in the updates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70928bf-0361-4e83-a09b-e67f11b38fdf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 2.1 \\*How to handle actual time limits <a id=\"sec2_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d599116b-6904-4e36-a761-9c52c68c0099",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the environments we will study in this notebook the time limits are artificially stopping the environment before it was done, so the time limit is not actually a part of the Markov decision process. \n",
    "\n",
    "But there are tasks where the time-limit itself is important. Lets say the task is for a humanoid robot to run as far as possible in 60 s. Optimizing this would lead to different behavior depending on the time: In the beginning it is important for the robot to not fall down, but towards the end of the 60 s it may be a good idea to be more aggressive even if this would mean that the robot falls down after the 60 s are over (throwing itself over the finish line). \n",
    "\n",
    "To encode this into an MDP, we could include e.g. \"time left\" in the state. Then any state with \"time left = 0\" is a terminating state. Without this the state would not have the Markov-property, and a  policy $\\pi(a|s)$ could not know if we are in the beginning of the race or if we are close to the end of the 60 s. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cddf49-8af0-4544-a986-3cbf49630055",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 3. Helper functions <a id=\"sec3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050203c3-76c6-4a54-940b-9b918b2b1ff2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We here define a function `test_policy` that can be used to see the policy we have found in action. It takes an agent and an environment, and runs the agents policy on the environment for one episode. If the environment has `render_mode=\"human\"` this will also render the environment for you. If the flag `render=True` some information will also be printed while the episode is going on. \n",
    "It will run until the episode is done or `max_steps` (default 40) has been taken.\n",
    "\n",
    "If you want to use a different rendering mode than `human` you can add code inside `if render:` to e.g. print textual representation of the environment or plot an `rgb_array`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39bcd4-e239-48eb-b28d-3b595fcac3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(agent, env, max_steps=40, render=True):\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not terminated and not truncated and step<max_steps:\n",
    "        action = agent.act(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        if render:\n",
    "            clear_output(wait=True)\n",
    "            # Show some information\n",
    "            print(\"Time step:\", step)\n",
    "            print(\"Action:\", action)\n",
    "            print(\"Reward:\", reward)\n",
    "            print(\"Total reward:\", total_reward)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca1849-e09e-4daf-b83f-d9e0a4d17ae0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 4. SARSA <a id=\"sec4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10ac67-6eaf-4b33-9c9e-0d56c19ea9be",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In this section we will implement and try out SARSA-control.\n",
    "\n",
    "We first implement the function `train_sarsa` with the arguments\n",
    "* `agent` - Should be an object with the methods `act` that implements the current policy, and `learn` that is used to update the estimated `Q`.\n",
    "* `env` - The environment \n",
    "* `n_episodes` - The number of episodes we should use to train the agent (as discussed above, the episodes may be truncated)\n",
    "\n",
    "The function also computes an array `steps` that shows the total number of time steps that have been used after each episode, and `total_rewards` that gives the total reward for each episode.\n",
    "\n",
    "**Task:** Read the code, and compare it with the pseudo-code for SARSA seen in the slides of Lecture 5. Make sure that you understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9def14-34e9-41c6-8680-5becb1571c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(agent, env, n_episodes):\n",
    "    step = 0\n",
    "    steps = np.zeros(n_episodes) # Steps after each episode\n",
    "    total_rewards = np.zeros(n_episodes)\n",
    "    for i in range(n_episodes):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        rewards=0\n",
    "        \n",
    "        state, info = env.reset()\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        while not terminated and not truncated:\n",
    "            state_next, reward, terminated, truncated, info = env.step(action)\n",
    "            action_next = agent.act(state_next)\n",
    "            agent.learn(state, action, reward, state_next, action_next)\n",
    "            state = state_next\n",
    "            action = action_next\n",
    "            step += 1\n",
    "            rewards += reward\n",
    "            \n",
    "        steps[i] = step\n",
    "        total_rewards[i] = rewards\n",
    "    return total_rewards, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1082d-adfa-4156-8820-65d208e95756",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We next define the class `SARSA` that implements the agent. \n",
    "\n",
    "**Task:** \n",
    "1. Implement a policy in `act` that is $\\varepsilon$-greedy w.r.t `self.Q`. ($\\varepsilon$ = `self.epsilon`)\n",
    "\n",
    "2. Implement the SARSA-update of $Q$ in `learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a97ba4-48f2-4f54-a32a-24bf41e47f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA():\n",
    "    def __init__(self, n_states, n_actions, gamma, alpha, epsilon):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        \n",
    "    def act(self, state):\n",
    "        # You can use np.random.choice(self.n_actions) to get a random action\n",
    "        # Implement epsilon-greedy policy\n",
    "            \n",
    "        return action\n",
    "            \n",
    "    def learn(self, s, a, r, s_next, a_next):\n",
    "        # Implement the TD(0) update of Q (see equation (6.7) in textbook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5391668d-5c56-472a-ac32-337b37225d22",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 4.1 SARSA on Example 3.5 <a id=\"sec4_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579ff86-3c4a-494b-bf8e-3eed870848ea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Here we will try SARSA on the `GridWorld-AB-v0`. The environment is described in Example 3.5.\n",
    "\n",
    "Let us first take a quick look at the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f35a71-6070-42f7-a6c1-1343d2bcee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('GridWorld-AB-v0', render_mode=\"human\") # the same as in Example 3.5\n",
    "state, info = env.reset()\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e5311b-626a-4d7c-bdf5-1c199ac6ec5f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We have 25 states and 4 actions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236afbcd-fe07-42ca-9893-f2ce6e24b6fe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The optimal policy, shown in Figure 3.5 in the textbook, is to always move towards $A$. This is a continuing environment, and there are no terminal states. However, the environment is implemented so that it will truncate after 100 time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8a50b-4a5a-4523-b5eb-2f19abe5ae84",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Training without rendering:**\n",
    "If you have rendering on while training, you computer will spend time on rendering the environment that could be better used training. Therefore it is recommended that you **do not** use `render_mode=\"human` while training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0dd00-2796-4f16-82b8-327582089082",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = gym.make(\"GridWorld-AB-v0\") # No rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62e31f-4c28-445f-91eb-fc1da5500de6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We now create the agent that will train on the environment. We use the discount $\\gamma = 0.9$, step length $\\alpha = 0.1$ and set $\\varepsilon = 0.1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40454437-b2b9-4498-bfb1-620f82d0cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SARSA(env_train.observation_space.n, env_train.action_space.n, gamma=0.9, alpha=0.1, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae5525-1df1-4479-90bb-2ae2b9172300",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We now train train the agent. If you have `render_mode=\"human\"` this will take some time. \n",
    "\n",
    "**Note:** If you run the cell below again without resetting the agent, you will continue from your already estimated $Q$. That is, running the cell below two times effectively doubles the number of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7c2ed-8345-4007-8eaa-246e95631521",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_resets = 200 # Train with n_resets * 100 time steps\n",
    "rewards, _ = train_sarsa(agent, env_train, n_resets) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d7afc-cc93-4793-9bc4-702bbb24d32c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We next plot the total reward for each 100 times steps in the training episodes. \n",
    "This should hopefully show that the total reward may start at a very small level, but that it will increase as the agent learns more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ad729-6f6f-470c-a581-4fab2816cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, n_resets+1), rewards)\n",
    "plt.xlabel(\"Number of resets\")\n",
    "plt.ylabel(\"Total reward over 100 time steps\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c34b2c-2bcb-489c-9504-676305022c7e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We next test to see how the policy works in practice. Note that we are still using the $\\varepsilon$-greedy policy in `agent`, so it will choose a random action with probability $\\varepsilon$. \n",
    "\n",
    "The main reason for using an $\\varepsilon$-greedy policy is to ensure that the agent explores while it trains. Here we want to evaluate the policy that the agent has learned, and it thus makes sense to set the exploration to zero ($\\varepsilon = 0$) and hence use the greedy policy w.r.t the estimated $Q$. (However, if you will continue to train after this, you should add exploration again)\n",
    "\n",
    "**Task:** Test your policy both with the $\\varepsilon$-greedy policy you trained your policy with, and the greedy policy ($\\varepsilon = 0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc70017-0c59-4fa1-b549-8708b7c917e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.epsilon = 0\n",
    "test_policy(agent, env, max_steps=40)\n",
    "#agent.epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7e174-3a6b-485a-9f27-552db2b4e5cd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can also visualize the greedy policy w.r.t $Q$ with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef8eaa-f126-4098-a7a2-437808367f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_greedy_policy(Q):\n",
    "    # Prints an illustration of the greedy policy with respect to Q\n",
    "    n_states = Q.shape[0]\n",
    "    greedy = np.full(n_states, 'W') \n",
    "    for s in range(n_states):\n",
    "            a = np.argmax(Q[s,:])\n",
    "            if a == 0:\n",
    "                greedy[s] = 'W'\n",
    "            elif a == 1:\n",
    "                greedy[s] = 'S'\n",
    "            elif a == 2:\n",
    "                greedy[s] = 'E'\n",
    "            elif a == 3:\n",
    "                greedy[s] = 'N'\n",
    "\n",
    "    print(greedy.reshape(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bf2b6-a611-426a-be30-19f33762008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_greedy_policy(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6f7b0-da09-4639-b471-9cc7b08c10dc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "If you only trained with $200 \\times 100$ time steps, it is common that SARSA finds a greedy policy that sometimes ends up going from $b$ to $B$ over and over again. This is not optimal, since going from $a$ to $A$ gives a higher reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06981eed-e9b4-4740-a907-f207baf4bd03",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:** Compare the policy your agent has found with the optimal policy in Figure 3.5 in the textbook. Is the policy optimal in all or at least most of the states? (Otherwise, train it longer using the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282ccf5-5c0f-41ee-a888-432cec18ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_resets = 1000 # Train with n_resets * 100 time steps\n",
    "rewards, _ = train_sarsa(agent, env_train, n_resets) \n",
    "render_greedy_policy(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16efbdcb-5848-4c36-9301-e9eab61cd90b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 4.2 Example 6.5: Windy Grid World <a id=\"sec4_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b1efb-1dc7-4a02-85e4-9cc6b440fabf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In this section we will try out Example 6.5 in the textbook. \n",
    "    \n",
    "In this environment we have a $7 \\times 10$ grid, with a starting point and a goal. It is a windy grid world, since in some of the columns the agent will get pushed up when it takes an action. See Example 6.5 for a detailed explanation. \n",
    "    \n",
    "Lets look at the state and action spaces of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aec515-aaa5-4d4d-8452-17d0b43520c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('GridWorld-Windy-v0', render_mode=\"human\")\n",
    "state, info = env.reset()\n",
    "print(\"State space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea42e0-0386-4c18-8c9e-a0f811b34d34",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "There are 70 states, and 4 actions corresponding to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a16d6d-99ce-43f4-aec1-f0addce7da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEST = 0\n",
    "SOUTH = 1\n",
    "EAST = 2\n",
    "NORTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714ee2e-df3a-4d9e-a34c-83c8f353bc66",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Reward**: The reward is -1 for each action until the goal is reached, so to maximize total reward the agent should find the goal (that is again supposed to look like an ice cream...) in as few steps as possible. \n",
    "\n",
    "**Optimal policy:** The optimal policy uses 15 steps and is shown in Example 6.5 in the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b30d3c-c32b-44db-9b8a-e0752e7551cd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We will first try out the agent using the exact same setting as in Example 6.5 ($\\alpha = 0.5$ and $\\varepsilon=0.1$), training the agent using 170 episodes. However note that, due to the use of random actions, you will not get exactly the same result as in Example 6.5 every time. If you are unlucky, you may even get a policy that have a hard time reaching the goal.\n",
    "\n",
    "We first create the agent and an environment that does not render:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51921fed-6683-424c-b30d-ca1beb7915db",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SARSA(env.observation_space.n, env.action_space.n, gamma=1, alpha=0.5, epsilon=0.1)\n",
    "env_train = gym.make(\"GridWorld-Windy-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df3554-8b1d-4a4a-b2bd-166c76175e1d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Next we train the agent. \n",
    "\n",
    "**Note:** If you rerun the cell below, without resetting (creating a new) agent, you will start from the Q-table you previously trained. That is, you will effectively double the number of training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406ec46-7a9d-49d4-96ce-d1f1dd94be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes=170\n",
    "rewards, steps = train_sarsa(agent, env_train, n_episodes); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e8094-194b-4e40-bf12-81c7aa3012ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps, range(steps.shape[0]))\n",
    "plt.xlabel('Time steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be4da4-a35d-4e6a-8de0-6e3e6e85cce1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To see the policy in action we can use `test_policy`. \n",
    "\n",
    "As in the previous example, `test_policy` will use the $\\varepsilon$-greedy policy. To also see the greedy policy w.r.t $Q$ in action, first set `agent.epsilon = 0`.\n",
    "\n",
    "**Task:** Run the $\\varepsilon$-greedy you trained with a few times, and then try the greedy policy ($\\varepsilon = 0$). Does your greedy policy manage to get to the goal? Is your greedy policy optimal (15 time steps)?\n",
    "\n",
    "If your greedy policy is not (at least close to) optimal, or does not even get to the goal, try to train it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7b65b-675f-4fbd-9f76-cbfc12099ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon = 0\n",
    "test_policy(agent, env, max_steps=40)\n",
    "agent.epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e537921-c4c3-4675-b8fa-133adb921a9f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:** Solve Exercise 6.9 in the textbook. First try to find the optimal path by hand in this case, and then train an agent using SARSA. This case is implemented in the environment `GridWorld-WindyKing-v0`. Here the following eight actions are allowed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a72fd1-e41b-43aa-9aa4-62e9cce62c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEST = 0\n",
    "SOUTH = 1\n",
    "EAST = 2\n",
    "NORTH = 3\n",
    "NW = 4\n",
    "NE = 5\n",
    "SW = 6\n",
    "SE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c073b78-c3a9-4f8a-93e7-ecdd757c71e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9ab09-3f33-4d2c-9329-da8ebed06251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for training and testing the agent (you can use train_sarsa and test_policy from before)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03c58d1-4747-4cf6-af5d-ea8819df41b6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 5. Q-learning <a id=\"sec5\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d211e3b-7454-45b1-a6c6-926524b7d4c9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We now implement $Q$-learning. We first define the function `train_q`. It is very similar to `train_sarsa`, but for $Q$-learning we do not need `action_next` when we update the $Q$-function.\n",
    "\n",
    "**Task:** Look through the code, and compare it with the pseudo-code in the slides of Lecture 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e78920-4f83-47e8-9a5a-21bada1d5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q(agent, env, n_episodes, max_steps=50000):\n",
    "    step = 0\n",
    "    steps = np.zeros(n_episodes) # Steps after each episode\n",
    "    total_rewards = np.zeros(n_episodes)\n",
    "    for i in range(n_episodes):\n",
    "        rewards=0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        state, info = env.reset()\n",
    "        while not terminated and not truncated:\n",
    "            action = agent.act(state)\n",
    "            state_next, reward, terminated, truncated, info = env.step(action)\n",
    "            agent.learn(state, action, reward, state_next)\n",
    "            state = state_next\n",
    "            step += 1\n",
    "            rewards += reward\n",
    "            \n",
    "            if step>max_steps:\n",
    "                return steps, rewards\n",
    "        steps[i] = step\n",
    "        total_rewards[i] = rewards\n",
    "    return total_rewards, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd95c70-7f15-47b8-99f0-742b40585265",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Next we define the class `QAgent`. Note that the goal of $Q$-learning is to estimate the optimal $Q$-function while running a different behavioral policy. Here we implement the behavioral policy ($\\varepsilon$-greedy w.r.t to current estimate of $Q$) in `act`.\n",
    "\n",
    "**Task:** \n",
    "1. Implement the behavior policy in `act`. That is $\\varepsilon$-greedy w.r.t `self.Q`. ($\\varepsilon$ = `self.epsilon`)\n",
    "\n",
    "2. Implement the Q-learning update in `learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415af77-f171-4350-94b1-68cc4ae2785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    def __init__(self, n_states, n_actions, gamma, alpha, epsilon):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Implement the self.epsilon-greedy policy\n",
    "            \n",
    "        return action\n",
    "            \n",
    "    def learn(self, s, a, r, s_next):\n",
    "        # Implement the Q-learning update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925135d-a8a7-4c54-9f03-67cb80d55828",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 5.1 Example 6.6 <a id=\"sec5_1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2895b10-e301-47aa-8cbc-11aaba4fc8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0', render_mode=\"human\")\n",
    "state, info = env.reset()\n",
    "print(\"State space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66866c6-8ca2-4787-90c3-c9da79f7cc57",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "* **State space:** One state for each of the possible agent positions.\n",
    "* **Action space:** The are 4 possible actions corresponding to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145cc49e-e92d-4331-956b-75342216caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357dc8bd-2466-4b34-98f5-ce612e38fabd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Reward:** -1 for each step, except when the agent enters the cliff region resulting in a -100 reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab35826-bd2a-494d-8ad5-c2ef70e64c94",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We will train one agent using $Q$-learning and one agent using SARSA.\n",
    "\n",
    "Both uses $\\gamma = 1$, $\\alpha = 0.3$ and $\\varepsilon = 0.1$. We also setup an environment without rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a3748-340e-4813-9797-af8d9c5a037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = gym.make(\"CliffWalking-v0\")\n",
    "agentQ = QAgent(env.observation_space.n, env.action_space.n, gamma=1, alpha=0.3, epsilon=0.1)\n",
    "agentSARSA = SARSA(env.observation_space.n, env.action_space.n, gamma=1, alpha=0.3, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54f1a9-306e-4b7a-b579-c7ca33eb0b10",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Train both agent for e.g. 1000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8686c-6cf3-4a67-a7e6-032d9c7088f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q(agentQ, env_train, n_episodes=1000)\n",
    "train_sarsa(agentSARSA, env_train, n_episodes=1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b6a592-8152-4556-8096-cb0db9d50fb0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You can now test to run your two policies. \n",
    "\n",
    "**Note:** In both cases you use the `act`-method that is implemented with an $\\varepsilon$-greedy policy. To use the greedy policy w.r.t $Q$ we first have to set $\\varepsilon = 0$. \n",
    "\n",
    "**Task:** Try to run both with $\\varepsilon$-greedy and greedy policies ($\\varepsilon = 0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad51ca6-9b69-4dfe-8e45-043d015d06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agentQ.epsilon = 0\n",
    "total_reward = test_policy(agentQ, env, max_steps=100)\n",
    "#agentQ.epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce55f74-814c-4c33-afc8-b205f569f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agentSARSA.epsilon = 0\n",
    "total_reward = test_policy(agentSARSA, env, max_steps=100)\n",
    "#agentSARSA.epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4224b990-f02e-42c8-9ce7-e74c3f5526ec",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can see that the two policies behaves quite differently. SARSA tries to learn the best $\\varepsilon$-soft policy while $Q$-learning tries to learn the optimal $q_*$. \n",
    "\n",
    "In this example, the $Q$-learning algorithm will thus try to take as short path to the goal as possible. However, the SARSA-algorithm looks at $\\varepsilon$-soft policies, and thus also has to take into account that there is a non-zero probability that a non-greedy action is taken. Since the cost of moving into the cliff region is so large, it will take a safer path to ensure that a non-optimal action will not lead into the cliff.\n",
    "\n",
    "**Task:** \n",
    "Below is a code that will run both the policies learned from $Q$-learning and SARSA 1000 times. It then prints the mean reward. \n",
    "\n",
    "Try to run the code both using the $\\varepsilon$-greedy policies and the greedy policy ($\\varepsilon = 0$). We can see that if the policy is $\\varepsilon$-soft then the $Q$-learning version does worse because it more often moves into the cliff region. However, when we set $\\varepsilon=0$ then $Q$-learning is best because it takes the shortest path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd0b3f-e18c-43c5-b05d-d70f57bf4bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "reward_Q = np.zeros(n_episodes)\n",
    "reward_SARSA = np.zeros(n_episodes)\n",
    "#agentQ.epsilon = 0\n",
    "#agentSARSA.epsilon = 0\n",
    "for k in range(n_episodes):\n",
    "    reward_Q[k] = test_policy(agentQ, env_train, max_steps=100, render=False)\n",
    "    reward_SARSA[k] = test_policy(agentSARSA, env_train, max_steps=100, render=False)\n",
    "\n",
    "#agentQ.epsilon = 0.1\n",
    "#agentSARSA.epsilon = 0.1\n",
    "    \n",
    "print(\"Mean reward Q: \", np.mean(reward_Q))\n",
    "print(\"Mean reward SARSA: \", np.mean(reward_SARSA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545d4a3-c2ba-48c2-b769-8343e4b5c2d3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 6. A note on exploration <a id=\"sec6\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b379c-39de-4a2f-892b-957a8ec906e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In this notebook we have used $\\varepsilon$-greedy policies to ensure that we always continue to explore during training. However, there are other ways to get exploration and we will discuss them later in the course. \n",
    "\n",
    "An easy way to at get at least temporary exploration would be to initialize your $Q$-function with large values compared to the best possible return (except for terminating states which should be initialized to 0). \n",
    "\n",
    "This would mean that state/action pairs that we have never seen before will have a large estimated value, and therefor the agent is encouraged to visit them. \n",
    "\n",
    "For example, in the Windy GridWorld, the rewards are always negative. We initialize $Q$ with zeros. When we now are in a state and try an action, we will update the $Q$-value for this state/action pair to something negative. Next time we come to this state, the actions we have not tried yet all have estimated value 0, so they will look better! \n",
    "\n",
    "**Task:** Try to train both `Gridworld-Windy-v0` and `GridWorld-5x5-AB-v0` with $\\varepsilon = 0$. Use the discussion above to explain the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e132f-5499-4080-a181-a5c29078de28",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 7. \\* MountainCar-v0 <a id=\"sec7\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76987ede-6d4f-458d-ad4b-3c19688d285a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "So far in the course we have looked at environment with a finite number of states and actions. Hence we can represent the $Q$-function as a matrix where each element corresponds to a state/action-pair. \n",
    "\n",
    "If the state (or action) space is continuous (infinitely many states) this is not possible. We will start looking in to these types of problems in Lecture 6. \n",
    "\n",
    "However, already now we can mention one simple trick that can sometimes work in these cases - namely discretization of the state space. \n",
    "\n",
    "As an example we look at the `MountainCar-v0` environment studied in Tinkering Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e0446-b613-42f0-976d-5364b1976dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "state, info = env.reset()\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b515ac-78d9-4c7f-a0ba-8bcbea1b0aea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Here the state contains two elements: the position and velocity of the car, so there are infinitely many states.  \n",
    "\n",
    "**Idea:** We divide the state space into a number of discrete states as in the image below.\n",
    "<center><img src=\"./grid.png\"></center>\n",
    "Hence, every position/velocity pair that ends up in tile 4 is considered to be in state 4. An implication of this is that we learn the same $Q$-value for any position/velocity pair in tile 4 etc. Hence, the $Q$-values are now only approximations of the true $Q$-values, but if the grid is fine enough we may still be able to learn a good policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7a6a6-70f6-4b9f-855f-b168cf6218e8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The function `toDiscreteState` below can be used to divide the 2-dimensional state space of MountainCar into a tiles[0]  ×  tiles[1] grid. Given a 2-dimensional state of MountainCar it will return an integer representing the discretized state. It will be enough to use a  30×30  grid, i.e., 900 states. (It may also work well with fewer states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdba595-58e0-4a69-9f14-b3ddca6a32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDiscreteState(state, tiles=np.array([30,30])):\n",
    "    max_range = np.array([1.2, 0.07]) #  -1.2 <= pos <= 1.2, -0.07 <= vel <= 0.07\n",
    "    \n",
    "    state_tile = np.floor((tiles)*(state+max_range)/(2*max_range))\n",
    "    \n",
    "    if state_tile[0] >= tiles[0]:\n",
    "        state_tile[0] = tiles[0]-1\n",
    "    if state_tile[1] >= tiles[1]:\n",
    "        state_tile[1] = tiles[1]-1\n",
    "        \n",
    "    return int(state_tile[0]+state_tile[1]*tiles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6a951-7da5-4d65-9207-370d358dfc0e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The function `train_car` below can be used to train a SARSA-agent using the discretized states. It is implemented just as `train_sarsa` but uses the discreteized states instead of the true continuous states.\n",
    "\n",
    "We also define `test_car` that is similar to `test_policy` above, but uses `toDiscreteState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686f409-ebea-4113-acd3-13e36153898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_car(agent, env, n_episodes):\n",
    "    total_rewards = np.zeros(n_episodes)\n",
    "    for i in range(n_episodes):\n",
    "        state, info = env.reset()\n",
    "        state = toDiscreteState(state) # To discrete state!\n",
    "        action = agent.act(state)\n",
    "        rewards=0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        while not terminated and not truncated:\n",
    "            state_next, reward, terminated, truncated, info = env.step(action)\n",
    "            state_next = toDiscreteState(state_next) # To discrete state!\n",
    "            action_next = agent.act(state_next)\n",
    "            agent.learn(state, action, reward, state_next, action_next)\n",
    "            state = state_next\n",
    "            action = action_next\n",
    "            rewards += reward\n",
    "            \n",
    "        total_rewards[i] = rewards\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Episode\", i)\n",
    "            \n",
    "    clear_output(wait=True)\n",
    "    print(\"Done\")\n",
    "    return total_rewards\n",
    "\n",
    "def test_car(agent, env, wait=0.01, render=True): \n",
    "    state, info = env.reset()\n",
    "    state = toDiscreteState(state)\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not terminated and not truncated:\n",
    "        action = agent.act(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        state = toDiscreteState(state)\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        \n",
    "        if render:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            # Show some information\n",
    "            print(\"Time step:\", step)\n",
    "            print(\"Reward:\", reward)\n",
    "            print(\"Total reward:\", total_reward)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb73c24-9ed8-4908-ac10-2418e8ac78fb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We use the SARSA-agent implemented previously. We use discount $\\gamma = 0.99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a090be-afc3-4a33-85e3-96a67695b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = gym.make('MountainCar-v0')\n",
    "agent = SARSA(30*30, 3, gamma=0.99, alpha=0.1, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e9982c-4c54-4202-8830-12087a92d28f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This is a relatively hard problem to solve. The reward signal gives $-1$ for each step until the flag is reached. Hence, before the agent has reached the flag the first time all actions will look equally bad. For this reason you will probably not see any improvement at all for about 1000 episodes.  \n",
    "\n",
    "The environment is truncated after 200 time steps. So if an episode goes on for longer than 200 time steps, it will be ended prematurely.\n",
    "\n",
    "Try to train it for 2000 episodes to start with (this will take some time). To find a policy that is near optimal, you will probably have to increase the number of episodes to at least 10 000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c6cd5-a6e4-444a-896c-7d2972e9b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 2000\n",
    "rewards = train_car(agent, env_train, n_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c051817-f7e2-49e4-92f7-35f8edb1a7a1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We now plot the total rewards given after each episode. Note that, since the episode stops if the episode goes on for more than 200 time steps, to worst possible total reward we can get is -200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d81c3e-2a80-4ab7-8ac6-8697947f79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d683c-748d-4b5f-8dc2-936fd2489658",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Finally we test to see how the car behaves in practice. If it does not manage to reach the flag most time you run it, try to train it for more episodes. \n",
    "\n",
    "*Note:* If you re-run the code cell with `train_car` without resetting the agent, it will continue to train from your current estimated `Q`-function instead of restarting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c61309-4df5-4f8e-9345-61d7056c232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon = 0\n",
    "total_reward = test_car(agent, env)\n",
    "agent.epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85920f9f-0d17-4cb5-8fd7-ecadfae720a0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Note on exploration:** Here we again have a case where we initialize $Q$ to zero, but all rewards are negative. Hence, this will aid with the exploration, since $Q$ for actions not tested before will always look better than for actions we have tested previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f068a25-eb99-4fc8-ae29-6d40a7befcba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
